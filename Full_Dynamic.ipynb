{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Full_Dynamic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDgwyIXH99_8"
      },
      "source": [
        "# Dynamic-DeepHit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1clng8aDM0u",
        "outputId": "16e20af7-4c58-4733-8467-86080bf37e03"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not connected to a GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OuN1stf0-lZ1",
        "outputId": "2b935b42-4e47-4553-8316-ceb3749a14e0"
      },
      "source": [
        "pip install deephit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deephit\n",
            "  Downloading deephit-0.0.18.tar.gz (8.5 kB)\n",
            "Collecting lifelines==0.25.10\n",
            "  Downloading lifelines-0.25.10-py3-none-any.whl (347 kB)\n",
            "\u001b[K     |████████████████████████████████| 347 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from deephit) (1.19.5)\n",
            "Collecting pandas==1.2.3\n",
            "  Downloading pandas-1.2.3-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 49.3 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.24.1\n",
            "  Downloading scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 100.7 MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.15.5\n",
            "  Downloading tensorflow-1.15.5-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 197 bytes/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from lifelines==0.25.10->deephit) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.7/dist-packages (from lifelines==0.25.10->deephit) (3.2.2)\n",
            "Collecting formulaic<0.3,>=0.2.2\n",
            "  Downloading formulaic-0.2.4-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.7/dist-packages (from lifelines==0.25.10->deephit) (1.3)\n",
            "Collecting autograd-gamma>=0.3\n",
            "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3->deephit) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3->deephit) (2.8.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.1->deephit) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.1->deephit) (3.0.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->deephit) (0.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->deephit) (0.37.0)\n",
            "Collecting h5py<=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 35.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->deephit) (0.2.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 47.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->deephit) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->deephit) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->deephit) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->deephit) (1.41.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->deephit) (3.17.3)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 52.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->deephit) (0.8.1)\n",
            "INFO: pip is looking at multiple versions of scikit-learn to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting numpy==1.19.5\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 172 kB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of lifelines to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of deephit to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting deephit\n",
            "  Downloading deephit-0.0.17.tar.gz (8.5 kB)\n",
            "Collecting tensorflow==1.15.3\n",
            "  Downloading tensorflow-1.15.3-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 4.6 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.3->deephit) (1.13.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.3->deephit) (3.3.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.5 MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd>=1.3->lifelines==0.25.10->deephit) (0.16.0)\n",
            "Collecting interface-meta>=1.2\n",
            "  Downloading interface_meta-1.2.4-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.3->deephit) (3.1.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines==0.25.10->deephit) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines==0.25.10->deephit) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines==0.25.10->deephit) (2.4.7)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->deephit) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->deephit) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->deephit) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->deephit) (4.8.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.3->deephit) (1.5.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->deephit) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->deephit) (3.6.0)\n",
            "Building wheels for collected packages: deephit, gast, autograd-gamma\n",
            "  Building wheel for deephit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deephit: filename=deephit-0.0.17-py3-none-any.whl size=9054 sha256=ec8b7c0cc6f1d965775824c8716b351189da132210caab1cc25f698771be7e0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/7f/4d/99246fc1a6e7bd72e5db0c5635c85f31a55dbc8e5ba3f34695\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=aa896cd616a2e2dabe1e45677fe461c4cc44698ab4330e7cad5fd615810cd208\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4049 sha256=7682456694943c63804fc84d07a423862698cafd746ed7f70d9636e3b5fa5f56\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/01/ee/1331593abb5725ff7d8c1333aee93a50a1c29d6ddda9665c9f\n",
            "Successfully built deephit gast autograd-gamma\n",
            "Installing collected packages: pandas, interface-meta, tensorflow-estimator, tensorboard, keras-applications, gast, formulaic, autograd-gamma, tensorflow, scikit-learn, lifelines, deephit\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.1\n",
            "    Uninstalling scikit-learn-1.0.1:\n",
            "      Successfully uninstalled scikit-learn-1.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.14.1 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.15.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.2.3 which is incompatible.\u001b[0m\n",
            "Successfully installed autograd-gamma-0.5.0 deephit-0.0.17 formulaic-0.2.4 gast-0.2.2 interface-meta-1.2.4 keras-applications-1.0.8 lifelines-0.25.10 pandas-1.2.3 scikit-learn-0.24.1 tensorboard-1.15.0 tensorflow-1.15.3 tensorflow-estimator-1.15.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QujOTWZr8wM5",
        "outputId": "008540e1-c46c-4b78-9e73-273cafc78165"
      },
      "source": [
        "pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (1.15.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.41.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow) (4.8.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMY9ytFk_8w0"
      },
      "source": [
        "#import tensorflow as tf\n",
        "import numpy as np\n",
        "from deephit import *\n",
        "\n",
        "\n",
        "### CONSTRUCT MULTICELL FOR MULTI-LAYER RNNS\n",
        "def create_rnn_cell(num_units, num_layers, keep_prob, RNN_type, activation_fn): \n",
        "    '''\n",
        "        GOAL         : create multi-cell (including a single cell) to construct multi-layer RNN\n",
        "        num_units    : number of units in each layer\n",
        "        num_layers   : number of layers in MulticellRNN\n",
        "        keep_prob    : keep probabilty [0, 1]  (if None, dropout is not employed)\n",
        "        RNN_type     : either 'LSTM' or 'GRU'\n",
        "    '''\n",
        "    if activation_fn == 'None':\n",
        "        activation_fn = tf.nn.tanh\n",
        "\n",
        "    cells = []\n",
        "    for _ in range(num_layers):\n",
        "        if RNN_type == 'GRU':\n",
        "            cell = tf.contrib.rnn.GRUCell(num_units, activation=activation_fn)\n",
        "        elif RNN_type == 'LSTM':\n",
        "            cell = tf.contrib.rnn.LSTMCell(num_units, activation=activation_fn, state_is_tuple=True)\n",
        "            # cell = tf.contrib.rnn.LSTMCell(num_units, activation=activation_fn)\n",
        "        if not keep_prob is None:\n",
        "            cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=keep_prob, output_keep_prob=keep_prob) # state_keep_prob=keep_prob\n",
        "        cells.append(cell)\n",
        "    cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
        "    \n",
        "    return cell\n",
        "\n",
        "\n",
        "### EXTRACT STATE OUTPUT OF MULTICELL-RNNS\n",
        "def create_concat_state(state, num_layers, RNN_type, BiRNN=None):\n",
        "    '''\n",
        "        GOAL\t     : concatenate the tuple-type tensor (state) into a single tensor\n",
        "        state        : input state is a tuple ofo MulticellRNN (i.e. output of MulticellRNN)\n",
        "                       consist of only hidden states h for GRU and hidden states c and h for LSTM\n",
        "        num_layers   : number of layers in MulticellRNN\n",
        "        RNN_type     : either 'LSTM' or 'GRU'\n",
        "    '''\n",
        "    for i in range(num_layers):\n",
        "        if BiRNN != None:\n",
        "            if RNN_type == 'LSTM':\n",
        "                tmp = tf.concat([state[0][i][1], state[1][i][1]], axis=1) ## i-th layer, h state for LSTM\n",
        "            elif RNN_type == 'GRU':\n",
        "                tmp = tf.concat([state[0][i], state[1][i]], axis=1) ## i-th layer, h state for GRU\n",
        "            else:\n",
        "                print('ERROR: WRONG RNN CELL TYPE')\n",
        "        else:\n",
        "            if RNN_type == 'LSTM':\n",
        "                tmp = state[i][1] ## i-th layer, h state for LSTM\n",
        "            elif RNN_type == 'GRU':\n",
        "                tmp = state[i] ## i-th layer, h state for GRU\n",
        "            else:\n",
        "                print('ERROR: WRONG RNN CELL TYPE')\n",
        "\n",
        "        if i == 0:\n",
        "            rnn_state_out = tmp\n",
        "        else:\n",
        "            rnn_state_out = tf.concat([rnn_state_out, tmp], axis = 1)\n",
        "    \n",
        "    return rnn_state_out\n",
        "\n",
        "\n",
        "### FEEDFORWARD NETWORK\n",
        "def create_FCNet(inputs, num_layers, h_dim, h_fn, o_dim, o_fn, w_init, w_reg=None, keep_prob=1.0):\n",
        "    '''\n",
        "        GOAL             : Create FC network with different specifications \n",
        "        inputs (tensor)  : input tensor\n",
        "        num_layers       : number of layers in FCNet\n",
        "        h_dim  (int)     : number of hidden units\n",
        "        h_fn             : activation function for hidden layers (default: tf.nn.relu)\n",
        "        o_dim  (int)     : number of output units\n",
        "        o_fn             : activation function for output layers (defalut: None)\n",
        "        w_init           : initialization for weight matrix (defalut: Xavier)\n",
        "        keep_prob        : keep probabilty [0, 1]  (if None, dropout is not employed)\n",
        "    '''\n",
        "    # default active functions (hidden: relu, out: None)\n",
        "    if h_fn is None:\n",
        "        h_fn = tf.nn.relu\n",
        "    if o_fn is None:\n",
        "        o_fn = None\n",
        "\n",
        "    # default initialization functions (weight: Xavier, bias: None)\n",
        "    if w_init is None:\n",
        "        w_init = tf.contrib.layers.xavier_initializer() # Xavier initialization\n",
        "\n",
        "    for layer in range(num_layers):\n",
        "        if num_layers == 1:\n",
        "            out = FC_Net(inputs, o_dim, activation_fn=o_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
        "        else:\n",
        "            if layer == 0:\n",
        "                h = FC_Net(inputs, h_dim, activation_fn=h_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
        "                if not keep_prob is None:\n",
        "                    h = tf.nn.dropout(h, keep_prob=keep_prob)\n",
        "\n",
        "            elif layer > 0 and layer != (num_layers-1): # layer > 0:\n",
        "                h = FC_Net(h, h_dim, activation_fn=h_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
        "                if not keep_prob is None:\n",
        "                    h = tf.nn.dropout(h, keep_prob=keep_prob)\n",
        "\n",
        "            else: # layer == num_layers-1 (the last layer)\n",
        "                out = FC_Net(h, o_dim, activation_fn=o_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqyjZ7ChATwl"
      },
      "source": [
        "def save_logging(dictionary, log_name):\n",
        "    with open(log_name, 'w') as f:\n",
        "        for key, value in dictionary.items():\n",
        "            f.write('%s:%s\\n' % (key, value))\n",
        "\n",
        "\n",
        "def load_logging(filename):\n",
        "    data = dict()\n",
        "    with open(filename) as f:\n",
        "        def is_float(input):\n",
        "            try:\n",
        "                num = float(input)\n",
        "            except ValueError:\n",
        "                return False\n",
        "            return True\n",
        "\n",
        "        for line in f.readlines():\n",
        "            if ':' in line:\n",
        "                key,value = line.strip().split(':', 1)\n",
        "                if value.isdigit():\n",
        "                    data[key] = int(value)\n",
        "                elif is_float(value):\n",
        "                    data[key] = float(value)\n",
        "                elif value == 'None':\n",
        "                    data[key] = None\n",
        "                else:\n",
        "                    data[key] = value\n",
        "            else:\n",
        "                pass # deal with bad lines of text here    \n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "majCm4a7AVvQ"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "##### USER-DEFINED FUNCTIONS\n",
        "def f_get_fc_mask1(meas_time, num_Event, num_Category):\n",
        "    '''\n",
        "        mask1 is required to get the contional probability (to calculate the denominator part)\n",
        "        mask1 size is [N, num_Event, num_Category]. 1's until the last measurement time\n",
        "    '''\n",
        "    mask = np.zeros([np.shape(meas_time)[0], num_Event, num_Category]) # for denominator\n",
        "    for i in range(np.shape(meas_time)[0]):\n",
        "        mask[i, :, :int(meas_time[i, 0]+1)] = 1 # last measurement time\n",
        "\n",
        "    return mask\n",
        "\n",
        "def f_get_minibatch(mb_size, x, x_mi, label, time, mask1, mask2, mask3):\n",
        "    idx = range(np.shape(x)[0])\n",
        "    idx = random.sample(idx, mb_size)\n",
        "\n",
        "    x_mb     = x[idx, :, :].astype(float)\n",
        "    x_mi_mb  = x_mi[idx, :, :].astype(float)\n",
        "    k_mb     = label[idx, :].astype(float) # censoring(0)/event(1,2,..) label\n",
        "    t_mb     = time[idx, :].astype(float)\n",
        "    m1_mb    = mask1[idx, :, :].astype(float) #fc_mask\n",
        "    m2_mb    = mask2[idx, :, :].astype(float) #fc_mask\n",
        "    m3_mb    = mask3[idx, :].astype(float) #fc_mask\n",
        "    return x_mb, x_mi_mb, k_mb, t_mb, m1_mb, m2_mb, m3_mb\n",
        "\n",
        "\n",
        "def f_get_boosted_trainset(x, x_mi, time, label, mask1, mask2, mask3):\n",
        "    _, num_Event, num_Category  = np.shape(mask1)  # dim of mask3: [subj, Num_Event, Num_Category]\n",
        "    meas_time = np.concatenate([np.zeros([np.shape(x)[0], 1]), np.cumsum(x[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
        "\n",
        "    total_sample = 0\n",
        "    for i in range(np.shape(x)[0]):\n",
        "        total_sample += np.sum(np.sum(x[i], axis=1) != 0)\n",
        "\n",
        "    new_label          = np.zeros([total_sample, np.shape(label)[1]])\n",
        "    new_time           = np.zeros([total_sample, np.shape(time)[1]])\n",
        "    new_x              = np.zeros([total_sample, np.shape(x)[1], np.shape(x)[2]])\n",
        "    new_x_mi           = np.zeros([total_sample, np.shape(x_mi)[1], np.shape(x_mi)[2]])\n",
        "    new_mask1          = np.zeros([total_sample, np.shape(mask1)[1], np.shape(mask1)[2]])\n",
        "    new_mask2          = np.zeros([total_sample, np.shape(mask2)[1], np.shape(mask2)[2]])\n",
        "    new_mask3          = np.zeros([total_sample, np.shape(mask3)[1]])\n",
        "\n",
        "    tmp_idx = 0\n",
        "    for i in range(np.shape(x)[0]):\n",
        "        max_meas = np.sum(np.sum(x[i], axis=1) != 0)\n",
        "\n",
        "        for t in range(max_meas):\n",
        "            new_label[tmp_idx+t, 0] = label[i,0]\n",
        "            new_time[tmp_idx+t, 0]  = time[i,0]\n",
        "\n",
        "            new_x[tmp_idx+t,:(t+1), :] = x[i,:(t+1), :]\n",
        "            new_x_mi[tmp_idx+t,:(t+1), :] = x_mi[i,:(t+1), :]\n",
        "\n",
        "            new_mask1[tmp_idx+t, :, :] = f_get_fc_mask1(meas_time[i,t].reshape([-1,1]), num_Event, num_Category) #age at the measurement\n",
        "            new_mask2[tmp_idx+t, :, :] = mask2[i, :, :]\n",
        "            new_mask3[tmp_idx+t, :]    = mask3[i, :]\n",
        "\n",
        "        tmp_idx += max_meas\n",
        "        \n",
        "    return(new_x, new_x_mi, new_time, new_label, new_mask1, new_mask2, new_mask3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCAKCiMSAX5i"
      },
      "source": [
        "'''\n",
        "Implemented: 02/12/2018\n",
        "  > For survival analysis evaluation\n",
        "\n",
        "First implemented by Kartik Ahuja\n",
        "Modified by CHANGHEE LEE\n",
        "\n",
        "Modifcation List:\n",
        "\t- 08/08/2018: Brier Score added\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "from lifelines import KaplanMeierFitter\n",
        "\n",
        "\n",
        "### C(t)-INDEX CALCULATION\n",
        "def c_index(Prediction, Time_survival, Death, Time):\n",
        "    '''\n",
        "        This is a cause-specific c(t)-index\n",
        "        - Prediction      : risk at Time (higher --> more risky)\n",
        "        - Time_survival   : survival/censoring time\n",
        "        - Death           :\n",
        "            > 1: death\n",
        "            > 0: censored (including death from other cause)\n",
        "        - Time            : time of evaluation (time-horizon when evaluating C-index)\n",
        "    '''\n",
        "    N = len(Prediction)\n",
        "    A = np.zeros((N,N))\n",
        "    Q = np.zeros((N,N))\n",
        "    N_t = np.zeros((N,N))\n",
        "    Num = 0\n",
        "    Den = 0\n",
        "    for i in range(N):\n",
        "        A[i, np.where(Time_survival[i] < Time_survival)] = 1\n",
        "        Q[i, np.where(Prediction[i] > Prediction)] = 1\n",
        "  \n",
        "        if (Time_survival[i]<=Time and Death[i]==1):\n",
        "            N_t[i,:] = 1\n",
        "\n",
        "    Num  = np.sum(((A)*N_t)*Q)\n",
        "    Den  = np.sum((A)*N_t)\n",
        "\n",
        "    if Num == 0 and Den == 0:\n",
        "        result = -1 # not able to compute c-index!\n",
        "    else:\n",
        "        result = float(Num/Den)\n",
        "\n",
        "    return result\n",
        "\n",
        "### BRIER-SCORE\n",
        "def brier_score(Prediction, Time_survival, Death, Time):\n",
        "    N = len(Prediction)\n",
        "    y_true = ((Time_survival <= Time) * Death).astype(float)\n",
        "\n",
        "    return np.mean((Prediction - y_true)**2)\n",
        "\n",
        "    # result2[k, t] = brier_score_loss(risk[:, k], ((te_time[:,0] <= eval_horizon) * (te_label[:,0] == k+1)).astype(int))\n",
        "\n",
        "\n",
        "##### WEIGHTED C-INDEX & BRIER-SCORE\n",
        "def CensoringProb(Y, T):\n",
        "\n",
        "    T = T.reshape([-1]) # (N,) - np array\n",
        "    Y = Y.reshape([-1]) # (N,) - np array\n",
        "\n",
        "    kmf = KaplanMeierFitter()\n",
        "    kmf.fit(T, event_observed=(Y==0).astype(int))  # censoring prob = survival probability of event \"censoring\"\n",
        "    G = np.asarray(kmf.survival_function_.reset_index()).transpose()\n",
        "    G[1, G[1, :] == 0] = G[1, G[1, :] != 0][-1]  #fill 0 with ZoH (to prevent nan values)\n",
        "    \n",
        "    return G\n",
        "\n",
        "\n",
        "\n",
        "### C(t)-INDEX CALCULATION\n",
        "def weighted_c_index(T_train, Y_train, Prediction, T_test, Y_test, Time):\n",
        "    '''\n",
        "        This is a cause-specific c(t)-index\n",
        "        - Prediction      : risk at Time (higher --> more risky)\n",
        "        - Time_survival   : survival/censoring time\n",
        "        - Death           :\n",
        "            > 1: death\n",
        "            > 0: censored (including death from other cause)\n",
        "        - Time            : time of evaluation (time-horizon when evaluating C-index)\n",
        "    '''\n",
        "    G = CensoringProb(Y_train, T_train)\n",
        "\n",
        "    N = len(Prediction)\n",
        "    A = np.zeros((N,N))\n",
        "    Q = np.zeros((N,N))\n",
        "    N_t = np.zeros((N,N))\n",
        "    Num = 0\n",
        "    Den = 0\n",
        "    for i in range(N):\n",
        "        tmp_idx = np.where(G[0,:] >= T_test[i])[0]\n",
        "\n",
        "        if len(tmp_idx) == 0:\n",
        "            W = (1./G[1, -1])**2\n",
        "        else:\n",
        "            W = (1./G[1, tmp_idx[0]])**2\n",
        "\n",
        "        A[i, np.where(T_test[i] < T_test)] = 1. * W\n",
        "        Q[i, np.where(Prediction[i] > Prediction)] = 1. # give weights\n",
        "\n",
        "        if (T_test[i]<=Time and Y_test[i]==1):\n",
        "            N_t[i,:] = 1.\n",
        "\n",
        "    Num  = np.sum(((A)*N_t)*Q)\n",
        "    Den  = np.sum((A)*N_t)\n",
        "\n",
        "    if Num == 0 and Den == 0:\n",
        "        result = -1 # not able to compute c-index!\n",
        "    else:\n",
        "        result = float(Num/Den)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def weighted_brier_score(T_train, Y_train, Prediction, T_test, Y_test, Time):\n",
        "    G = CensoringProb(Y_train, T_train)\n",
        "    N = len(Prediction)\n",
        "\n",
        "    W = np.zeros(len(Y_test))\n",
        "    Y_tilde = (T_test > Time).astype(float)\n",
        "\n",
        "    for i in range(N):\n",
        "        tmp_idx1 = np.where(G[0,:] >= T_test[i])[0]\n",
        "        tmp_idx2 = np.where(G[0,:] >= Time)[0]\n",
        "\n",
        "        if len(tmp_idx1) == 0:\n",
        "            G1 = G[1, -1]\n",
        "        else:\n",
        "            G1 = G[1, tmp_idx1[0]]\n",
        "\n",
        "        if len(tmp_idx2) == 0:\n",
        "            G2 = G[1, -1]\n",
        "        else:\n",
        "            G2 = G[1, tmp_idx2[0]]\n",
        "        W[i] = (1. - Y_tilde[i])*float(Y_test[i])/G1 + Y_tilde[i]/G2\n",
        "\n",
        "    y_true = ((T_test <= Time) * Y_test).astype(float)\n",
        "\n",
        "    return np.mean(W*(Y_tilde - (1.-Prediction))**2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8MRZ7IP_1B4"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "from tensorflow.contrib.layers import fully_connected as FC_Net\n",
        "from tensorflow.python.ops.rnn import _transpose_batch_time\n",
        "\n",
        "\n",
        "#import utils_network as utils\n",
        "\n",
        "_EPSILON = 1e-08\n",
        "\n",
        "\n",
        "\n",
        "##### USER-DEFINED FUNCTIONS\n",
        "def log(x):\n",
        "    return tf.log(x + _EPSILON)\n",
        "\n",
        "def div(x, y):\n",
        "    return tf.div(x, (y + _EPSILON))\n",
        "\n",
        "def get_seq_length(sequence):\n",
        "    used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
        "    tmp_length = tf.reduce_sum(used, 1)\n",
        "    tmp_length = tf.cast(tmp_length, tf.int32)\n",
        "    return tmp_length\n",
        "\n",
        "\n",
        "class Model_Longitudinal_Attention:\n",
        "    # def __init__(self, sess, name, mb_size, input_dims, network_settings):\n",
        "    def __init__(self, sess, name, input_dims, network_settings):\n",
        "        self.sess               = sess\n",
        "        self.name               = name\n",
        "\n",
        "        # INPUT DIMENSIONS\n",
        "        self.x_dim              = input_dims['x_dim']\n",
        "        self.x_dim_cont         = input_dims['x_dim_cont']\n",
        "        self.x_dim_bin          = input_dims['x_dim_bin']\n",
        "\n",
        "        self.num_Event          = input_dims['num_Event']\n",
        "        self.num_Category       = input_dims['num_Category']\n",
        "        self.max_length         = input_dims['max_length']\n",
        "\n",
        "        # NETWORK HYPER-PARMETERS\n",
        "        self.h_dim1             = network_settings['h_dim_RNN']\n",
        "        self.h_dim2             = network_settings['h_dim_FC']\n",
        "        self.num_layers_RNN     = network_settings['num_layers_RNN']\n",
        "        self.num_layers_ATT     = network_settings['num_layers_ATT']\n",
        "        self.num_layers_CS      = network_settings['num_layers_CS']\n",
        "\n",
        "        self.RNN_type           = network_settings['RNN_type']\n",
        "\n",
        "        self.FC_active_fn       = network_settings['FC_active_fn']\n",
        "        self.RNN_active_fn      = network_settings['RNN_active_fn']\n",
        "        self.initial_W          = network_settings['initial_W']\n",
        "        \n",
        "        self.reg_W              = tf.contrib.layers.l1_regularizer(scale=network_settings['reg_W'])\n",
        "        self.reg_W_out          = tf.contrib.layers.l1_regularizer(scale=network_settings['reg_W_out'])\n",
        "\n",
        "        self._build_net()\n",
        "\n",
        "\n",
        "    def _build_net(self):\n",
        "        with tf.variable_scope(self.name):\n",
        "            #### PLACEHOLDER DECLARATION\n",
        "            self.mb_size     = tf.placeholder(tf.int32, [], name='batch_size')\n",
        "\n",
        "            self.lr_rate     = tf.placeholder(tf.float32)\n",
        "            self.keep_prob   = tf.placeholder(tf.float32)                                                      #keeping rate\n",
        "            self.a           = tf.placeholder(tf.float32)\n",
        "            self.b           = tf.placeholder(tf.float32)\n",
        "            self.c           = tf.placeholder(tf.float32)\n",
        "\n",
        "            self.x           = tf.placeholder(tf.float32, shape=[None, self.max_length, self.x_dim])\n",
        "            self.x_mi        = tf.placeholder(tf.float32, shape=[None, self.max_length, self.x_dim])           \n",
        "            #this is the missing indicator (including for cont. & binary) (includes delta)\n",
        "            self.k           = tf.placeholder(tf.float32, shape=[None, 1])                                     #event/censoring label (censoring:0)\n",
        "            self.t           = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "\n",
        "\n",
        "            self.fc_mask1    = tf.placeholder(tf.float32, shape=[None, self.num_Event, self.num_Category])     #for denominator\n",
        "            self.fc_mask2    = tf.placeholder(tf.float32, shape=[None, self.num_Event, self.num_Category])     #for Loss 1\n",
        "            self.fc_mask3    = tf.placeholder(tf.float32, shape=[None, self.num_Category])                     #for Loss 2\n",
        "\n",
        "            \n",
        "            seq_length     = get_seq_length(self.x)\n",
        "            tmp_range      = tf.expand_dims(tf.range(0, self.max_length, 1), axis=0)\n",
        "            \n",
        "            self.rnn_mask1 = tf.cast(tf.less_equal(tmp_range, tf.expand_dims(seq_length - 1, axis=1)), tf.float32)            \n",
        "            self.rnn_mask2 = tf.cast(tf.equal(tmp_range, tf.expand_dims(seq_length - 1, axis=1)), tf.float32) \n",
        "            \n",
        "            \n",
        "            ### DEFINE LOOP FUNCTION FOR RAW_RNN w/ TEMPORAL ATTENTION\n",
        "            def loop_fn_att(time, cell_output, cell_state, loop_state):\n",
        "\n",
        "                emit_output = cell_output \n",
        "\n",
        "                if cell_output is None:  # time == 0\n",
        "                    next_cell_state = cell.zero_state(self.mb_size, tf.float32)\n",
        "                    next_loop_state = loop_state_ta\n",
        "                else:\n",
        "                    next_cell_state = cell_state\n",
        "                    tmp_h = create_concat_state(next_cell_state, self.num_layers_RNN, self.RNN_type)\n",
        "\n",
        "                    e = create_FCNet(tf.concat([tmp_h, all_last], axis=1), self.num_layers_ATT, self.h_dim2, \n",
        "                                           tf.nn.tanh, 1, None, self.initial_W, keep_prob=self.keep_prob)\n",
        "                    e = tf.exp(e)\n",
        "\n",
        "                    next_loop_state = (loop_state[0].write(time-1, e),                # save att power (e_{j})\n",
        "                                       loop_state[1].write(time-1, tmp_h))  # save all the hidden states\n",
        "\n",
        "                # elements_finished = (time >= seq_length)\n",
        "                elements_finished = (time >= self.max_length-1)\n",
        "\n",
        "                #this gives the break-point (no more recurrence after the max_length)\n",
        "                finished = tf.reduce_all(elements_finished)    \n",
        "                next_input = tf.cond(finished, lambda: tf.zeros([self.mb_size, 2*self.x_dim], dtype=tf.float32),  # [x_hist, mi_hist]\n",
        "                                               lambda: inputs_ta.read(time))\n",
        "\n",
        "                return (elements_finished, next_input, next_cell_state, emit_output, next_loop_state)\n",
        "\n",
        "\n",
        "            \n",
        "            # divide into the last x and previous x's\n",
        "            x_last = tf.slice(self.x, [0,(self.max_length-1), 1], [-1,-1,-1])      #current measurement\n",
        "            x_last = tf.reshape(x_last, [-1, (self.x_dim_cont+self.x_dim_bin)])    #remove the delta of the last measurement\n",
        "\n",
        "            x_last = tf.reduce_sum(tf.tile(tf.expand_dims(self.rnn_mask2, axis=2), [1,1,self.x_dim]) * self.x, reduction_indices=1)    #sum over time since all others time stamps are 0\n",
        "            x_last = tf.slice(x_last, [0,1], [-1,-1])                               #remove the delta of the last measurement\n",
        "            x_hist = self.x * (1.-tf.tile(tf.expand_dims(self.rnn_mask2, axis=2), [1,1,self.x_dim]))                                    #since all others time stamps are 0 and measurements are 0-padded\n",
        "            x_hist = tf.slice(x_hist, [0, 0, 0], [-1,(self.max_length-1),-1])  \n",
        "\n",
        "            # do same thing for missing indicator\n",
        "            mi_last = tf.slice(self.x_mi, [0,(self.max_length-1), 1], [-1,-1,-1])      #current measurement\n",
        "            mi_last = tf.reshape(mi_last, [-1, (self.x_dim_cont+self.x_dim_bin)])    #remove the delta of the last measurement\n",
        "\n",
        "            mi_last = tf.reduce_sum(tf.tile(tf.expand_dims(self.rnn_mask2, axis=2), [1,1,self.x_dim]) * self.x_mi, reduction_indices=1)    #sum over time since all others time stamps are 0\n",
        "            mi_last = tf.slice(mi_last, [0,1], [-1,-1])                               #remove the delta of the last measurement\n",
        "            mi_hist = self.x_mi * (1.-tf.tile(tf.expand_dims(self.rnn_mask2, axis=2), [1,1,self.x_dim]))                                    #since all others time stamps are 0 and measurements are 0-padded\n",
        "            mi_hist = tf.slice(mi_hist, [0, 0, 0], [-1,(self.max_length-1),-1])  \n",
        "\n",
        "            all_hist = tf.concat([x_hist, mi_hist], axis=2)\n",
        "            all_last = tf.concat([x_last, mi_last], axis=1)\n",
        "\n",
        "\n",
        "            #extract inputs for the temporal attention: mask (to incorporate only the measured time) and x_{M}\n",
        "            seq_length     = get_seq_length(x_hist)\n",
        "            rnn_mask_att   = tf.cast(tf.not_equal(tf.reduce_sum(x_hist, reduction_indices=2), 0), dtype=tf.float32)  #[mb_size, max_length-1], 1:measurements 0:no measurements\n",
        "            \n",
        "\n",
        "            ##### SHARED SUBNETWORK: RNN w/ TEMPORAL ATTENTION\n",
        "            #change the input tensor to TensorArray format with [max_length, mb_size, x_dim]\n",
        "            inputs_ta = tf.TensorArray(dtype=tf.float32, size=self.max_length-1).unstack(_transpose_batch_time(all_hist), name = 'Shared_Input')\n",
        "\n",
        "\n",
        "            #create a cell with RNN hyper-parameters (RNN types, #layers, #nodes, activation functions, keep proability)\n",
        "            cell = create_rnn_cell(self.h_dim1, self.num_layers_RNN, self.keep_prob, \n",
        "                                         self.RNN_type, self.RNN_active_fn)\n",
        "\n",
        "            #define the loop_state TensorArray for information from rnn time steps\n",
        "            loop_state_ta = (tf.TensorArray(size=self.max_length-1, dtype=tf.float32),  #e values (e_{j})\n",
        "                             tf.TensorArray(size=self.max_length-1, dtype=tf.float32))  #hidden states (h_{j})\n",
        "            \n",
        "            rnn_outputs_ta, self.rnn_final_state, loop_state_ta = tf.nn.raw_rnn(cell, loop_fn_att)\n",
        "            #rnn_outputs_ta  : TensorArray\n",
        "            #rnn_final_state : Tensor\n",
        "            #rnn_states_ta   : (TensorArray, TensorArray)\n",
        "\n",
        "            rnn_outputs = _transpose_batch_time(rnn_outputs_ta.stack())\n",
        "            # rnn_outputs =  tf.reshape(rnn_outputs, [-1, self.max_length-1, self.h_dim1])\n",
        "\n",
        "            rnn_states  = _transpose_batch_time(loop_state_ta[1].stack())\n",
        "\n",
        "            att_weight  = _transpose_batch_time(loop_state_ta[0].stack()) #e_{j}\n",
        "            att_weight  = tf.reshape(att_weight, [-1, self.max_length-1]) * rnn_mask_att # masking to set 0 for the unmeasured e_{j}\n",
        "\n",
        "            #get a_{j} = e_{j}/sum_{l=1}^{M-1}e_{l}\n",
        "            self.att_weight  = div(att_weight,(tf.reduce_sum(att_weight, axis=1, keepdims=True) + _EPSILON)) #softmax (tf.exp is done, previously)\n",
        "\n",
        "            # 1) expand att_weight to hidden state dimension, 2) c = \\sum_{j=1}^{M} a_{j} x h_{j}\n",
        "            self.context_vec = tf.reduce_sum(tf.tile(tf.reshape(self.att_weight, [-1, self.max_length-1, 1]), [1, 1, self.num_layers_RNN*self.h_dim1]) * rnn_states, axis=1)\n",
        "\n",
        "\n",
        "            self.z_mean      = FC_Net(rnn_outputs, self.x_dim, activation_fn=None, weights_initializer=self.initial_W, scope=\"RNN_out_mean1\")\n",
        "            self.z_std       = tf.exp(FC_Net(rnn_outputs, self.x_dim, activation_fn=None, weights_initializer=self.initial_W, scope=\"RNN_out_std1\"))\n",
        "\n",
        "            epsilon          = tf.random_normal([self.mb_size, self.max_length-1, self.x_dim], mean=0.0, stddev=1.0, dtype=tf.float32)\n",
        "            self.z           = self.z_mean + self.z_std * epsilon\n",
        "\n",
        "            \n",
        "            ##### CS-SPECIFIC SUBNETWORK w/ FCNETS \n",
        "            inputs = tf.concat([x_last, self.context_vec], axis=1)\n",
        "\n",
        "\n",
        "            #1 layer for combining inputs\n",
        "            h = FC_Net(inputs, self.h_dim2, activation_fn=self.FC_active_fn, weights_initializer=self.initial_W, scope=\"Layer1\")\n",
        "            h = tf.nn.dropout(h, keep_prob=self.keep_prob)\n",
        "\n",
        "            # (num_layers_CS-1) layers for cause-specific (num_Event subNets)\n",
        "            out = []\n",
        "            for _ in range(self.num_Event):\n",
        "                cs_out = create_FCNet(h, (self.num_layers_CS), self.h_dim2, self.FC_active_fn, self.h_dim2, self.FC_active_fn, self.initial_W, self.reg_W, self.keep_prob)\n",
        "                out.append(cs_out)\n",
        "            out = tf.stack(out, axis=1) # stack referenced on subject\n",
        "            out = tf.reshape(out, [-1, self.num_Event*self.h_dim2])\n",
        "            out = tf.nn.dropout(out, keep_prob=self.keep_prob)\n",
        "\n",
        "            out = FC_Net(out, self.num_Event * self.num_Category, activation_fn=tf.nn.softmax, \n",
        "                         weights_initializer=self.initial_W, weights_regularizer=self.reg_W_out, scope=\"Output\")\n",
        "            self.out = tf.reshape(out, [-1, self.num_Event, self.num_Category])\n",
        "\n",
        "\n",
        "            ##### GET LOSS FUNCTIONS\n",
        "            self.loss_Log_Likelihood()      #get loss1: Log-Likelihood loss\n",
        "            self.loss_Ranking()             #get loss2: Ranking loss\n",
        "            self.loss_RNN_Prediction()      #get loss3: RNN prediction loss\n",
        "\n",
        "            self.LOSS_TOTAL     = self.a*self.LOSS_1 + self.b*self.LOSS_2 + self.c*self.LOSS_3 + tf.losses.get_regularization_loss()\n",
        "            self.LOSS_BURNIN    = self.LOSS_3 + tf.losses.get_regularization_loss()\n",
        "\n",
        "            self.solver         = tf.train.AdamOptimizer(learning_rate=self.lr_rate).minimize(self.LOSS_TOTAL)\n",
        "            self.solver_burn_in = tf.train.AdamOptimizer(learning_rate=self.lr_rate).minimize(self.LOSS_BURNIN)\n",
        "\n",
        "\n",
        "    ### LOSS-FUNCTION 1 -- Log-likelihood loss\n",
        "    def loss_Log_Likelihood(self):\n",
        "        sigma3 = tf.constant(1.0, dtype=tf.float32)\n",
        "\n",
        "        I_1 = tf.sign(self.k)\n",
        "        denom = 1 - tf.reduce_sum(tf.reduce_sum(self.fc_mask1 * self.out, reduction_indices=2), reduction_indices=1, keepdims=True) # make subject specific denom.\n",
        "        denom = tf.clip_by_value(denom, tf.cast(_EPSILON, dtype=tf.float32), tf.cast(1.-_EPSILON, dtype=tf.float32))\n",
        "\n",
        "        #for uncenosred: log P(T=t,K=k|x,Y,t>t_M)\n",
        "        tmp1 = tf.reduce_sum(tf.reduce_sum(self.fc_mask2 * self.out, reduction_indices=2), reduction_indices=1, keepdims=True)\n",
        "        tmp1 = I_1 * log(div(tmp1,denom))\n",
        "\n",
        "        #for censored: log \\sum P(T>t|x,Y,t>t_M)\n",
        "        tmp2 = tf.reduce_sum(tf.reduce_sum(self.fc_mask2 * self.out, reduction_indices=2), reduction_indices=1, keepdims=True)\n",
        "        tmp2 = (1. - I_1) * log(div(tmp2,denom))\n",
        "\n",
        "        self.LOSS_1 = - tf.reduce_mean(tmp1 + sigma3*tmp2)\n",
        "\n",
        "\n",
        "    ### LOSS-FUNCTION 2 -- Ranking loss\n",
        "    def loss_Ranking(self):\n",
        "        sigma1 = tf.constant(0.1, dtype=tf.float32)\n",
        "\n",
        "        eta = []\n",
        "        for e in range(self.num_Event):\n",
        "            one_vector = tf.ones_like(self.t, dtype=tf.float32)\n",
        "            I_2 = tf.cast(tf.equal(self.k, e+1), dtype = tf.float32) #indicator for event\n",
        "            I_2 = tf.diag(tf.squeeze(I_2))\n",
        "            tmp_e = tf.reshape(tf.slice(self.out, [0, e, 0], [-1, 1, -1]), [-1, self.num_Category]) #event specific joint prob.\n",
        "\n",
        "            R = tf.matmul(tmp_e, tf.transpose(self.fc_mask3)) #no need to divide by each individual dominator\n",
        "            # r_{ij} = risk of i-th pat based on j-th time-condition (last meas. time ~ event time) , i.e. r_i(T_{j})\n",
        "\n",
        "            diag_R = tf.reshape(tf.diag_part(R), [-1, 1])\n",
        "            R = tf.matmul(one_vector, tf.transpose(diag_R)) - R # R_{ij} = r_{j}(T_{j}) - r_{i}(T_{j})\n",
        "            R = tf.transpose(R)                                 # Now, R_{ij} (i-th row j-th column) = r_{i}(T_{i}) - r_{j}(T_{i})\n",
        "\n",
        "            T = tf.nn.relu(tf.sign(tf.matmul(one_vector, tf.transpose(self.t)) - tf.matmul(self.t, tf.transpose(one_vector))))\n",
        "            # T_{ij}=1 if t_i < t_j  and T_{ij}=0 if t_i >= t_j\n",
        "\n",
        "            T = tf.matmul(I_2, T) # only remains T_{ij}=1 when event occured for subject i\n",
        "\n",
        "            tmp_eta = tf.reduce_mean(T * tf.exp(-R/sigma1), reduction_indices=1, keepdims=True)\n",
        "\n",
        "            eta.append(tmp_eta)\n",
        "        eta = tf.stack(eta, axis=1) #stack referenced on subjects\n",
        "        eta = tf.reduce_mean(tf.reshape(eta, [-1, self.num_Event]), reduction_indices=1, keepdims=True)\n",
        "\n",
        "        self.LOSS_2 = tf.reduce_sum(eta) #sum over num_Events\n",
        "\n",
        "\n",
        "    ### LOSS-FUNCTION 3 -- RNN prediction loss\n",
        "    def loss_RNN_Prediction(self):\n",
        "        tmp_x  = tf.slice(self.x, [0,1,0], [-1,-1,-1])  # (t=2 ~ M)\n",
        "        tmp_mi = tf.slice(self.x_mi, [0,1,0], [-1,-1,-1])  # (t=2 ~ M)\n",
        "\n",
        "        tmp_mask1  = tf.tile(tf.expand_dims(self.rnn_mask1, axis=2), [1,1,self.x_dim]) #for hisotry (1...J-1)\n",
        "        tmp_mask1  = tmp_mask1[:, :(self.max_length-1), :] \n",
        "\n",
        "        zeta = tf.reduce_mean(tf.reduce_sum(tmp_mask1 * (1. - tmp_mi) * tf.pow(self.z - tmp_x, 2), reduction_indices=1))  #loss calculated for selected features.\n",
        "\n",
        "        self.LOSS_3 = zeta\n",
        "\n",
        " \n",
        "    def get_cost(self, DATA, MASK, MISSING, PARAMETERS, keep_prob, lr_train):\n",
        "        (x_mb, k_mb, t_mb)        = DATA\n",
        "        (m1_mb, m2_mb, m3_mb)     = MASK\n",
        "        (x_mi_mb)                 = MISSING\n",
        "        (alpha, beta, gamma)      = PARAMETERS\n",
        "        return self.sess.run(self.LOSS_TOTAL, \n",
        "                             feed_dict={self.x:x_mb, self.x_mi: x_mi_mb, self.k:k_mb, self.t:t_mb, \n",
        "                                        self.fc_mask1: m1_mb, self.fc_mask2:m2_mb, self.fc_mask3: m3_mb, \n",
        "                                        self.a:alpha, self.b:beta, self.c:gamma,\n",
        "                                        self.mb_size: np.shape(x_mb)[0], self.keep_prob:keep_prob, self.lr_rate:lr_train})\n",
        "\n",
        "    def train(self, DATA, MASK, MISSING, PARAMETERS, keep_prob, lr_train):\n",
        "        (x_mb, k_mb, t_mb)        = DATA\n",
        "        (m1_mb, m2_mb, m3_mb)     = MASK\n",
        "        (x_mi_mb)                 = MISSING\n",
        "        (alpha, beta, gamma)      = PARAMETERS\n",
        "        return self.sess.run([self.solver, self.LOSS_TOTAL], \n",
        "                             feed_dict={self.x:x_mb, self.x_mi: x_mi_mb, self.k:k_mb, self.t:t_mb,\n",
        "                                        self.fc_mask1: m1_mb, self.fc_mask2:m2_mb, self.fc_mask3: m3_mb, \n",
        "                                        self.a:alpha, self.b:beta, self.c:gamma,\n",
        "                                        self.mb_size: np.shape(x_mb)[0], self.keep_prob:keep_prob, self.lr_rate:lr_train})\n",
        "    \n",
        "    def train_burn_in(self, DATA, MISSING, keep_prob, lr_train):\n",
        "        (x_mb, k_mb, t_mb)        = DATA\n",
        "        (x_mi_mb)                 = MISSING\n",
        "\n",
        "        return self.sess.run([self.solver_burn_in, self.LOSS_3], \n",
        "                             feed_dict={self.x:x_mb, self.x_mi: x_mi_mb, self.k:k_mb, self.t:t_mb, \n",
        "                                        self.mb_size: np.shape(x_mb)[0], self.keep_prob:keep_prob, self.lr_rate:lr_train})\n",
        "    \n",
        "    def predict(self, x_test, x_mi_test, keep_prob=1.0):\n",
        "        return self.sess.run(self.out, feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n",
        "\n",
        "    def predict_z(self, x_test, x_mi_test, keep_prob=1.0):\n",
        "        return self.sess.run(self.z, feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n",
        "\n",
        "    def predict_rnnstate(self, x_test, x_mi_test, keep_prob=1.0):\n",
        "        return self.sess.run(self.rnn_final_state, feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n",
        "\n",
        "    def predict_att(self, x_test, x_mi_test, keep_prob=1.0):\n",
        "        return self.sess.run(self.att_weight, feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n",
        "\n",
        "    def predict_context_vec(self, x_test, x_mi_test, keep_prob=1.0):\n",
        "        return self.sess.run(self.context_vec, feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n",
        "\n",
        "    def get_z_mean_and_std(self, x_test, x_mi_test, keep_prob=1.0):\n",
        "        return self.sess.run([self.z_mean, self.z_std], feed_dict={self.x: x_test, self.x_mi: x_mi_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ac153r69-AF"
      },
      "source": [
        "_EPSILON = 1e-08\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#import import_data as impt\n",
        "\n",
        "#from class_DeepLongitudinal import Model_Longitudinal_Attention\n",
        "\n",
        "#from utils_eval             import c_index, brier_score\n",
        "#from utils_log              import save_logging, load_logging\n",
        "#from utils_helper           import f_get_minibatch, f_get_boosted_trainset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwHrbOLy_jm6"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder as lbe\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2hBqfxi9-AJ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "##### USER-DEFINED FUNCTIONS\n",
        "def f_get_Normalization(X, norm_mode):    \n",
        "    num_Patient, num_Feature = np.shape(X)\n",
        "    \n",
        "    if norm_mode == 'standard': #zero mean unit variance\n",
        "        for j in range(num_Feature):\n",
        "            if np.nanstd(X[:,j]) != 0:\n",
        "                X[:,j] = (X[:,j] - np.nanmean(X[:, j]))/np.nanstd(X[:,j])\n",
        "            else:\n",
        "                X[:,j] = (X[:,j] - np.nanmean(X[:, j]))\n",
        "    elif norm_mode == 'normal': #min-max normalization\n",
        "        for j in range(num_Feature):\n",
        "            X[:,j] = (X[:,j] - np.nanmin(X[:,j]))/(np.nanmax(X[:,j]) - np.nanmin(X[:,j]))\n",
        "    else:\n",
        "        print(\"INPUT MODE ERROR!\")\n",
        "    \n",
        "    return X\n",
        "\n",
        "\n",
        "def f_get_fc_mask1(meas_time, num_Event, num_Category):\n",
        "    '''\n",
        "        mask3 is required to get the contional probability (to calculate the denominator part)\n",
        "        mask3 size is [N, num_Event, num_Category]. 1's until the last measurement time\n",
        "    '''\n",
        "    mask = np.zeros([np.shape(meas_time)[0], num_Event, num_Category]) # for denominator\n",
        "    for i in range(np.shape(meas_time)[0]):\n",
        "        mask[i, :, :int(meas_time[i, 0]+1)] = 1 # last measurement time\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "def f_get_fc_mask2(time, label, num_Event, num_Category):\n",
        "    '''\n",
        "        mask4 is required to get the log-likelihood loss \n",
        "        mask4 size is [N, num_Event, num_Category]\n",
        "            if not censored : one element = 1 (0 elsewhere)\n",
        "            if censored     : fill elements with 1 after the censoring time (for all events)\n",
        "    '''\n",
        "    mask = np.zeros([np.shape(time)[0], num_Event, num_Category]) # for the first loss function\n",
        "    for i in range(np.shape(time)[0]):\n",
        "        if label[i,0] != 0:  #not censored\n",
        "            mask[i,int(label[i,0]-1),int(time[i,0])] = 1\n",
        "        else: #label[i,2]==0: censored\n",
        "            mask[i,:,int(time[i,0]+1):] =  1 #fill 1 until from the censoring time (to get 1 - \\sum F)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def f_get_fc_mask3(time, meas_time, num_Category):\n",
        "    '''\n",
        "        mask5 is required calculate the ranking loss (for pair-wise comparision)\n",
        "        mask5 size is [N, num_Category]. \n",
        "        - For longitudinal measurements:\n",
        "             1's from the last measurement to the event time (exclusive and inclusive, respectively)\n",
        "             denom is not needed since comparing is done over the same denom\n",
        "        - For single measurement:\n",
        "             1's from start to the event time(inclusive)\n",
        "    '''\n",
        "    mask = np.zeros([np.shape(time)[0], num_Category]) # for the first loss function\n",
        "    if np.shape(meas_time):  #lonogitudinal measurements \n",
        "        for i in range(np.shape(time)[0]):\n",
        "            t1 = int(meas_time[i, 0]) # last measurement time\n",
        "            t2 = int(time[i, 0]) # censoring/event time\n",
        "            mask[i,(t1+1):(t2+1)] = 1  #this excludes the last measurement time and includes the event time\n",
        "    else:                    #single measurement\n",
        "        for i in range(np.shape(time)[0]):\n",
        "            t = int(time[i, 0]) # censoring/event time\n",
        "            mask[i,:(t+1)] = 1  #this excludes the last measurement time and includes the event time\n",
        "    return mask\n",
        "\n",
        "\n",
        "\n",
        "##### TRANSFORMING DATA\n",
        "def f_construct_dataset(df, feat_list):\n",
        "    '''\n",
        "        id   : patient indicator\n",
        "        tte  : time-to-event or time-to-censoring\n",
        "            - must be synchronized based on the reference time\n",
        "        time: time at which observations are measured\n",
        "            - must be synchronized based on the reference time (i.e., times start from 0)\n",
        "        label: event/censoring information\n",
        "            - 0: censoring\n",
        "            - 1: event type 1\n",
        "            - 2: event type 2\n",
        "            ...\n",
        "    '''\n",
        "\n",
        "    grouped  = df.groupby(['ID'])\n",
        "    id_list  = pd.unique(df['ID'])\n",
        "    max_meas = np.max(grouped.count())[0]\n",
        "\n",
        "    data     = np.zeros([len(id_list), max_meas, len(feat_list)+1])\n",
        "    pat_info = np.zeros([len(id_list), 5])\n",
        "\n",
        "    for i, tmp_id in enumerate(id_list):\n",
        "        tmp = grouped.get_group(tmp_id).reset_index(drop=True)\n",
        "\n",
        "        pat_info[i,4] = tmp.shape[0]                                   #number of measurement\n",
        "        pat_info[i,3] = np.max(tmp['time'])     #last measurement time\n",
        "        pat_info[i,2] = tmp['label'][0]      #cause\n",
        "        pat_info[i,1] = tmp['tte'][0]         #time_to_event\n",
        "        pat_info[i,0] = tmp['ID'][0]      \n",
        "\n",
        "        data[i, :int(pat_info[i, 4]), 1:]  = tmp[feat_list]\n",
        "        data[i, :int(pat_info[i, 4]-1), 0] = np.diff(tmp['time'])\n",
        "    \n",
        "    return pat_info, data\n",
        "\n",
        "\n",
        "def import_dataset(norm_mode = 'standard'):\n",
        "\n",
        "    df_  = pd.read_csv('/content/dataset_completo_modelli.txt' , sep = ' ')\n",
        "\n",
        "    df_['time'] = df_['tstart']\n",
        "\n",
        "    df_['label'] = df_['CVD1_COD_15']\n",
        "    for i in np.unique(df_['ID']):\n",
        "      if sum(df_.loc[df_.index[df_['ID']==i],'label'])>0:\n",
        "        df_.loc[df_.index[df_['ID']==i],'label'] = 1\n",
        "    df_['time_CVD1'] = np.float32(np.where( df_['time_CVD1'] == 'non evento' , 0 , df_['time_CVD1']))\n",
        "    df_['tte'] = np.where(df_['label'] == 1 , df_['time_CVD1'], df_['end'] )\n",
        "\n",
        "    df_[['SESSO']] = np.where(df_[['SESSO']]=='F',1,0) \n",
        "    df_[['anno_inizio_ARV']] = np.where(df_[['anno_inizio_ARV']]=='<=2007',1,0) \n",
        "    df_[['FDRn']] = np.where(df_[['FDRn']]=='MSM',1,0)\n",
        "    df_[['HCV']] = np.float32(np.where(df_[['HCV']]=='non noto',0,df_[['HCV']]))\n",
        "    df_[['HBV']] = np.float32(np.where(df_[['HBV']]=='non noto',0,df_[['HBV']]))\n",
        "\n",
        "    bin_list           = ['SESSO' , 'RAZZA', 'anno_inizio_ARV' , 'FDRn' , 'HCV' , 'HBV']\n",
        "    cont_list          = [ 'AGE_UPD' , \"diabete_si_no\" , 'ipert_si_no' , 'tumore_si_no'  , 'aids_si_no' , 'CD4' , \"COLEST\"  , 'VIREMIA' , 'Hb' , 'PLT' ,\n",
        "                            'TRIG' , 'CREATININA' , 'ALT' , 'AST' , 'INI_time_exp' , \"PI_time_exp\" , 'NRTI_time_exp' , 'NNRTI_time_exp']\n",
        "    feat_list          = cont_list + bin_list\n",
        "    df_                = df_[['ID', 'tte', 'time', 'label']  + feat_list]\n",
        "    df_ = df_.dropna()\n",
        "    df_org_            = df_.copy(deep=True)\n",
        "\n",
        "    df_[cont_list]     = f_get_Normalization(np.asarray(df_[cont_list]).astype(float), norm_mode)\n",
        "\n",
        "    pat_info, data     = f_construct_dataset(df_, feat_list)\n",
        "    _, data_org        = f_construct_dataset(df_org_, feat_list)\n",
        "\n",
        "    data_mi                  = np.zeros(np.shape(data))\n",
        "    data_mi[np.isnan(data)]  = 1\n",
        "    data_org[np.isnan(data)] = 0\n",
        "    data[np.isnan(data)]     = 0 \n",
        "\n",
        "    x_dim           = np.shape(data)[2] # 1 + x_dim_cont + x_dim_bin (including delta)\n",
        "    x_dim_cont      = len(cont_list)\n",
        "    x_dim_bin       = len(bin_list) \n",
        "\n",
        "    last_meas       = pat_info[:,[3]]  #pat_info[:, 3] contains age at the last measurement\n",
        "    label           = pat_info[:,[2]]  #two competing risks\n",
        "    time            = pat_info[:,[1]]  #age when event occurred\n",
        "\n",
        "    num_Category    = int(np.max(pat_info[:, 1]) * 1.2) #or specifically define larger than the max tte\n",
        "    num_Event       = len(np.unique(label)) - 1\n",
        "\n",
        "    if num_Event == 1:\n",
        "        label[np.where(label!=0)] = 1 #make single risk\n",
        "\n",
        "    mask1           = f_get_fc_mask1(last_meas, num_Event, num_Category)\n",
        "    mask2           = f_get_fc_mask2(time, label, num_Event, num_Category)\n",
        "    mask3           = f_get_fc_mask3(time, -1, num_Category)\n",
        "\n",
        "    DIM             = (x_dim, x_dim_cont, x_dim_bin)\n",
        "    DATA            = (data, time, label)\n",
        "    # DATA            = (data, data_org, time, label)\n",
        "    MASK            = (mask1, mask2, mask3)\n",
        "\n",
        "    return DIM, DATA, MASK, data_mi\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _f_get_pred(sess, model, data, data_mi, pred_horizon):\n",
        "    '''\n",
        "        predictions based on the prediction time.\n",
        "        create new_data and new_mask2 that are available previous or equal to the prediction time (no future measurements are used)\n",
        "    '''\n",
        "    new_data    = np.zeros(np.shape(data))\n",
        "    new_data_mi = np.zeros(np.shape(data_mi))\n",
        "\n",
        "    meas_time = np.concatenate([np.zeros([np.shape(data)[0], 1]), np.cumsum(data[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
        "\n",
        "    for i in range(np.shape(data)[0]):\n",
        "        last_meas = np.sum(meas_time[i, :] <= pred_horizon)\n",
        "\n",
        "        new_data[i, :last_meas, :]    = data[i, :last_meas, :]\n",
        "        new_data_mi[i, :last_meas, :] = data_mi[i, :last_meas, :]\n",
        "\n",
        "    return model.predict(new_data, new_data_mi)\n",
        "\n",
        "\n",
        "\n",
        "def f_get_risk_predictions(sess, model, data_, data_mi_, pred_time, eval_time):\n",
        "    \n",
        "    pred = _f_get_pred(sess, model, data_[[0]], data_mi_[[0]], 0)\n",
        "    _, num_Event, num_Category = np.shape(pred)\n",
        "       \n",
        "    risk_all = {}\n",
        "    \n",
        "    for k in range(num_Event):\n",
        "        risk_all[k] = np.zeros([np.shape(data_)[0], len(pred_time), len(eval_time)])\n",
        "            \n",
        "    for p, p_time in enumerate(pred_time):\n",
        "        ### PREDICTION\n",
        "        pred_horizon = int(p_time)\n",
        "        pred = _f_get_pred(sess, model, data_, data_mi_, pred_horizon)\n",
        "\n",
        "\n",
        "        for t, t_time in enumerate(eval_time):\n",
        "            eval_horizon = int(t_time) + pred_horizon #if eval_horizon >= num_Category, output the maximum...\n",
        "\n",
        "            # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
        "            risk = np.sum(pred[:,:,pred_horizon:(eval_horizon+1)], axis=2) #risk score until eval_time\n",
        "            risk = risk / (np.sum(np.sum(pred[:,:,pred_horizon:], axis=2), axis=1, keepdims=True) +_EPSILON) #conditioniong on t > t_pred\n",
        "            \n",
        "            for k in range(num_Event):\n",
        "                risk_all[k][:, p, t] = risk[:, k]\n",
        "                \n",
        "    return risk_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqoHn0Sj9-AN"
      },
      "source": [
        "### 1. Import Dataset\n",
        "#####      - Users must prepare dataset in csv format and modify 'import_data.py' following our examplar 'PBC2'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8PX0h5n9-AP",
        "outputId": "81d8a93e-ae6a-40b3-a7d6-4b59efa6b959"
      },
      "source": [
        "data_mode                   = 'PBC2' \n",
        "seed                        = 1234\n",
        "\n",
        "##### IMPORT DATASET\n",
        "'''\n",
        "    num_Category            = max event/censoring time * 1.2\n",
        "    num_Event               = number of evetns i.e. len(np.unique(label))-1\n",
        "    max_length              = maximum number of measurements\n",
        "    x_dim                   = data dimension including delta (1 + num_features)\n",
        "    x_dim_cont              = dim of continuous features\n",
        "    x_dim_bin               = dim of binary features\n",
        "    mask1, mask2, mask3     = used for cause-specific network (FCNet structure)\n",
        "'''\n",
        "\n",
        "if data_mode == 'PBC2':\n",
        "    (x_dim, x_dim_cont, x_dim_bin), (data, time, label), (mask1, mask2, mask3), (data_mi) = import_dataset(norm_mode = 'standard')\n",
        "    \n",
        "    # This must be changed depending on the datasets, prediction/evaliation times of interest\n",
        "    #pred_time = [2,5,8,11,15] # prediction time (in months)\n",
        "    #eval_time = [2,5,8,11,15] # months evaluation time (for C-index and Brier-Score)\n",
        "    pred_time = np.arange(0,16,1)\n",
        "    eval_time = np.arange(0,16,1) # prediction time (in months)\n",
        "else:\n",
        "    print ('ERROR:  DATA_MODE NOT FOUND !!!')\n",
        "\n",
        "_, num_Event, num_Category  = np.shape(mask1)  # dim of mask3: [subj, Num_Event, Num_Category]\n",
        "max_length                  = np.shape(data)[1] \n",
        "\n",
        "\n",
        "file_path = '{}'.format(data_mode)\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    os.makedirs(file_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    ID     tte      time  label  ...  anno_inizio_ARV  FDRn  HCV  HBV\n",
            "1   48  14.266   0.00000      1  ...                1     1  0.0  1.0\n",
            "2   48  14.266   0.05749      1  ...                1     1  0.0  1.0\n",
            "3   48  14.266   0.17796      1  ...                1     1  0.0  1.0\n",
            "4   48  14.266   0.40520      1  ...                1     1  0.0  1.0\n",
            "5   48  14.266   0.76112      1  ...                1     1  0.0  1.0\n",
            "..  ..     ...       ...    ...  ...              ...   ...  ...  ...\n",
            "76  48  14.266  12.57769      1  ...                1     1  0.0  1.0\n",
            "77  48  14.266  12.67625      1  ...                1     1  0.0  1.0\n",
            "78  48  14.266  12.80219      1  ...                1     1  0.0  1.0\n",
            "79  48  14.266  13.13347      1  ...                1     1  0.0  1.0\n",
            "80  48  14.266  13.54689      1  ...                1     1  0.0  1.0\n",
            "\n",
            "[80 rows x 28 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq6ieBKI9-AS"
      },
      "source": [
        "### 2. Set Hyper-Parameters\n",
        "##### - Play with your own hyper-parameters!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwWzcitb9-AV",
        "outputId": "23ff8ec6-b37a-4924-accf-d15d2b553b21"
      },
      "source": [
        "burn_in_mode                = 'ON' #{'ON', 'OFF'}\n",
        "boost_mode                  = 'ON' #{'ON', 'OFF'}\n",
        "\n",
        "##### HYPER-PARAMETERS\n",
        "new_parser = {'mb_size': 32,\n",
        "\n",
        "             'iteration_burn_in': 2000,\n",
        "             'iteration':1000,\n",
        "\n",
        "             'keep_prob': 0.65,\n",
        "             'lr_train': 426*1e-6,\n",
        "\n",
        "             'h_dim_RNN':75,\n",
        "             'h_dim_FC' : 75,\n",
        "             'num_layers_RNN':3,\n",
        "             'num_layers_ATT':3,\n",
        "             'num_layers_CS' :3,\n",
        "\n",
        "             'RNN_type':'LSTM', #{'LSTM', 'GRU'}\n",
        "\n",
        "             'FC_active_fn' : tf.nn.relu,\n",
        "             'RNN_active_fn': tf.nn.tanh,\n",
        "\n",
        "            'reg_W'         : 1e-5,\n",
        "            'reg_W_out'     : 0.,\n",
        "\n",
        "             'alpha' :5.0,\n",
        "             'beta'  :1.,\n",
        "             'gamma' :3.\n",
        "}\n",
        "\n",
        "\n",
        "# INPUT DIMENSIONS\n",
        "input_dims                  = { 'x_dim'         : x_dim,\n",
        "                                'x_dim_cont'    : x_dim_cont,\n",
        "                                'x_dim_bin'     : x_dim_bin,\n",
        "                                'num_Event'     : num_Event,\n",
        "                                'num_Category'  : num_Category,\n",
        "                                'max_length'    : max_length }\n",
        "\n",
        "# NETWORK HYPER-PARMETERS\n",
        "network_settings            = { 'h_dim_RNN'         : new_parser['h_dim_RNN'],\n",
        "                                'h_dim_FC'          : new_parser['h_dim_FC'],\n",
        "                                'num_layers_RNN'    : new_parser['num_layers_RNN'],\n",
        "                                'num_layers_ATT'    : new_parser['num_layers_ATT'],\n",
        "                                'num_layers_CS'     : new_parser['num_layers_CS'],\n",
        "                                'RNN_type'          : new_parser['RNN_type'],\n",
        "                                'FC_active_fn'      : new_parser['FC_active_fn'],\n",
        "                                'RNN_active_fn'     : new_parser['RNN_active_fn'],\n",
        "                                'initial_W'         : tf.contrib.layers.xavier_initializer(),\n",
        "\n",
        "                                'reg_W'             : new_parser['reg_W'],\n",
        "                                'reg_W_out'         : new_parser['reg_W_out']\n",
        "                                 }\n",
        "\n",
        "\n",
        "mb_size           = new_parser['mb_size']\n",
        "iteration         = new_parser['iteration']\n",
        "iteration_burn_in = new_parser['iteration_burn_in']\n",
        "\n",
        "keep_prob         = new_parser['keep_prob']\n",
        "lr_train          = new_parser['lr_train']\n",
        "\n",
        "alpha             = new_parser['alpha']\n",
        "beta              = new_parser['beta']\n",
        "gamma             = new_parser['gamma']\n",
        "\n",
        "# SAVE HYPERPARAMETERS\n",
        "log_name = file_path + '/hyperparameters_log.txt'\n",
        "#save_logging(new_parser, log_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrLni2_q-kA-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK3Fp7Ju9-AY"
      },
      "source": [
        "### 3. Split Dataset into Train/Valid/Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkgsZODA9-AZ",
        "outputId": "27e52247-cdd1-433d-f478-622895cdde29"
      },
      "source": [
        "### TRAINING-TESTING SPLIT\n",
        "(tr_data, datate , tr_data_mi, data_mite, tr_time, timete , tr_label, labelte , \n",
        " tr_mask1,mask1te , tr_mask2,mask2te, tr_mask3, mask3te) = train_test_split(data, data_mi,time, label, mask1, mask2, mask3 ,\n",
        "                                                                            test_size=0.2  ,stratify = label ) \n",
        "\n",
        "#(tr_data,va_data, tr_data_mi, va_data_mi, tr_time,va_time, tr_label,va_label, \n",
        " #tr_mask1,va_mask1, tr_mask2,va_mask2, tr_mask3,va_mask3) = train_test_split(tr_data, tr_data_mi, tr_time, tr_label, tr_mask1, tr_mask2, tr_mask3, test_size=0.2, random_state=seed) \n",
        "tr_data.shape\n",
        "#if boost_mode == 'ON':\n",
        "#    tr_data, tr_data_mi, tr_time, tr_label, tr_mask1, tr_mask2, tr_mask3 = f_get_boosted_trainset(tr_data, tr_data_mi, tr_time, tr_label, tr_mask1, tr_mask2, tr_mask3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3609, 456, 25)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYpas6qoC9Mt",
        "outputId": "1c79f6d9-4020-4b20-d1c3-5aabac77dc39"
      },
      "source": [
        "a = data[0,0,1]\n",
        "a\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.34103447768572615"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BGcMki8iC9m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5TKgDDQhuSm"
      },
      "source": [
        "def import_dataset_test( norm_mode = 'standard'):\n",
        "\n",
        "    #df_= pd.read_csv('/content/database_time_dep_CM.csv')\n",
        "    df_ = pd.read_csv('/content/dataset_completo_modelli_test.txt' ,index_col=0 , sep=',')\n",
        "    \n",
        "    df_['time'] = df_['tstart']\n",
        "\n",
        "    df_['label'] = df_['CVD1_COD_15']\n",
        "    for i in np.unique(df_['ID']):\n",
        "      if sum(df_.loc[df_.index[df_['ID']==i],'label'])>0:\n",
        "        df_.loc[df_.index[df_['ID']==i],'label'] = 1\n",
        "    df_['time_CVD1'] = np.float32(np.where( df_['time_CVD1'] == 'non evento' , 0 , df_['time_CVD1']))\n",
        "    df_['tte'] = np.where(df_['label'] == 1 , df_['time_CVD1'] - df_['time'], df_['end'] - df_['time'])\n",
        "\n",
        "    df_[['SESSO']] = np.where(df_[['SESSO']]=='F',1,0) \n",
        "    df_[['anno_inizio_ARV']] = np.where(df_[['anno_inizio_ARV']]=='<=2007',1,0) \n",
        "    df_[['FDRn']] = np.where(df_[['FDRn']]=='MSM',1,0)\n",
        "    df_[['HCV']] = np.float32(np.where(df_[['HCV']]=='non noto',0,df_[['HCV']]))\n",
        "    df_[['HBV']] = np.float32(np.where(df_[['HBV']]=='non noto',0,df_[['HBV']]))\n",
        "\n",
        "    bin_list           = ['SESSO' , 'RAZZA', 'anno_inizio_ARV' , 'FDRn' , 'HCV' , 'HBV']\n",
        "    cont_list          = [ 'AGE_UPD' , \"diabete_si_no\" , 'ipert_si_no' , 'tumore_si_no'  , 'aids_si_no' , 'CD4' , \"COLEST\"  , 'VIREMIA' , 'Hb' , 'PLT' ,\n",
        "                            'TRIG' , 'CREATININA' , 'ALT' , 'AST' , 'INI_time_exp' , \"PI_time_exp\" , 'NRTI_time_exp' , 'NNRTI_time_exp']\n",
        "    feat_list          = cont_list + bin_list\n",
        "    df_                = df_[['ID', 'tte', 'time', 'label']  + feat_list]\n",
        "    df_ = df_.dropna()\n",
        "    \n",
        "    df_org_            = df_.copy(deep=True)\n",
        "\n",
        "    df_[cont_list]     = f_get_Normalization(np.asarray(df_[cont_list]).astype(float), norm_mode)\n",
        "\n",
        "    pat_info, data     = f_construct_dataset(df_, feat_list)\n",
        "    _, data_org        = f_construct_dataset(df_org_, feat_list)\n",
        "\n",
        "    data_mi                  = np.zeros(np.shape(data))\n",
        "    data_mi[np.isnan(data)]  = 1\n",
        "    data_org[np.isnan(data)] = 0\n",
        "    data[np.isnan(data)]     = 0 \n",
        "\n",
        "    x_dim           = np.shape(data)[2] # 1 + x_dim_cont + x_dim_bin (including delta)\n",
        "    x_dim_cont      = len(cont_list)\n",
        "    x_dim_bin       = len(bin_list) \n",
        "\n",
        "    last_meas       = pat_info[:,[3]]  #pat_info[:, 3] contains age at the last measurement\n",
        "    label           = pat_info[:,[2]]  #two competing risks\n",
        "    time            = pat_info[:,[1]]  #age when event occurred\n",
        "\n",
        "    num_Category    = int(np.max(pat_info[:, 1]) * 2) #or specifically define larger than the max tte\n",
        "    num_Event       = len(np.unique(label)) - 1\n",
        "\n",
        "    if num_Event == 1:\n",
        "        label[np.where(label!=0)] = 1 #make single risk\n",
        "\n",
        "    mask1           = f_get_fc_mask1(last_meas, num_Event, num_Category)\n",
        "    mask2           = f_get_fc_mask2(time, label, num_Event, num_Category)\n",
        "    mask3           = f_get_fc_mask3(time, -1, num_Category)\n",
        "\n",
        "    DIM             = (x_dim, x_dim_cont, x_dim_bin)\n",
        "    DATA            = (data, time, label)\n",
        "    # DATA            = (data, data_org, time, label)\n",
        "    MASK            = (mask1, mask2, mask3)\n",
        "\n",
        "    return DIM, DATA, MASK, data_mi\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GEaRXkod2ij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb715f2d-d7b4-4d1b-f2af-d87c095a0c2a"
      },
      "source": [
        "#(x_dimte, x_dim_contte, x_dim_binte), (datate, timete, labelte), (mask1te, mask2te, mask3te), (data_mite) = import_dataset_test('standard')\n",
        "\n",
        "a=[1,2,3,4,5]\n",
        "a.remove(2)\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 3, 4, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6-qSAVs9-Ac"
      },
      "source": [
        "### 4. Train the Networ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "husGO8T09-Ad",
        "outputId": "e5547065-1299-4b79-916a-e66fe4ec238e"
      },
      "source": [
        "##### CREATE DYNAMIC-DEEPFHT NETWORK\n",
        "tf.reset_default_graph()\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.Session(config=config)\n",
        "\n",
        "model = Model_Longitudinal_Attention(sess, \"Dyanmic-DeepHit\", input_dims, network_settings)\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "eval_time = np.arange(0,16,1)\n",
        " \n",
        "### TRAINING - BURN-IN\n",
        "if burn_in_mode == 'ON':\n",
        "    print( \"BURN-IN TRAINING ...\")\n",
        "    for itr in range(iteration_burn_in):\n",
        "        x_mb, x_mi_mb, k_mb, t_mb, m1_mb, m2_mb, m3_mb = f_get_minibatch(mb_size, tr_data, tr_data_mi, tr_label, tr_time, tr_mask1, tr_mask2, tr_mask3)\n",
        "        DATA = (x_mb, k_mb, t_mb)\n",
        "        MISSING = (x_mi_mb)\n",
        "\n",
        "        _, loss_curr = model.train_burn_in(DATA, MISSING, keep_prob, lr_train)\n",
        "\n",
        "        if (itr+1)%200 == 0:\n",
        "            print('itr: {:04d} | loss: {:.4f}'.format(itr+1, loss_curr))\n",
        "\n",
        "\n",
        "### TRAINING - MAIN\n",
        "print( \"MAIN TRAINING ...\")\n",
        "min_valid = 0.5\n",
        "c = []\n",
        "cte = []\n",
        "losss = []\n",
        "\n",
        "for itr in range(iteration):\n",
        "    x_mb, x_mi_mb, k_mb, t_mb, m1_mb, m2_mb, m3_mb = f_get_minibatch(mb_size, tr_data, tr_data_mi, tr_label, tr_time, tr_mask1, tr_mask2, tr_mask3)\n",
        "    DATA = (x_mb, k_mb, t_mb)\n",
        "    MASK = (m1_mb, m2_mb, m3_mb)\n",
        "    MISSING = (x_mi_mb)\n",
        "    PARAMETERS = (alpha, beta, gamma)\n",
        "\n",
        "    _, loss_curr = model.train(DATA, MASK, MISSING, PARAMETERS, keep_prob, lr_train)\n",
        "    losss.append(loss_curr)\n",
        "   \n",
        "        \n",
        "\n",
        "    ### VALIDATION  (based on average C-index of our interest)\n",
        "    if (itr+1)%(int((np.array(tr_data).shape[0]/mb_size))) == 0:   \n",
        "        print('itr: {:04d} | loss: {:.4f}'.format(itr+1, loss_curr))     \n",
        "        risk_alltr  = f_get_risk_predictions(sess, model, tr_data, tr_data_mi, pred_time, eval_time)\n",
        "        risk_allte = f_get_risk_predictions(sess, model, datate, data_mite, pred_time, eval_time)\n",
        "        for p, p_time in enumerate(pred_time):\n",
        "            pred_horizon = int(p_time)\n",
        "            val_result1 = np.zeros([num_Event, len(eval_time)])\n",
        "            val_result1te = np.zeros([num_Event, len(eval_time)])\n",
        "            \n",
        "            for t, t_time in enumerate(eval_time):                \n",
        "                eval_horizon = int(t_time) + pred_horizon\n",
        "                for k in range(num_Event):\n",
        "                    val_result1[k, t] = c_index(risk_alltr[k][:, p, t], tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "                    val_result1te[k, t] = c_index(risk_allte[k][:, p, t], timete , (labelte[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            \n",
        "            if p == 0:\n",
        "                val_final1 = val_result1\n",
        "                val_final1te = val_result1te\n",
        "            else:\n",
        "                val_final1 = np.append(val_final1, val_result1, axis=0)\n",
        "                val_final1te = np.append(val_final1te, val_result1te, axis=0)\n",
        "        tmp_valid = np.mean(val_final1)\n",
        "        tmp_validte = np.mean(val_final1te)\n",
        "        print( 'updated.... average c-index = ' + str('%.4f' %(tmp_valid)))\n",
        "        print( 'updated.... average c-index test set = ' + str('%.4f' %(tmp_validte)))\n",
        "        if tmp_valid >  min_valid:\n",
        "            min_valid = tmp_valid\n",
        "            saver.save(sess, file_path + '/model')\n",
        "        c.append(tmp_valid)\n",
        "        cte.append(tmp_validte)\n",
        "\n",
        "losss = np.array(losss)        \n",
        "c  = np.array(c)      \n",
        "cte  = np.array(cte)      "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Scale of 0 disables regularizer.\n",
            "WARNING:tensorflow:From <ipython-input-5-8d47bab51813>:23: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-5-8d47bab51813>:28: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-5-8d47bab51813>:96: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/rnn.py:1230: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From <ipython-input-9-9948b64ab3a2>:20: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "BURN-IN TRAINING ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-bd575f19fa42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mMISSING\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_mi_mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_burn_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMISSING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-9948b64ab3a2>\u001b[0m in \u001b[0;36mtrain_burn_in\u001b[0;34m(self, DATA, MISSING, keep_prob, lr_train)\u001b[0m\n\u001b[1;32m    315\u001b[0m         return self.sess.run([self.solver_burn_in, self.LOSS_3], \n\u001b[1;32m    316\u001b[0m                              feed_dict={self.x:x_mb, self.x_mi: x_mi_mb, self.k:k_mb, self.t:t_mb, \n\u001b[0;32m--> 317\u001b[0;31m                                         self.mb_size: np.shape(x_mb)[0], self.keep_prob:keep_prob, self.lr_rate:lr_train})\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_mi_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqlz62BHC3oh"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.set(rc={'figure.figsize':(10,5)})\n",
        "sns.lineplot(np.arange(len(c)),c )\n",
        "plt.xlabel('Epoche')\n",
        "plt.ylabel('C-index')\n",
        "plt.title('Concordance index per ogni epoca')\n",
        "plt.savefig('Epoc.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNpiV1ocAgNr"
      },
      "source": [
        "\n",
        "def _f_get_pred(sess, model, _data, _data_mi, pred_horizon):\n",
        "    '''\n",
        "        predictions based on the prediction time.\n",
        "        create new_data and new_mask2 that are available previous or equal to the prediction time (no future measurements are used)\n",
        "    '''\n",
        "    new_data    = np.zeros(np.shape(_data))\n",
        "    new_data_mi = np.zeros(np.shape(_data_mi))\n",
        "\n",
        "    meas_time = np.concatenate([np.zeros([np.shape(_data)[0], 1]), np.cumsum(_data[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
        "\n",
        "    for i in range(np.shape(_data)[0]):\n",
        "        last_meas = np.sum(meas_time[i, :] <= pred_horizon)\n",
        "\n",
        "        new_data[i, :last_meas, :]    = _data[i, :last_meas, :]\n",
        "        new_data_mi[i, :last_meas, :] = _data_mi[i, :last_meas, :]\n",
        "\n",
        "    return model.predict(new_data, new_data_mi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQOHFHqTC7OA"
      },
      "source": [
        "## Curve e surv prob\n",
        "pred_time = np.arange(0,16,1)\n",
        "risk_all = _f_get_pred(sess, model, datate, data_mite, 0)\n",
        "risk_all.shape\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kb4Z7t3-RuQ"
      },
      "source": [
        "risk = np.zeros((903,25))\n",
        "for j in range(903):\n",
        "  for i in range(1,26):\n",
        "    risk[j,i-1] = risk_all[j,0,i-1]-risk_all[j,0,i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWFuQOqpI0jB"
      },
      "source": [
        "p = np.zeros((903,15 , 26))\n",
        "def f_get_pred_mod(model, data, data_mi, pred_horizon):\n",
        "    '''\n",
        "        predictions based on the prediction time.\n",
        "        create new_data and new_mask2 that are available previous or equal to the prediction time (no future measurements are used)\n",
        "    '''\n",
        "    new_data    = np.zeros(np.shape(data))\n",
        "    new_data_mi = np.zeros(np.shape(data_mi))\n",
        "\n",
        "    meas_time = np.concatenate([np.zeros([np.shape(data)[0], 1]), np.cumsum(data[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
        "\n",
        "    for i in range(np.shape(data)[0]):\n",
        "        last_meas = np.sum(meas_time[i, :] <= pred_horizon)\n",
        "\n",
        "        new_data[i, :last_meas, :]    = data[i, :last_meas, :]\n",
        "        new_data_mi[i, :last_meas, :] = data_mi[i, :last_meas, :]\n",
        "\n",
        "    return model.predict(new_data, new_data_mi)\n",
        "\n",
        "for nn,pred_time in enumerate(np.linspace(1,15,15)):\n",
        "  p[:,nn , :] = f_get_pred_mod(model , datate, data_mite , pred_time)[:,0,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDqs95VxLqUo"
      },
      "source": [
        "ppp = model.predict(datate, data_mite,pred_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbWTiItzqEdC"
      },
      "source": [
        "risk_all = f_get_risk_predictions(sess, model, datate, data_mite, pred_time, eval_time)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqhrsTXoHvgg"
      },
      "source": [
        "risk_all = risk_all[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5Xcx7jsGafm"
      },
      "source": [
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nw00JxdLCGB"
      },
      "source": [
        "S = np.zeros((903,16))\n",
        "S[:,0] = 1\n",
        "for j in range(903):\n",
        "  for i in range(0,15):\n",
        "    S[j,i] = math.exp(-p[j,i,0]*np.linspace(0,15,16)[i])\n",
        "S"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdwUy7fE8-0x"
      },
      "source": [
        "metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEN7W23B85tS"
      },
      "source": [
        "t = []\n",
        "for i in range(903):\n",
        "  m =(S[i,:])<0.98\n",
        "  if sum(m)== 0:\n",
        "    t.append(15)\n",
        "  else:\n",
        "    t.append(np.linspace(0,15,16)[m][0])\n",
        "\n",
        "t = np.array(t)\n",
        "t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB5BkAN29vRX"
      },
      "source": [
        "\n",
        "def _f_get_pred(sess, model, _data, _data_mi, pred_horizon):\n",
        "    '''\n",
        "        predictions based on the prediction time.\n",
        "        create new_data and new_mask2 that are available previous or equal to the prediction time (no future measurements are used)\n",
        "    '''\n",
        "    new_data    = np.zeros(np.shape(_data))\n",
        "    new_data_mi = np.zeros(np.shape(_data_mi))\n",
        "\n",
        "    meas_time = np.concatenate([np.zeros([np.shape(_data)[0], 1]), np.cumsum(_data[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
        "\n",
        "    for i in range(np.shape(_data)[0]):\n",
        "        last_meas = np.sum(meas_time[i, :] <= pred_horizon)\n",
        "\n",
        "        new_data[i, :last_meas, :]    = _data[i, :last_meas, :]\n",
        "        new_data_mi[i, :last_meas, :] = _data_mi[i, :last_meas, :]\n",
        "\n",
        "    return model.predict(new_data, new_data_mi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tszNfAix9vRZ"
      },
      "source": [
        "## Curve e surv prob\n",
        "risk_all = f_get_risk_predictions(sess, model, datate, data_mite, pred_time, eval_time)\n",
        "\n",
        "r = risk_all[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycSZpHdV9vRa"
      },
      "source": [
        "p = np.zeros((903,16 , 26))\n",
        "def f_get_pred_mod(model, data, data_mi, pred_horizon):\n",
        "    '''\n",
        "        predictions based on the prediction time.\n",
        "        create new_data and new_mask2 that are available previous or equal to the prediction time (no future measurements are used)\n",
        "    '''\n",
        "    new_data    = np.zeros(np.shape(data))\n",
        "    new_data_mi = np.zeros(np.shape(data_mi))\n",
        "\n",
        "    meas_time = np.concatenate([np.zeros([np.shape(data)[0], 1]), np.cumsum(data[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
        "\n",
        "    for i in range(np.shape(data)[0]):\n",
        "        last_meas = np.sum(meas_time[i, :] <= pred_horizon)\n",
        "\n",
        "        new_data[i, :last_meas, :]    = data[i, :last_meas, :]\n",
        "        new_data_mi[i, :last_meas, :] = data_mi[i, :last_meas, :]\n",
        "\n",
        "    return model.predict(new_data, new_data_mi)\n",
        "\n",
        "for nn,pred_time in enumerate(np.linspace(0,15,16)):\n",
        "  p[:,nn , :] = f_get_pred_mod(model , datate, data_mite , pred_time)[:,0,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4ytNjcM9vRa"
      },
      "source": [
        "p.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWCa1L-49vRb"
      },
      "source": [
        "S = np.zeros((903,16,26))\n",
        "S[:,0,:] = 1\n",
        "for i in range(1,16):\n",
        "  for j in range(903):\n",
        "    S[j,i,:] = S[j,i-1,:] * (1 - p[j,i,:])\n",
        "i    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDA8RWGh9vRb"
      },
      "source": [
        "S[3,:,25]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiQ5dzm79vRd"
      },
      "source": [
        "metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbRMXXif9vRd"
      },
      "source": [
        "t = []\n",
        "for i in range(903):\n",
        "  m =(S[i,:,25])<0.98\n",
        "  if sum(m)== 0:\n",
        "    t.append(15)\n",
        "  else:\n",
        "    t.append(np.linspace(0,15,16)[m][0])\n",
        "\n",
        "t = np.array(t)\n",
        "t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nZpVPw39vRe"
      },
      "source": [
        "label = labelte\n",
        "sum(label)\n",
        "#\n",
        "#cind = []\n",
        "#risk = np.zeros(shape=(903  ,15))\n",
        "#for t, t_time in enumerate(np.arange(0,15,1)):\n",
        "#  eval_horizon = int(t_time)\n",
        "#  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 903  )\n",
        "#\n",
        "#risk = pd.DataFrame(risk)\n",
        "sns.set(rc={'figure.figsize':(15,10)})\n",
        "for i in range(903 ):\n",
        "  if label[i] == 0:\n",
        "    sns.lineplot(np.arange(0,16,1),S[i,:,25] , alpha = .1, color = 'darkblue')\n",
        "\n",
        "\n",
        "for i in range(903  ):\n",
        "  if label[i] == 1:\n",
        "    sns.lineplot(np.arange(0,16,1),S[i,:,25] , alpha = .8, color = 'indianred')\n",
        "plt.title('Probabilità di evento MACE predetta per tutti i pazienti')  \n",
        "plt.savefig('sop_pred.png')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgNpRZqsN6WZ"
      },
      "source": [
        "t = []\n",
        "for i in range(903):\n",
        "  m =(S[i,:])<0.98\n",
        "  if sum(m)== 0:\n",
        "    t.append(15)\n",
        "  else:\n",
        "    t.append(np.linspace(1,15,16)[m][0])\n",
        "\n",
        "t = np.array(t)\n",
        "t\n",
        "time_pred = t\n",
        "time_true = timete\n",
        "MSE_e = np.mean(np.square(time_true[np.reshape(labelte , 903) == 1]-time_pred[np.reshape(labelte , 903) == 1]))\n",
        "MSE_a = np.mean(np.square(time_true[np.reshape(timete , 903) <= 15]-time_pred[np.reshape(timete , 903) <= 15]))\n",
        "cnt = 0\n",
        "for i in range(903):\n",
        "  if time_true[i] <= 15:\n",
        "    if time_true[i] < time_pred[i]:\n",
        "      cnt +=1\n",
        "perc = cnt/sum(time_true <= 15)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pzmhCCxN6WZ"
      },
      "source": [
        "sum(labelte)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7MQwyazN6WZ"
      },
      "source": [
        "t = []\n",
        "for i in range(903):\n",
        "  m =S[i,:,25]<0.98\n",
        "  if sum(m)== 0:\n",
        "    t.append(15)\n",
        "  else:\n",
        "    t.append(np.linspace(1,15,16)[m][0])\n",
        "\n",
        "t = np.array(t)\n",
        "t\n",
        "time_pred = t\n",
        "time_true = timete\n",
        "MSE_e = np.mean(np.square(time_true[np.reshape(labelte , 903) == 1]-time_pred[np.reshape(labelte , 903) == 1]))\n",
        "MSE_a = np.mean(np.square(time_true[np.reshape(timete , 903) <= 15]-time_pred[np.reshape(timete , 903) <= 15]))\n",
        "cnt = 0\n",
        "for i in range(903):\n",
        "  if time_true[i] <= 15:\n",
        "    if time_true[i] < time_pred[i]:\n",
        "      cnt +=1\n",
        "perc = cnt/sum(time_true <= 15)    \n",
        "print(MSE_e)\n",
        "print(MSE_a)\n",
        "print(perc)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XT0zgmuN6WZ"
      },
      "source": [
        "lab_pred = time_pred != 15."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udKvOvkvN6WZ"
      },
      "source": [
        "for eva in range(26): \n",
        "  t = []\n",
        "  for i in range(903):\n",
        "    m =S[i,:,25]<0.98\n",
        "    if sum(m)== 0:\n",
        "      t.append(15)\n",
        "    else:\n",
        "      t.append(np.linspace(1,15,16)[m][0])\n",
        "\n",
        "  t = np.array(t)\n",
        "  t\n",
        "  time_pred = t\n",
        "  time_true = timete\n",
        "  MSE_e = np.mean(np.square(time_true[np.reshape(labelte , 903) == 1]-time_pred[np.reshape(labelte , 903) == 1]))\n",
        "  MSE_a = np.mean(np.square(time_true[np.reshape(timete , 903) <= 15]-time_pred[np.reshape(timete , 903) <= 15]))\n",
        "  cnt = 0\n",
        "  for i in range(903):\n",
        "    if time_true[i] <= 15:\n",
        "      if time_true[i] < time_pred[i]:\n",
        "        cnt +=1\n",
        "  perc = cnt/sum(time_true <= 15)    \n",
        "\n",
        "\n",
        "  lab_pred = time_pred != 15.\n",
        "  TP = sum(np.diag(np.multiply(lab_pred,labelte)))\n",
        "  TN = sum(np.diag(labelte+lab_pred) == 0)\n",
        "  FP = sum(lab_pred) - TP\n",
        "  FN = sum(lab_pred==False) - TN\n",
        "  print(eva)\n",
        "  print(\"sens:\"  + str(TP / (TP+FN)) + \"acc:\" + str((TP+TN)/(TP+TN+FP+FN))+ \"sec:\" + str((TN)/(TN+FP)) + ' ' + str(MSE_e) + ' ' +str(MSE_a) + ' ' + str(perc))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "359ikXV29-Ah"
      },
      "source": [
        "pred_time = np.arange(0,16,1)\n",
        "\n",
        "saver.restore(sess, file_path + '/model')\n",
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, datate, data_mite, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t], timete , (labelte[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t], timete , (labelte[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "    \n",
        "c = np.mean(final1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0pl5zL-4hAy"
      },
      "source": [
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFY4BLkvdqU2"
      },
      "source": [
        "c_m = c\n",
        "c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb5ub_uobFpK"
      },
      "source": [
        "def import_dataset_shuffle( data_s , data_mi_s  , col , norm_mode = 'standard'):\n",
        "    \n",
        "    data_s_mod = data_s.copy()\n",
        "    data_mi_s_mod = data_mi_s.copy()\n",
        "    data_mi_s = None\n",
        "    data_s = None\n",
        "    tempo  = 'time'\n",
        "   \n",
        "    bin_list           = ['SESSO' , 'RAZZA', 'anno_inizio_ARV' , 'FDRn' , 'HCV' , 'HBV']\n",
        "    cont_list          = [ 'AGE_UPD' , \"diabete_si_no\" , 'ipert_si_no' , 'tumore_si_no'  , 'aids_si_no' , 'CD4' , \"COLEST\"  , 'VIREMIA' , 'Hb' , 'PLT' ,\n",
        "                            'TRIG' , 'CREATININA' , 'ALT' , 'AST' , 'INI_time_exp' , \"PI_time_exp\" , 'NRTI_time_exp' , 'NNRTI_time_exp']\n",
        "    feat_list  = tempo + cont_list + bin_list\n",
        "\n",
        "    i = -1\n",
        "    column = feat_list\n",
        "    for j in range(len(column)):\n",
        "      if column[j] == col:\n",
        "        i = j\n",
        "    s = np.random.permutation(range(3609))  \n",
        "    for k in range(len(data_s_mod[0,:,0])):\n",
        "\n",
        "\n",
        "      data_s_mod[:,k,i] = (data_s_mod[s,k,i])\n",
        "      data_mi_s_mod[:,k,i] = (data_mi_s_mod[s,k,i])\n",
        "    \n",
        "    \n",
        "    return data_s_mod , data_mi_s_mod\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1IMwA1UGU90"
      },
      "source": [
        "bin_list           = ['SESSO' , 'RAZZA', 'anno_inizio_ARV' , 'FDRn' , 'HCV' , 'HBV']\n",
        "cont_list          = [ 'AGE_UPD' , \"diabete_si_no\" , 'ipert_si_no' , 'tumore_si_no'  , 'aids_si_no' , 'CD4' , \"COLEST\"  , 'VIREMIA' , 'Hb' , 'PLT' ,\n",
        "                        'TRIG' , 'CREATININA' , 'ALT' , 'AST' , 'INI_time_exp' , \"PI_time_exp\" , 'NRTI_time_exp' , 'NNRTI_time_exp']\n",
        "feat_list          = cont_list + bin_list\n",
        "feat_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFfzfa7f7LO5"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'AGE_UPD' )\n",
        "data_imp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLBBMzFh7LPL"
      },
      "source": [
        "pred_time = np.arange(0,16,1)\n",
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "#risk_all = f_get_risk_predictions(sess, model, data_imp, data_mite, pred_time, eval_time)\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_age = np.mean(df1,axis =1)[0]\n",
        "c_age"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1narPSw7LPM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DgfpPEw7LPM"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'COLEST')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lClRyCtu7LPN"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t], tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t], tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "\n",
        "    \n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_col = np.mean(df1,axis =1)[0]\n",
        "c_col"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s_Gy75f7LPO"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'VIREMIA')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4Mpnyyi7LPO"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_vir = np.mean(df1,axis =1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIbmRkfz7LPP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQhtLPtV7LPP"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'CD4')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08Le0_is7LPQ"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_cd4 = np.mean(df1,axis =1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9sEzg7o7LPS"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'DIABETE')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMJ7hVV47LPT"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_dia = np.mean(df1,axis =1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka8gQZlgHwVW"
      },
      "source": [
        "df1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYQ6w5AW7LPV"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'SESSO')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Qaa-aHK7LPW"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_sex = np.mean(df1,axis =1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GSJD8Yj7L3D"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'ipert_si_no')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCyopwp97L3D"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_ipert_si_no = np.mean(df1,axis =1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adxcbkNb7L3D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHlNrzGx7L3E"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'tumore_si_no')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ottWXCt7L3E"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_tumore_si_no = np.mean(df1,axis =1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9Onu9qT7L3F"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'aids_si_no')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDMkyfmh7L3F"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_aids_si_no = np.mean(df1,axis =1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emdvNixq7L3G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrPTWGKW7L3G"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'Hb')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bGcKXNL7L3G"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_Hb = np.mean(df1,axis =1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJP4U6fI7L3H"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'PLT')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bd6fcH37L3H"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_PLT = np.mean(df1,axis =1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GRg7mYe7L3H"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'PI_time_exp')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNxgjjR37L3I"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_PI_time_exp = np.mean(df1,axis =1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeAvs4V3b3YA"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'NRTI_time_exp')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcoO-qHGa2oy"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_NRTI_time_exp = np.mean(df1,axis =1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGGOhQdr9-Ah"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv-3fpJ_chWf"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'NNRTI_time_exp')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKq5OMe2chWj"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_NNRTI_time_exp = np.mean(df1,axis =1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw0PyZY9csOw"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'RAZZA')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5fGFeBkcsOx"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_RAZZA = np.mean(df1,axis =1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGlBwVaOc7D4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD24BllRclNo"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'anno_inizio_ARV')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_xisKkOclNp"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_anno_inizio_ARV = np.mean(df1,axis =1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPTebq9jdEql"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'FDRn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCF9UsfxdEqm"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_FDRn = np.mean(df1,axis =1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3Ghmp_SdFIQ"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'HCV')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mFxz1yQdFIR"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_HCV = np.mean(df1,axis =1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT7XrZdR_bCb"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'HBV')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AkFPHjI_bCh"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_HBV = np.mean(df1,axis =1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxWCqATH_bCi"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'CREATININA')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6bJ8N6Q_bCi"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_CREATININA = np.mean(df1,axis =1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-DKaX5N_bCj"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'ALT')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j0zMAYV_bCk"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_ALT = np.mean(df1,axis =1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GkMqh9-_ncK"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'AST')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9NOwDrL_ncL"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_AST = np.mean(df1,axis =1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rETbqOAY_ncM"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'INI_time_exp')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBQG2P-C_ncN"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_INI_time_exp = np.mean(df1,axis =1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7xSjxwLj1GJ"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'TRIG')\n",
        "data_imp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GYZAZWVj1GL"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_TRIG = np.mean(df1,axis =1)[0]\n",
        "c_TRIG"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-Bbb9Q3c71i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9k4MFr3--1T"
      },
      "source": [
        "data_imp , data_impte = import_dataset_shuffle(tr_data, tr_data_mi,'NNRTI_time_exp')\n",
        "data_imp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bge0QKxm--1T"
      },
      "source": [
        "\n",
        "risk_all = f_get_risk_predictions(sess, model, data_imp, data_impte, pred_time, eval_time)\n",
        "\n",
        "for p, p_time in enumerate(pred_time):\n",
        "    pred_horizon = int(p_time)\n",
        "    result1, result2 = np.zeros([num_Event, len(eval_time)]), np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "    for t, t_time in enumerate(eval_time):                \n",
        "        eval_horizon = int(t_time) + pred_horizon\n",
        "        for k in range(num_Event):\n",
        "            result1[k, t] = c_index(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "            result2[k, t] = brier_score(risk_all[k][:, p, t],tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "    \n",
        "    if p == 0:\n",
        "        final1, final2 = result1, result2\n",
        "    else:\n",
        "        final1, final2 = np.append(final1, result1, axis=0), np.append(final2, result2, axis=0)\n",
        "        \n",
        "        \n",
        "row_header = []\n",
        "for p_time in pred_time:\n",
        "    for t in range(num_Event):\n",
        "        row_header.append('pred_time {}: event_{}'.format(p_time,k+1))\n",
        "            \n",
        "col_header = []\n",
        "for t_time in eval_time:\n",
        "    col_header.append('eval_time {}'.format(t_time))\n",
        "\n",
        "# c-index result\n",
        "df1 = pd.DataFrame(final1, index = row_header, columns=col_header)\n",
        "\n",
        "# brier-score result\n",
        "df2 = pd.DataFrame(final2, index = row_header, columns=col_header)\n",
        "\n",
        "### PRINT RESULTS\n",
        "print('========================================================')\n",
        "print('--------------------------------------------------------')\n",
        "print('- C-INDEX: ')\n",
        "print(df1)\n",
        "print('--------------------------------------------------------')\n",
        "print('- BRIER-SCORE: ')\n",
        "print(df2)\n",
        "print('========================================================')\n",
        "c_NRTTI_time_exp = np.mean(df1,axis =1)[0]\n",
        "c_NRTTI_time_exp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b85QMXM6Bwkk"
      },
      "source": [
        "c_m = np.mean(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo9UwD5sdawa"
      },
      "source": [
        "sns.set(rc={'figure.figsize':(12,10)})\n",
        "d = pd.DataFrame({'Variables':['età' , 'cd4' , 'colesterolo'  , 'diabete' , 'sesso' , 'viremia'   , 'ipert_si_no' , 'tumore_si_no'  , \n",
        "                             'aids_si_no' , 'Hb' , \"PI_time_exp\" , 'NRTI_time_exp' , 'NNRTI_time_exp',\n",
        "                              'RAZZA', 'anno_inizio_ARV' , 'FDRn' , 'HCV' , 'HBV' , 'PLT' ,  'TRIG' , 'CREATININA' , 'ALT' , \n",
        "                             'AST' ,'INI_time_exp' ], \n",
        "                  'Feature importance':[c_m-c_age , c_m-c_cd4 , c_m-c_col , c_m-c_dia , c_m-c_sex , c_m-c_vir,\n",
        "                           c_m-c_ipert_si_no , c_m-c_tumore_si_no , c_m-c_aids_si_no ,\n",
        "                            c_m-c_Hb, c_m-c_PI_time_exp , c_m-c_NRTI_time_exp , c_m-c_NRTTI_time_exp , \n",
        "                            c_m-c_RAZZA , c_m-c_anno_inizio_ARV ,c_m-c_FDRn , c_m-c_HCV ,  c_m-c_HBV ,\n",
        "                            c_m-c_PLT , c_m-c_TRIG , c_m-c_CREATININA , c_m-c_ALT , c_m-c_AST , c_m-c_INI_time_exp]})\n",
        "d.sort_values(by=['Feature importance'],\n",
        "                               ascending=False, inplace=True)\n",
        "sns.barplot(y = 'Variables',x= 'Feature importance',data=d, color= 'darkblue', orient= 'h')\n",
        "plt.savefig('feature_imp1.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SotIlQBbNDp"
      },
      "source": [
        "label = labelte\n",
        "sum(label)\n",
        "p = model.predict( datate , data_mite)\n",
        "\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(903  ,3000))\n",
        "for t, t_time in enumerate(np.linspace(1,15,3000 )):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 903  )\n",
        "\n",
        "risk = pd.DataFrame(risk)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFB7DP9BkYb8"
      },
      "source": [
        "time_pred = np.zeros(903)\n",
        "t = np.linspace(0,15,3000)\n",
        "for k in range(903):\n",
        "  if sum((risk.loc[k,:]>0.02))>0:\n",
        "    time_pred[k] = t[(risk.loc[k,:]>0.02)][0]\n",
        "  else:\n",
        "    time_pred[k] = 15\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUJ_Yw0QoV-K"
      },
      "source": [
        "time_true = timete\n",
        "MSE_e = np.mean(np.square(time_true[np.reshape(labelte , 903) == 1]-time_pred[np.reshape(labelte , 903) == 1]))\n",
        "MSE_a = np.mean(np.square(time_true[np.reshape(timete , 903) <= 15]-time_pred[np.reshape(timete , 903) <= 15]))\n",
        "cnt = 0\n",
        "for i in range(903):\n",
        "  if time_true[i] <= 15:\n",
        "    if time_true[i] < time_pred[i]:\n",
        "      cnt +=1\n",
        "perc = cnt/sum(time_true <= 15)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8d6PF2nkBlb"
      },
      "source": [
        "print(MSE_e)\n",
        "print(MSE_a)\n",
        "print(perc)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPPuyJGhrxoF"
      },
      "source": [
        "lab_pred = time_pred != 15."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deK-AnefsJyC"
      },
      "source": [
        "TP = sum(np.diag(np.multiply(lab_pred,labelte)))\n",
        "TN = sum(np.diag(labelte+lab_pred) == 0)\n",
        "FP = sum(lab_pred) - TP\n",
        "FN = sum(lab_pred==False) - TN\n",
        "print(\"sens:\"  + str(TP / (TP+FN)))\n",
        "print(\"acc:\" + str((TP+TN)/(TP+TN+FP+FN)))\n",
        "\n",
        "print(\"sec:\" + str((TN)/(TN+FP)))\n",
        "print(str(TP)+\"|\"+str(FN))\n",
        "print(str(FP)+\"|\"+str(TN))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbTf-Mnjw-ol"
      },
      "source": [
        "pip install shap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-ZMh-IZm2dM"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost.sklearn import XGBRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn import tree\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BduGR_H0yUFG"
      },
      "source": [
        "a = pd.DataFrame(pd.concat(pd.DataFrame(data[1:5,:,:]) ,pd.DataFrame( data_mi[1:5,:,:])), axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXpqQVqmTJGV"
      },
      "source": [
        "def pred(data ):\n",
        "  risk_all = f_get_risk_predictions(sess, model, data,data_mi[1:50,:,:] ,pred_time = pred_time, eval_time = eval_time)\n",
        "  rr = risk_all[0]\n",
        "  return rr\n",
        "  \n",
        "rf_shap_values = shap.KernelExplainer(pred, data[1:50,0,:]  )\n",
        "data_mi.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0iDyBaw1B-q"
      },
      "source": [
        "risk_all[0][:,:,6].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyvPEIApXyH4"
      },
      "source": [
        "shap_val = rf_shap_values.shap_values(data[1:50,0,:] ,nsamples=10 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwNs84u4a_ki"
      },
      "source": [
        "shap.initjs()\n",
        "bin_list           = [ 'anno_inizio_ARV' , 'HCV' ]\n",
        "cont_list          = [ 'AGE_UPD' , 'PLT', 'VIREMIA' , 'PI_time_exp' , 'NRTI_time_exp'  , 'NNRTI_time_exp' ]\n",
        "feat_list          = cont_list + bin_list\n",
        "feat_list          = cont_list + bin_list\n",
        "data,data_mi,t = DATA\n",
        "shap.force_plot(rf_shap_values.expected_value, shap_val , data, link='logit' , feature_names= feat_list )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkURuZnEY91E"
      },
      "source": [
        "\n",
        "shap.dependence_plot(3,shap_val, data , interaction_index=0, feature_names =feat_list )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWbTt0S8SsQQ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np5vzRTzfDrF"
      },
      "source": [
        "bin_list           = [ 'anno_inizio_ARV' , 'HCV' ]\n",
        "cont_list          = [ 'AGE_UPD' , 'PLT', 'VIREMIA' , 'PI_time_exp' , 'NRTI_time_exp'  , 'NNRTI_time_exp' ]\n",
        "feat_list          = cont_list + bin_list       \n",
        "feat_list          = cont_list + bin_list\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "figure(figsize=(12, 10))\n",
        "shap.summary_plot(shap_val, features=data, feature_names=feat_list, plot_size=(12,10) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usoQV_51T3oG"
      },
      "source": [
        "vals = np.abs(shap_val).mean(0)\n",
        "\n",
        "\n",
        "feature_importance = pd.DataFrame(list(zip(feat_list, vals)),\n",
        "                                  columns=['Variables','Feature importance'])\n",
        "feature_importance.sort_values(by=['Feature importance'],\n",
        "                               ascending=False, inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ini1BCEmpom1"
      },
      "source": [
        "sns.set(rc={'figure.figsize':(12,10)})\n",
        "\n",
        "sns.barplot(y = 'Variables',x = 'Feature importance',data=feature_importance, orient = 'h' , color=\"darkblue\" )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}