{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev59KYcc_wIh"
      },
      "source": [
        "\n",
        "## Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3_YTcaJJeIPm",
        "outputId": "e0addf0c-8efa-4539-ec9a-65e692d0af6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deephit\n",
            "  Downloading deephit-0.0.18.tar.gz (8.5 kB)\n",
            "Collecting lifelines==0.25.10\n",
            "  Downloading lifelines-0.25.10-py3-none-any.whl (347 kB)\n",
            "\u001b[K     |████████████████████████████████| 347 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from deephit) (1.19.5)\n",
            "Collecting pandas==1.2.3\n",
            "  Downloading pandas-1.2.3-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 57.4 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.24.1\n",
            "  Downloading scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.1 MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.15.5\n",
            "  Downloading tensorflow-1.15.5-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 1.0 kB/s \n",
            "\u001b[?25hCollecting autograd-gamma>=0.3\n",
            "  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n",
            "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.7/dist-packages (from lifelines==0.25.10->deephit) (1.3)\n",
            "Collecting formulaic<0.3,>=0.2.2\n",
            "  Downloading formulaic-0.2.4-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.7/dist-packages (from lifelines==0.25.10->deephit) (3.2.2)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from lifelines==0.25.10->deephit) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3->deephit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3->deephit) (2018.9)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.1->deephit) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.1->deephit) (1.1.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->deephit) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->deephit) (1.1.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 45.7 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 72.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->deephit) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->deephit) (0.12.0)\n",
            "Collecting h5py<=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 66.4 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of scikit-learn to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting numpy==1.19.5\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 62.8 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of lifelines to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of deephit to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting deephit\n",
            "  Downloading deephit-0.0.17.tar.gz (8.5 kB)\n",
            "Collecting tensorflow==1.15.3\n",
            "  Downloading tensorflow-1.15.3-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 23 kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.3->deephit) (1.42.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.3->deephit) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.3->deephit) (1.13.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.3->deephit) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.3->deephit) (0.37.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.3->deephit) (3.17.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.3->deephit) (0.8.1)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd>=1.3->lifelines==0.25.10->deephit) (0.16.0)\n",
            "Collecting interface-meta>=1.2\n",
            "  Downloading interface_meta-1.2.4-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.5->deephit) (3.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines==0.25.10->deephit) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines==0.25.10->deephit) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines==0.25.10->deephit) (1.3.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->deephit) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->deephit) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->deephit) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->deephit) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->deephit) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->deephit) (3.6.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.5->deephit) (1.5.2)\n",
            "Building wheels for collected packages: deephit, gast, autograd-gamma\n",
            "  Building wheel for deephit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deephit: filename=deephit-0.0.17-py3-none-any.whl size=9054 sha256=d272c40c8cc422805f62704d2a75e42958881b1e2cc35855ddde6b63d9bd077e\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/7f/4d/99246fc1a6e7bd72e5db0c5635c85f31a55dbc8e5ba3f34695\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=c4b49499f524154eee7614362d90be4281bcb8d87f5f6d837a0dd931659096f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4049 sha256=5942c4eb31ddd668ec1c83561ad44fc3564cbf8620a50b9f74897c2fcaaff1be\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/01/ee/1331593abb5725ff7d8c1333aee93a50a1c29d6ddda9665c9f\n",
            "Successfully built deephit gast autograd-gamma\n",
            "Installing collected packages: pandas, interface-meta, tensorflow-estimator, tensorboard, keras-applications, gast, formulaic, autograd-gamma, tensorflow, scikit-learn, lifelines, deephit\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.1\n",
            "    Uninstalling scikit-learn-1.0.1:\n",
            "      Successfully uninstalled scikit-learn-1.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.6 requires tensorflow>=2.0.0, but you have tensorflow 1.15.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.2.3 which is incompatible.\u001b[0m\n",
            "Successfully installed autograd-gamma-0.5.0 deephit-0.0.17 formulaic-0.2.4 gast-0.2.2 interface-meta-1.2.4 keras-applications-1.0.8 lifelines-0.25.10 pandas-1.2.3 scikit-learn-0.24.1 tensorboard-1.15.0 tensorflow-1.15.3 tensorflow-estimator-1.15.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install deephit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdYnuCl7_0yw",
        "outputId": "9c3b82b2-1187-42ee-c088-d2dd00411587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lifelines in /usr/local/lib/python3.7/dist-packages (0.25.10)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.2.3)\n",
            "Requirement already satisfied: formulaic<0.3,>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from lifelines) (0.2.4)\n",
            "Requirement already satisfied: numpy<1.20.0,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.19.5)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (3.2.2)\n",
            "Requirement already satisfied: autograd-gamma>=0.3 in /usr/local/lib/python3.7/dist-packages (from lifelines) (0.5.0)\n",
            "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.3)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd>=1.3->lifelines) (0.16.0)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from formulaic<0.3,>=0.2.2->lifelines) (0.8.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from formulaic<0.3,>=0.2.2->lifelines) (1.13.3)\n",
            "Requirement already satisfied: interface-meta>=1.2 in /usr/local/lib/python3.7/dist-packages (from formulaic<0.3,>=0.2.2->lifelines) (1.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (1.3.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->lifelines) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0->lifelines) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "pip install lifelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "_zqUSXoV_2XI",
        "outputId": "e5afd21b-5a51-4813-db67-0bfce73e93a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==1.1.5\n",
            "  Downloading pandas-1.1.5-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.1.5) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.2.3\n",
            "    Uninstalling pandas-1.2.3:\n",
            "      Successfully uninstalled pandas-1.2.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "deephit 0.0.17 requires pandas==1.2.3, but you have pandas 1.1.5 which is incompatible.\u001b[0m\n",
            "Successfully installed pandas-1.1.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install pandas==1.1.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QQqovyDr__tP"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "#import dataframe_image as dfi\n",
        "from lifelines import KaplanMeierFitter \n",
        "from lifelines.statistics import (logrank_test, \n",
        "                                  pairwise_logrank_test, \n",
        "                                  multivariate_logrank_test, \n",
        "                                  survival_difference_at_fixed_point_in_time_test)\n",
        "from lifelines import CoxPHFitter\n",
        "from lifelines.datasets import load_rossi\n",
        "import lifelines\n",
        "from sklearn.model_selection import train_test_split\n",
        "plt.style.use('seaborn')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from lifelines.utils import concordance_index\n",
        "#import import_data as impt\n",
        "from sklearn.utils import resample\n",
        "#from deephit import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YSeFWjvdJMo"
      },
      "source": [
        "\n",
        "### Bootstrap with noise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_TWKcwjpdPqm"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('/content/dataset_completo_modelli_train_bl.txt', sep =',' )\n",
        "df_train['time_CVD1'] = np.float32(np.where( df_train['time_CVD1'] == 'non evento' , 0 , df_train['time_CVD1']))\n",
        "df_train[['time']] = np.where(df_train[['ID_CVD']] == 1 , df_train[['time_CVD1']] , df_train[['end']])\n",
        "df_boot_train = resample( df_train , random_state= 1234 , n_samples = 7 * df_train.shape[0] , stratify=\n",
        "                         df_train['ID_CVD'])\n",
        "df_boot_train.to_csv(\"data_train_boot.txt\", sep=',')\n",
        "df_test = pd.read_csv('/content/dataset_completo_modelli_test_bl.txt', sep =',' )\n",
        "df_test['time_CVD1'] = np.float32(np.where( df_test['time_CVD1'] == 'non evento' , 0 , df_test['time_CVD1']))\n",
        "df_test[['time']] = np.where(df_test[['ID_CVD']] == 1 , df_test[['time_CVD1']] , df_test[['end']])\n",
        "df_boot_test = df_test\n",
        "df_boot_test.to_csv(\"data_test_boost.txt\" , sep = ',' )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smR85r2vf8gR",
        "outputId": "16a4ad15-7432-4ddf-ed90-7c684554ff7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(ilocs[0], value)\n"
          ]
        }
      ],
      "source": [
        "df_boot_train.columns\n",
        "df_boot_train_jitter = df_boot_train\n",
        "variabili =  ['AGE_UPD' , 'VIREMIA', 'CD4', 'COLEST', 'Hb', 'PLT', 'TRIG', 'CREATININA', 'ALT',\n",
        "       'AST', 'INI_time_exp', 'PI_time_exp', 'NRTI_time_exp', 'NNRTI_time_exp']\n",
        "n = df_boot_train.shape[0]\n",
        "for v in variabili:\n",
        "  sdev = np.sqrt(np.var(df_boot_train.loc[:,v]))\n",
        "  wn = np.random.normal(size = n, loc = 0 ,scale =sdev / 3 ) \n",
        "  df_boot_train_jitter.loc[:,v] = df_boot_train.loc[:,v]+ wn\n",
        "sdev = np.sqrt(np.var(df_boot_train.loc[:,'time']))\n",
        "wn = np.random.normal(size = n, loc = 0 ,scale =sdev / 3 ) \n",
        "df_boot_train_jitter.loc[:,'time'] = df_boot_train.loc[:,'time']+ wn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wBeDS5QfoGEJ"
      },
      "outputs": [],
      "source": [
        "df_boot_test.columns\n",
        "df_boot_test_jitter = df_boot_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxoVqyVtJkzB"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mwpZk-IKuDFa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import tensorflow as tf\n",
        "import random\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#import import_data as impt\n",
        "\n",
        "from deephit import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GUMJfnpqLkrf"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random \n",
        "import datetime\n",
        "import importlib\n",
        "import math\n",
        "\n",
        "from lifelines import KaplanMeierFitter \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "from termcolor import colored\n",
        "from scipy import stats\n",
        "import pickle as pkl\n",
        "import tensorflow as tf \n",
        "#from tensorflow.contrib.layers import fully_connected as FC_Net \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "#from sklearn.metrics import brier_score_loss, train_test_split,  roc_auc_score\n",
        "#from sklearn.model_selection import  \n",
        "#from sklearn.metrics import\n",
        "#import sklearn.metrics as metrics\n",
        "\n",
        "'''\n",
        "This declare DeepHit architecture:\n",
        "\n",
        "INPUTS:\n",
        "    - input_dims: dictionary of dimension information\n",
        "        > x_dim: dimension of features\n",
        "        > num_Event: number of competing events (this does not include censoring label)\n",
        "        > num_Category: dimension of time horizon of interest, i.e., |T| where T = {0, 1, ..., T_max-1}\n",
        "                      : this is equivalent to the output dimension\n",
        "    - network_settings:\n",
        "        > h_dim_shared & num_layers_shared: number of nodes and number of fully-connected layers for the shared subnetwork\n",
        "        > h_dim_CS & num_layers_CS: number of nodes and number of fully-connected layers for the cause-specific subnetworks\n",
        "        > active_fn: 'relu', 'elu', 'tanh'\n",
        "        > initial_W: Xavier initialization is used as a baseline\n",
        "\n",
        "LOSS FUNCTIONS:\n",
        "    - 1. loglikelihood (this includes log-likelihood of subjects who are censored)\n",
        "    - 2. rankding loss (this is calculated only for acceptable pairs; see the paper for the definition)\n",
        "    - 3. calibration loss (this is to reduce the calibration loss; this is not included in the paper version)\n",
        "'''\n",
        "\n",
        "     \n",
        "global _EPSILON\n",
        "_EPSILON = 1e-08\n",
        "\n",
        "### MASK FUNCTIONS\n",
        "'''\n",
        "    fc_mask2      : To calculate LOSS_1 (log-likelihood loss)\n",
        "    fc_mask3      : To calculate LOSS_2 (ranking loss)\n",
        "'''\n",
        "def impt_f_get_fc_mask2(time, label, num_Event, num_Category):\n",
        "    '''\n",
        "        mask4 is required to get the log-likelihood loss\n",
        "        mask4 size is [N, num_Event, num_Category]\n",
        "            if not censored : one element = 1 (0 elsewhere)\n",
        "            if censored     : fill elements with 1 after the censoring time (for all events)\n",
        "    '''\n",
        "    #print('impt_f_get_fc_mask2')\n",
        "    mask = np.zeros([np.shape(time)[0], num_Event, num_Category]) # for the first loss function\n",
        "    for i in range(np.shape(time)[0]):\n",
        "        if label[i,0] != 0:  #not censored\n",
        "            mask[i,int(label[i,0]-1),int(time[i,0])] = 1\n",
        "        else: #label[i,2]==0: censored\n",
        "            mask[i,:,int(time[i,0]+1):] =  1 #fill 1 until from the censoring time (to get 1 - \\sum F)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def impt_f_get_fc_mask3(time, meas_time, num_Category):\n",
        "    '''\n",
        "        mask5 is required calculate the ranking loss (for pair-wise comparision)\n",
        "        mask5 size is [N, num_Category].\n",
        "        - For longitudinal measurements:\n",
        "             1's from the last measurement to the event time (exclusive and inclusive, respectively)\n",
        "             denom is not needed since comparing is done over the same denom\n",
        "        - For single measurement:\n",
        "             1's from start to the event time(inclusive)\n",
        "    '''\n",
        "    # print('impt_f_get_fc_mask3')\n",
        "    mask = np.zeros([np.shape(time)[0], num_Category]) # for the first loss function\n",
        "    if np.shape(meas_time):  #lonogitudinal measurements\n",
        "        for i in range(np.shape(time)[0]):\n",
        "            t1 = int(meas_time[i, 0]) # last measurement time\n",
        "            t2 = int(time[i, 0]) # censoring/event time\n",
        "            mask[i,(t1+1):(t2+1)] = 1  #this excludes the last measurement time and includes the event time\n",
        "    else:                    #single measurement\n",
        "        for i in range(np.shape(time)[0]):\n",
        "            t = int(time[i, 0]) # censoring/event time\n",
        "            mask[i,:(t+1)] = 1  #this excludes the last measurement time and includes the event time\n",
        "    return mask\n",
        "\n",
        "# los function\n",
        "########## ADAPTED IMPORT FOR LOS\n",
        "def get_Normalization_los(dffeatures, data_stats): # only implemented standard, but can include max-min type in the future\n",
        "    # print('get_Normalization_los')\n",
        "    data=dffeatures\n",
        "    for feat in dffeatures.columns:\n",
        "        if data_stats.loc[feat,'std'] !=0:\n",
        "            data[feat]=(dffeatures[feat] - data_stats.loc[feat,'mean'])/data_stats.loc[feat,'std']\n",
        "        else:\n",
        "            data[feat]=(dffeatures[feat] - data_stats.loc[feat,'mean'])\n",
        "    return np.asarray(data)\n",
        "\n",
        "\n",
        "def import_dataset_los_old(dflabel,dfval,dffeatures,data_stats,num_Category,num_Event,x_dim):\n",
        "    # print('import_dataset_los')\n",
        "    label           = np.asarray(dflabel)\n",
        "    time            = np.asarray(dfval)\n",
        "    data            = get_Normalization_los(dffeatures,data_stats)\n",
        "    mask1           = impt_f_get_fc_mask2(time, label, num_Event, num_Category)\n",
        "    mask2           = impt_f_get_fc_mask3(time, -1, num_Category)\n",
        "    DIM             = (x_dim)\n",
        "    DATA            = (data, time, label)\n",
        "    MASK            = (mask1, mask2)\n",
        "    \n",
        "    return DIM, DATA, MASK\n",
        "\n",
        "def import_dataset_los(dffeatures,data_stats,num_Category,num_Event,x_dim):\n",
        "    label           = np.asarray([[1]]) # arbitrary, adaptation for pred.\n",
        "    time            = np.asarray([[700]])\n",
        "    data            = get_Normalization_los(dffeatures,data_stats)\n",
        "    mask1           = impt_f_get_fc_mask2(time, label, num_Event, num_Category)\n",
        "    mask2           = impt_f_get_fc_mask3(time, -1, num_Category)\n",
        "    MASK            = (mask1, mask2)\n",
        "\n",
        "    return data, MASK\n",
        "def test(x):\n",
        "    print('x = ', x)\n",
        "\n",
        "\n",
        "##### USER-DEFINED FUNCTIONS\n",
        "def log(x):\n",
        "    #print('logX')\n",
        "    #tf.log is deprecated. Please use tf.math.log instead.\n",
        "    return tf.math.log(x + _EPSILON)\n",
        "\n",
        "def div(x, y):\n",
        "    #print('divXY')\n",
        "    return tf.div(x, (y + _EPSILON))\n",
        "\n",
        "def load_dffromCsv(dfname):\n",
        "    dir='/dbfs/user/hive/warehouse/'\n",
        "    df=pd.read_csv(dir+dfname)\n",
        "    print(dfname, 'loaded!')\n",
        "    return df\n",
        "\n",
        "def save_dftoCsv(df, dfname):\n",
        "    dir='/dbfs/user/hive/warehouse/'\n",
        "    df.to_csv(dir+dfname, index = False)\n",
        "    print(dfname, 'saved!')\n",
        "\n",
        "\n",
        "class Model_DeepHit:\n",
        "    def __init__(self, sess, name, input_dims, network_settings):\n",
        "        self.sess               = sess\n",
        "        self.name               = name\n",
        "\n",
        "        # INPUT DIMENSIONS\n",
        "        self.x_dim              = input_dims['x_dim']\n",
        "\n",
        "        self.num_Event          = input_dims['num_Event']\n",
        "        self.num_Category       = input_dims['num_Category']\n",
        "\n",
        "        # NETWORK HYPER-PARMETERS\n",
        "        self.h_dim_shared       = network_settings['h_dim_shared']\n",
        "        self.h_dim_CS           = network_settings['h_dim_CS']\n",
        "        self.num_layers_shared  = network_settings['num_layers_shared']\n",
        "        self.num_layers_CS      = network_settings['num_layers_CS']\n",
        "\n",
        "        self.active_fn          = network_settings['active_fn']\n",
        "        self.initial_W          = network_settings['initial_W']\n",
        "        self.reg_W              = tf.contrib.layers.l2_regularizer(scale=1.0)\n",
        "        self.reg_W_out          = tf.contrib.layers.l1_regularizer(scale=1.0)\n",
        "\n",
        "        self._build_net()\n",
        "\n",
        "\n",
        "    def _build_net(self):\n",
        "        with tf.variable_scope(self.name):\n",
        "            #### PLACEHOLDER DECLARATION\n",
        "            #The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
        "            self.mb_size     = tf.compat.v1.placeholder(tf.int32, [], name='batch_size')\n",
        "            self.lr_rate     = tf.compat.v1.placeholder(tf.float32, [], name='learning_rate')\n",
        "            self.keep_prob   = tf.compat.v1.placeholder(tf.float32, [], name='keep_probability')   #keeping rate\n",
        "            self.a           = tf.compat.v1.placeholder(tf.float32, [], name='alpha')\n",
        "            self.b           = tf.compat.v1.placeholder(tf.float32, [], name='beta')\n",
        "\n",
        "            self.x           = tf.compat.v1.placeholder(tf.float32, shape=[None, self.x_dim], name='inputs')\n",
        "            self.k           = tf.compat.v1.placeholder(tf.float32, shape=[None, 1], name='labels')     #event/censoring label (censoring:0)\n",
        "            self.t           = tf.compat.v1.placeholder(tf.float32, shape=[None, 1], name='timetoevents')\n",
        "\n",
        "            self.fc_mask1    = tf.compat.v1.placeholder(tf.float32, shape=[None, self.num_Event, self.num_Category], name='mask1')  #for Loss 1\n",
        "            self.fc_mask2    = tf.compat.v1.placeholder(tf.float32, shape=[None, self.num_Category], name='mask2')  #for Loss 2 / Loss 3\n",
        "\n",
        "\n",
        "            ##### SHARED SUBNETWORK w/ FCNETS\n",
        "            shared_out = utils_create_FCNet(self.x, self.num_layers_shared, self.h_dim_shared, self.active_fn, self.h_dim_shared, self.active_fn, self.initial_W, self.keep_prob, self.reg_W)\n",
        "            last_x = self.x  #for residual connection\n",
        "\n",
        "            h = tf.concat([last_x, shared_out], axis=1)\n",
        "\n",
        "            #(num_layers_CS) layers for cause-specific (num_Event subNets)\n",
        "            out = []\n",
        "            for _ in range(self.num_Event):\n",
        "                cs_out = utils_create_FCNet(h, (self.num_layers_CS), self.h_dim_CS, self.active_fn, self.h_dim_CS, self.active_fn, self.initial_W, self.keep_prob, self.reg_W)\n",
        "                out.append(cs_out)\n",
        "            out = tf.stack(out, axis=1) # stack referenced on subject\n",
        "            out = tf.reshape(out, [-1, self.num_Event*self.h_dim_CS])\n",
        "            out = tf.nn.dropout(out, keep_prob=self.keep_prob)\n",
        "\n",
        "            out = FC_Net(out, self.num_Event * self.num_Category, activation_fn=tf.nn.softmax, \n",
        "                         weights_initializer=self.initial_W, weights_regularizer=self.reg_W_out, scope=\"Output\")\n",
        "            self.out = tf.reshape(out, [-1, self.num_Event, self.num_Category])\n",
        "\n",
        "\n",
        "            ##### GET LOSS FUNCTIONS\n",
        "            self.loss_Log_Likelihood()      #get loss1: Log-Likelihood loss\n",
        "            self.loss_Ranking()        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            self.LOSS_TOTAL = self.a*self.LOSS_1 + self.b*self.LOSS_2   ## diff from original\n",
        "            #tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
        "            self.solver =  tf.compat.v1.train.AdamOptimizer(learning_rate=self.lr_rate).minimize(self.LOSS_TOTAL)\n",
        "\n",
        "\n",
        "    ### LOSS-FUNCTION 1 -- Log-likelihood loss\n",
        "    def loss_Log_Likelihood(self):\n",
        "        I_1 = tf.sign(self.k)\n",
        "\n",
        "        #for uncenosred: log P(T=t,K=k|x)\n",
        "        #keep_dims is deprecated, use keepdims instead\n",
        "        tmp1 = tf.reduce_sum(tf.reduce_sum(self.fc_mask1 * self.out, reduction_indices=2), reduction_indices=1, keepdims=True)\n",
        "        tmp1 = I_1 * log(tmp1)\n",
        "\n",
        "        #for censored: log \\sum P(T>t|x)\n",
        "        tmp2 = tf.reduce_sum(tf.reduce_sum(self.fc_mask1 * self.out, reduction_indices=2), reduction_indices=1, keepdims=True)\n",
        "        tmp2 = (1. - I_1) * log(tmp2)\n",
        "\n",
        "        self.LOSS_1 = - tf.reduce_mean(tmp1 + 1.0*tmp2)\n",
        "\n",
        "\n",
        "    ### LOSS-FUNCTION 2 -- Ranking loss\n",
        "    def loss_Ranking(self):\n",
        "        #x        sigma1 = tf.constant(0.1, dtype=tf.float32)\n",
        "        #x        sigma1 = tf.constant(0.65, dtype=tf.float32)\n",
        "        \n",
        "        sigma1 = tf.constant(0.65, dtype=tf.float32)\n",
        "        eta = []\n",
        "        #tf.diag is deprecated. Please use tf.linalg.tensor_diag instead.\n",
        "        #tf.diag_part is deprecated. Please use tf.linalg.tensor_diag_part instead.\n",
        "        for e in range(self.num_Event):\n",
        "            one_vector = tf.ones_like(self.t, dtype=tf.float32)\n",
        "            I_2 = tf.cast(tf.equal(self.k, e+1), dtype = tf.float32) #indicator for event\n",
        "            I_2 = tf.linalg.tensor_diag(tf.squeeze(I_2))\n",
        "            tmp_e = tf.reshape(tf.slice(self.out, [0, e, 0], [-1, 1, -1]), [-1, self.num_Category]) #event specific joint prob.\n",
        "\n",
        "            R = tf.matmul(tmp_e, tf.transpose(self.fc_mask2)) #no need to divide by each individual dominator\n",
        "            # r_{ij} = risk of i-th pat based on j-th time-condition (last meas. time ~ event time) , i.e. r_i(T_{j})\n",
        "\n",
        "            diag_R = tf.reshape(tf.linalg.tensor_diag_part(R), [-1, 1])\n",
        "            R = tf.matmul(one_vector, tf.transpose(diag_R)) - R # R_{ij} = r_{j}(T_{j}) - r_{i}(T_{j})\n",
        "            R = tf.transpose(R)                                 # Now, R_{ij} (i-th row j-th column) = r_{i}(T_{i}) - r_{j}(T_{i})\n",
        "\n",
        "            T = tf.nn.relu(tf.sign(tf.matmul(one_vector, tf.transpose(self.t)) - tf.matmul(self.t, tf.transpose(one_vector))))\n",
        "            # T_{ij}=1 if t_i < t_j  and T_{ij}=0 if t_i >= t_j\n",
        "\n",
        "            T = tf.matmul(I_2, T) # only remains T_{ij}=1 when event occured for subject i\n",
        "            #keep_dims is deprecated, use keepdims instead\n",
        "            tmp_eta = tf.reduce_mean(T * tf.exp(-R/sigma1), reduction_indices=1, keepdims=True)\n",
        "\n",
        "            eta.append(tmp_eta)\n",
        "        eta = tf.stack(eta, axis=1) #stack referenced on subjects\n",
        "        eta = tf.reduce_mean(tf.reshape(eta, [-1, self.num_Event]), reduction_indices=1, keepdims=True)\n",
        "\n",
        "        self.LOSS_2 = tf.reduce_sum(eta) #sum over num_Events\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    def get_cost(self, DATA, MASK, PARAMETERS, keep_prob, lr_train):\n",
        "        (x_mb, k_mb, t_mb) = DATA\n",
        "        (m1_mb, m2_mb) = MASK\n",
        "        (alpha, beta) = PARAMETERS\n",
        "        return self.sess.run(self.LOSS_TOTAL, \n",
        "                             feed_dict={self.x:x_mb, self.k:k_mb, self.t:t_mb, self.fc_mask1: m1_mb, self.fc_mask2:m2_mb, \n",
        "                                        self.a:alpha, self.b:beta,\n",
        "                                        self.mb_size: np.shape(x_mb)[0], self.keep_prob:keep_prob, self.lr_rate:lr_train})\n",
        "\n",
        "    def train(self, DATA, MASK, PARAMETERS, keep_prob, lr_train):\n",
        "        (x_mb, k_mb, t_mb) = DATA\n",
        "        (m1_mb, m2_mb) = MASK\n",
        "        (alpha, beta) = PARAMETERS\n",
        "        return self.sess.run([self.solver, self.LOSS_TOTAL], \n",
        "                             feed_dict={self.x:x_mb, self.k:k_mb, self.t:t_mb, self.fc_mask1: m1_mb, self.fc_mask2:m2_mb, \n",
        "                                        self.a:alpha, self.b:beta,\n",
        "                                        self.mb_size: np.shape(x_mb)[0], self.keep_prob:keep_prob, self.lr_rate:lr_train})\n",
        "    \n",
        "    def predict(self, x_test, keep_prob=1.0):\n",
        "        return self.sess.run(self.out, feed_dict={self.x: x_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n",
        "\n",
        "    # def predict(self, x_test, MASK, keep_prob=1.0):\n",
        "    #     (m1_test, m2_test) = MASK\n",
        "    #     return self.sess.run(self.out, \n",
        "    #                          feed_dict={self.x: x_test, self.rnn_mask1:m1_test, self.rnn_mask2:m2_test, self.keep_prob: keep_prob})\n",
        "###utility network\n",
        "\n",
        "\n",
        "### EXTRACT STATE OUTPUT OF MULTICELL-RNNS\n",
        "def utils_create_concat_state(state, num_layers, RNN_type):\n",
        "    '''\n",
        "        GOAL\t     : concatenate the tuple-type tensor (state) into a single tensor\n",
        "        state        : input state is a tuple ofo MulticellRNN (i.e. output of MulticellRNN)\n",
        "                       consist of only hidden states h for GRU and hidden states c and h for LSTM\n",
        "        num_layers   : number of layers in MulticellRNN\n",
        "        RNN_type     : either 'LSTM' or 'GRU'\n",
        "    '''\n",
        "    for i in range(num_layers):\n",
        "        if RNN_type == 'LSTM':\n",
        "            tmp = state[i][1] ## i-th layer, h state for LSTM\n",
        "        elif RNN_type == 'GRU':\n",
        "            tmp = state[i] ## i-th layer, h state for GRU\n",
        "        else:\n",
        "            print('ERROR: WRONG RNN CELL TYPE')\n",
        "\n",
        "        if i == 0:\n",
        "            rnn_state_out = tmp\n",
        "        else:\n",
        "            rnn_state_out = tf.concat([rnn_state_out, tmp], axis = 1)\n",
        "    \n",
        "    return rnn_state_out\n",
        "\n",
        "\n",
        "### FEEDFORWARD NETWORK\n",
        "def utils_create_FCNet(inputs, num_layers, h_dim, h_fn, o_dim, o_fn, w_init, keep_prob=1.0, w_reg=None):\n",
        "    '''\n",
        "        GOAL             : Create FC network with different specifications \n",
        "        inputs (tensor)  : input tensor\n",
        "        num_layers       : number of layers in FCNet\n",
        "        h_dim  (int)     : number of hidden units\n",
        "        h_fn             : activation function for hidden layers (default: tf.nn.relu)\n",
        "        o_dim  (int)     : number of output units\n",
        "        o_fn             : activation function for output layers (defalut: None)\n",
        "        w_init           : initialization for weight matrix (defalut: Xavier)\n",
        "        keep_prob        : keep probabilty [0, 1]  (if None, dropout is not employed)\n",
        "    '''\n",
        "    # default active functions (hidden: relu, out: None)\n",
        "    if h_fn is None:\n",
        "        h_fn = tf.nn.relu\n",
        "    if o_fn is None:\n",
        "        o_fn = None\n",
        "\n",
        "    # default initialization functions (weight: Xavier, bias: None)\n",
        "    if w_init is None:\n",
        "        w_init = tf.contrib.layers.xavier_initializer() # Xavier initialization\n",
        "\n",
        "    for layer in range(num_layers):\n",
        "        if num_layers == 1:\n",
        "            out = FC_Net(inputs, o_dim, activation_fn=o_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
        "        else:\n",
        "            if layer == 0:\n",
        "                h = FC_Net(inputs, h_dim, activation_fn=h_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
        "                if not keep_prob is None:\n",
        "                    h = tf.nn.dropout(h, keep_prob=keep_prob)\n",
        "\n",
        "            elif layer > 0 and layer != (num_layers-1): # layer > 0:\n",
        "                h = FC_Net(h, h_dim, activation_fn=h_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
        "                if not keep_prob is None:\n",
        "                    h = tf.nn.dropout(h, keep_prob=keep_prob)\n",
        "\n",
        "            else: # layer == num_layers-1 (the last layer)\n",
        "                out = FC_Net(h, o_dim, activation_fn=o_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
        "\n",
        "    return out\n",
        "'''\n",
        "This provide time-dependent Concordance index and Brier Score:\n",
        "    - Use weighted_c_index and weighted_brier_score, which are the unbiased estimates.\n",
        "    \n",
        "See equations and descriptions eq. (11) and (12) of the following paper:\n",
        "    - C. Lee, W. R. Zame, A. Alaa, M. van der Schaar, \"Temporal Quilting for Survival Analysis\", AISTATS 2019\n",
        "'''\n",
        "\n",
        "\n",
        "### C(t)-INDEX CALCULATION\n",
        "def c_index(Prediction, Time_survival, Death, Time):\n",
        "    '''\n",
        "        This is a cause-specific c(t)-index\n",
        "        - Prediction      : risk at Time (higher --> more risky)\n",
        "        - Time_survival   : survival/censoring time\n",
        "        - Death           :\n",
        "            > 1: death\n",
        "            > 0: censored (including death from other cause)\n",
        "        - Time            : time of evaluation (time-horizon when evaluating C-index)\n",
        "    '''\n",
        "    N = len(Prediction)\n",
        "    A = np.zeros((N,N))\n",
        "    Q = np.zeros((N,N))\n",
        "    N_t = np.zeros((N,N))\n",
        "    Num = 0\n",
        "    Den = 0\n",
        "    for i in range(N):\n",
        "        A[i, np.where(Time_survival[i] < Time_survival)] = 1\n",
        "        Q[i, np.where(Prediction[i] > Prediction)] = 1\n",
        "  \n",
        "        if (Time_survival[i]<=Time and Death[i]==1):\n",
        "            N_t[i,:] = 1\n",
        "\n",
        "    Num  = np.sum(((A)*N_t)*Q)\n",
        "    Den  = np.sum((A)*N_t)\n",
        "\n",
        "    if Num == 0 and Den == 0:\n",
        "        result = -1 # not able to compute c-index!\n",
        "    else:\n",
        "        result = float(Num/Den)\n",
        "\n",
        "    return result\n",
        "\n",
        "### BRIER-SCORE\n",
        "def brier_score(Prediction, Time_survival, Death, Time):\n",
        "    N = len(Prediction)\n",
        "    y_true = ((Time_survival <= Time) * Death).astype(float)\n",
        "\n",
        "    return np.mean((Prediction - y_true)**2)\n",
        "\n",
        "    # result2[k, t] = brier_score_loss(risk[:, k], ((te_time[:,0] <= eval_horizon) * (te_label[:,0] == k+1)).astype(int))\n",
        "\n",
        "\n",
        "##### WEIGHTED C-INDEX & BRIER-SCORE\n",
        "def CensoringProb(Y, T):\n",
        "\n",
        "    T = T.reshape([-1]) # (N,) - np array\n",
        "    Y = Y.reshape([-1]) # (N,) - np array\n",
        "\n",
        "    kmf = KaplanMeierFitter()\n",
        "    kmf.fit(T, event_observed=(Y==0).astype(int))  # censoring prob = survival probability of event \"censoring\"\n",
        "    G = np.asarray(kmf.survival_function_.reset_index()).transpose()\n",
        "    G[1, G[1, :] == 0] = G[1, G[1, :] != 0][-1]  #fill 0 with ZoH (to prevent nan values)\n",
        "    \n",
        "    return G\n",
        "\n",
        "\n",
        "### C(t)-INDEX CALCULATION: this account for the weighted average for unbaised estimation\n",
        "def weighted_c_index(T_train, Y_train, Prediction, T_test, Y_test, Time):\n",
        "    '''\n",
        "        Thi6@gmail.coms is a cause-specific c(t)-index\n",
        "        - Prediction      : risk at Time (higher --> more risky)\n",
        "        - Time_survival   : survival/censoring time\n",
        "        - Death           :\n",
        "            > 1: death\n",
        "            > 0: censored (including death from other cause)\n",
        "        - Time            : time of evaluation (time-horizon when evaluating C-index)\n",
        "    '''\n",
        "    G = CensoringProb(Y_train, T_train)\n",
        "\n",
        "    N = len(Prediction)\n",
        "    A = np.zeros((N,N))\n",
        "    Q = np.zeros((N,N))\n",
        "    N_t = np.zeros((N,N))\n",
        "    Num = 0\n",
        "    Den = 0\n",
        "    for i in range(N):\n",
        "        tmp_idx = np.where(G[0,:] >= T_test[i])[0]\n",
        "\n",
        "        if len(tmp_idx) == 0:\n",
        "            W = (1./G[1, -1])**2\n",
        "        else:\n",
        "            W = (1./G[1, tmp_idx[0]])**2\n",
        "\n",
        "        A[i, np.where(T_test[i] < T_test)] = 1. * W\n",
        "        Q[i, np.where(Prediction[i] > Prediction)] = 1. # give weights\n",
        "\n",
        "        if (T_test[i]<=Time and Y_test[i]==1):\n",
        "            N_t[i,:] = 1.\n",
        "\n",
        "    Num  = np.sum(((A)*N_t)*Q)\n",
        "    Den  = np.sum((A)*N_t)\n",
        "\n",
        "    if Num == 0 and Den == 0:\n",
        "        result = -1 # not able to compute c-index!\n",
        "    else:\n",
        "        result = float(Num/Den)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# this account for the weighted average for unbaised estimation\n",
        "def weighted_brier_score(T_train, Y_train, Prediction, T_test, Y_test, Time):\n",
        "    G = CensoringProb(Y_train, T_train)\n",
        "    N = len(Prediction)\n",
        "\n",
        "    W = np.zeros(len(Y_test))\n",
        "    Y_tilde = (T_test > Time).astype(float)\n",
        "\n",
        "    for i in range(N):\n",
        "        tmp_idx1 = np.where(G[0,:] >= T_test[i])[0]\n",
        "        tmp_idx2 = np.where(G[0,:] >= Time)[0]\n",
        "\n",
        "        if len(tmp_idx1) == 0:\n",
        "            G1 = G[1, -1]\n",
        "        else:\n",
        "            G1 = G[1, tmp_idx1[0]]\n",
        "\n",
        "        if len(tmp_idx2) == 0:\n",
        "            G2 = G[1, -1]\n",
        "        else:\n",
        "            G2 = G[1, tmp_idx2[0]]\n",
        "        W[i] = (1. - Y_tilde[i])*float(Y_test[i])/G1 + Y_tilde[i]/G2\n",
        "\n",
        "    y_true = ((T_test <= Time) * Y_test).astype(float)\n",
        "\n",
        "    return np.mean(W*(Y_tilde - (1.-Prediction))**2)\n",
        "'''\n",
        "This provide the dimension/data/mask to train/test the network.\n",
        "\n",
        "Once must construct a function similar to \"import_dataset_SYNTHETIC\":\n",
        "    - DATA FORMAT:\n",
        "        > data: covariates with x_dim dimension.\n",
        "        > label: 0: censoring, 1 ~ K: K competing(single) risk(s)\n",
        "        > time: time-to-event or time-to-censoring\n",
        "    - Based on the data, creat mask1 and mask2 that are required to calculate loss functions.\n",
        "'''\n",
        "\n",
        "    \n",
        "### random with replace minibatches function    \n",
        "def f_get_minibatch(mb_size, x, label, time, mask1, mask2):\n",
        "    idx = range(np.shape(x)[0])\n",
        "    idx = random.sample(idx, mb_size)\n",
        "\n",
        "    x_mb = x[idx, :].astype(np.float32)\n",
        "    k_mb = label[idx, :].astype(np.float32) # censoring(0)/event(1,2,..) label\n",
        "    t_mb = time[idx, :].astype(np.float32)\n",
        "    m1_mb = mask1[idx, :, :].astype(np.float32) #fc_mask\n",
        "    m2_mb = mask2[idx, :].astype(np.float32) #fc_mask\n",
        "    return x_mb, k_mb, t_mb, m1_mb, m2_mb\n",
        "\n",
        "def xbatchpredict(xdata,xfunc, n=10000):\n",
        "    xlength = len(xdata)\n",
        "    print(\"BatchSubmitPrediction, total size= \" + str(xlength))\n",
        "    for xindex in range (0, xlength, n):\n",
        "        x_pred_xindex = xfunc(xdata[xindex: xindex + n, :])\n",
        "        if xindex == 0:\n",
        "            pred_x = x_pred_xindex\n",
        "        else:\n",
        "            pred_x = np.concatenate((pred_x, x_pred_xindex), axis=0)\n",
        "            print(np.shape(pred_x))\n",
        "    return pred_x \n",
        "\n",
        "def restoreModel(data, input_dims, network_settings,model_path):\n",
        "    pred=None\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "    try:    \n",
        "        config = tf.compat.v1.ConfigProto()\n",
        "        config.gpu_options.allow_growth = True\n",
        "        with tf.Session() as sess:\n",
        "            #print('sess:', sess)\n",
        "            model2 = Model_DeepHit(sess, \"DeepHit\", input_dims, network_settings)\n",
        "            ##print('restored..')\n",
        "            saver = tf.compat.v1.train.Saver()\n",
        "            saver.restore(sess, model_path)\n",
        "            pred = model2.predict(data)\n",
        "            #print('pred=',pred)\n",
        "    except Exception as e:\n",
        "            print('error:', e)\n",
        "    return pred\n",
        "  \n",
        "def final_prediction(pred):\n",
        "      categorization=None\n",
        "      if pred is None:\n",
        "        return 'N/A', 'N/A'\n",
        "      else:\n",
        "            pred1 = pred[:,0,:]\n",
        "            pred2 = pred[:,1,:]\n",
        "            pred3 = pred1 + pred2\n",
        "\n",
        "            predlosmean = np.multiply(pred3, np.arange(1,pred3.shape[1]+1,1))\n",
        "            xlos=predlosmean.cumsum(axis=1)[:,pred3.shape[1]-1]\n",
        "            categorization=0,1,2,-99\n",
        "\n",
        "      return xlos, categorization \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GHKsjIkVMPBm"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This provide time-dependent Concordance index and Brier Score:\n",
        "    - Use weighted_c_index and weighted_brier_score, which are the unbiased estimates.\n",
        "    \n",
        "See equations and descriptions eq. (11) and (12) of the following paper:\n",
        "    - C. Lee, W. R. Zame, A. Alaa, M. van der Schaar, \"Temporal Quilting for Survival Analysis\", AISTATS 2019\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "from lifelines import KaplanMeierFitter\n",
        "\n",
        "\n",
        "### C(t)-INDEX CALCULATION\n",
        "def c_index(Prediction, Time_survival, Death, Time):\n",
        "    '''\n",
        "        This is a cause-specific c(t)-index\n",
        "        - Prediction      : risk at Time (higher --> more risky)\n",
        "        - Time_survival   : survival/censoring time\n",
        "        - Death           :\n",
        "            > 1: death\n",
        "            > 0: censored (including death from other cause)\n",
        "        - Time            : time of evaluation (time-horizon when evaluating C-index)\n",
        "    '''\n",
        "    N = len(Prediction)\n",
        "    A = np.zeros((N,N))\n",
        "    Q = np.zeros((N,N))\n",
        "    N_t = np.zeros((N,N))\n",
        "    Num = 0\n",
        "    Den = 0\n",
        "    for i in range(N):\n",
        "        A[i, np.where(Time_survival[i] < Time_survival)] = 1\n",
        "        Q[i, np.where(Prediction[i] > Prediction)] = 1\n",
        "  \n",
        "        if (Time_survival[i]<=Time and Death[i]==1):\n",
        "            N_t[i,:] = 1\n",
        "\n",
        "    Num  = np.sum(((A)*N_t)*Q)\n",
        "    Den  = np.sum((A)*N_t)\n",
        "\n",
        "    if Num == 0 and Den == 0:\n",
        "        result = -1 # not able to compute c-index!\n",
        "    else:\n",
        "        result = float(Num/Den)\n",
        "\n",
        "    return result\n",
        "\n",
        "### BRIER-SCORE\n",
        "def brier_score(Prediction, Time_survival, Death, Time):\n",
        "    N = len(Prediction)\n",
        "    y_true = ((Time_survival <= Time) * Death).astype(float)\n",
        "\n",
        "    return np.mean((Prediction - y_true)**2)\n",
        "\n",
        "    # result2[k, t] = brier_score_loss(risk[:, k], ((te_time[:,0] <= eval_horizon) * (te_label[:,0] == k+1)).astype(int))\n",
        "\n",
        "\n",
        "##### WEIGHTED C-INDEX & BRIER-SCORE\n",
        "def CensoringProb(Y, T):\n",
        "\n",
        "    T = T.reshape([-1]) # (N,) - np array\n",
        "    Y = Y.reshape([-1]) # (N,) - np array\n",
        "\n",
        "    kmf = KaplanMeierFitter()\n",
        "    kmf.fit(T, event_observed=(Y==0).astype(int))  # censoring prob = survival probability of event \"censoring\"\n",
        "    G = np.asarray(kmf.survival_function_.reset_index()).transpose()\n",
        "    G[1, G[1, :] == 0] = G[1, G[1, :] != 0][-1]  #fill 0 with ZoH (to prevent nan values)\n",
        "    \n",
        "    return G\n",
        "\n",
        "\n",
        "### C(t)-INDEX CALCULATION: this account for the weighted average for unbaised estimation\n",
        "def weighted_c_index(T_train, Y_train, Prediction, T_test, Y_test, Time):\n",
        "    '''\n",
        "        This is a cause-specific c(t)-index\n",
        "        - Prediction      : risk at Time (higher --> more risky)\n",
        "        - Time_survival   : survival/censoring time\n",
        "        - Death           :\n",
        "            > 1: death\n",
        "            > 0: censored (including death from other cause)\n",
        "        - Time            : time of evaluation (time-horizon when evaluating C-index)\n",
        "    '''\n",
        "    G = CensoringProb(Y_train, T_train)\n",
        "\n",
        "    N = len(Prediction)\n",
        "    A = np.zeros((N,N))\n",
        "    Q = np.zeros((N,N))\n",
        "    N_t = np.zeros((N,N))\n",
        "    Num = 0\n",
        "    Den = 0\n",
        "    for i in range(N):\n",
        "        tmp_idx = np.where(G[0,:] >= T_test[i])[0]\n",
        "\n",
        "        if len(tmp_idx) == 0:\n",
        "            W = (1./G[1, -1])**2\n",
        "        else:\n",
        "            W = (1./G[1, tmp_idx[0]])**2\n",
        "\n",
        "        A[i, np.where(T_test[i] < T_test)] = 1. * W\n",
        "        Q[i, np.where(Prediction[i] > Prediction)] = 1. # give weights\n",
        "\n",
        "        if (T_test[i]<=Time and Y_test[i]==1):\n",
        "            N_t[i,:] = 1.\n",
        "\n",
        "    Num  = np.sum(((A)*N_t)*Q)\n",
        "    Den  = np.sum((A)*N_t)\n",
        "\n",
        "    if Num == 0 and Den == 0:\n",
        "        result = -1 # not able to compute c-index!\n",
        "    else:\n",
        "        result = float(Num/Den)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# this account for the weighted average for unbaised estimation\n",
        "def weighted_brier_score(T_train, Y_train, Prediction, T_test, Y_test, Time):\n",
        "    G = CensoringProb(Y_train, T_train)\n",
        "    N = len(Prediction)\n",
        "\n",
        "    W = np.zeros(len(Y_test))\n",
        "    Y_tilde = (T_test > Time).astype(float)\n",
        "\n",
        "    for i in range(N):\n",
        "        tmp_idx1 = np.where(G[0,:] >= T_test[i])[0]\n",
        "        tmp_idx2 = np.where(G[0,:] >= Time)[0]\n",
        "\n",
        "        if len(tmp_idx1) == 0:\n",
        "            G1 = G[1, -1]\n",
        "        else:\n",
        "            G1 = G[1, tmp_idx1[0]]\n",
        "\n",
        "        if len(tmp_idx2) == 0:\n",
        "            G2 = G[1, -1]\n",
        "        else:\n",
        "            G2 = G[1, tmp_idx2[0]]\n",
        "        W[i] = (1. - Y_tilde[i])*float(Y_test[i])/G1 + Y_tilde[i]/G2\n",
        "\n",
        "    y_true = ((T_test <= Time) * Y_test).astype(float)\n",
        "\n",
        "    return np.mean(W*(Y_tilde - (1.-Prediction))**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ygx3Z3q9Mgvu"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This declare DeepHit architecture:\n",
        "INPUTS:\n",
        "    - input_dims: dictionary of dimension information\n",
        "        > x_dim: dimension of features\n",
        "        > num_Event: number of competing events (this does not include censoring label)\n",
        "        > num_Category: dimension of time horizon of interest, i.e., |T| where T = {0, 1, ..., T_max-1}\n",
        "                      : this is equivalent to the output dimension\n",
        "    - network_settings:\n",
        "        > h_dim_shared & num_layers_shared: number of nodes and number of fully-connected layers for the shared subnetwork\n",
        "        > h_dim_CS & num_layers_CS: number of nodes and number of fully-connected layers for the cause-specific subnetworks\n",
        "        > active_fn: 'relu', 'elu', 'tanh'\n",
        "        > initial_W: Xavier initialization is used as a baseline\n",
        "LOSS FUNCTIONS:\n",
        "    - 1. loglikelihood (this includes log-likelihood of subjects who are censored)\n",
        "    - 2. rankding loss (this is calculated only for acceptable pairs; see the paper for the definition)\n",
        "    - 3. calibration loss (this is to reduce the calibration loss; this is not included in the paper version)\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "#from tensorflow.contrib.layers import fully_connected as FC_Net\n",
        "\n",
        "### user-defined functions\n",
        "#import utils_network as utils\n",
        "\n",
        "_EPSILON = 1e-08\n",
        "\n",
        "\n",
        "\n",
        "##### USER-DEFINED FUNCTIONS\n",
        "def log(x):\n",
        "    return tf.log(x + _EPSILON)\n",
        "\n",
        "def div(x, y):\n",
        "    return tf.div(x, (y + _EPSILON))\n",
        "\n",
        "\n",
        "class Model_DeepHit:\n",
        "    def __init__(self, sess, name, input_dims, network_settings):\n",
        "        self.sess               = sess\n",
        "        self.name               = name\n",
        "\n",
        "        # INPUT DIMENSIONS\n",
        "        self.x_dim              = input_dims['x_dim']\n",
        "\n",
        "        self.num_Event          = input_dims['num_Event']\n",
        "        self.num_Category       = input_dims['num_Category']\n",
        "\n",
        "        # NETWORK HYPER-PARMETERS\n",
        "        self.h_dim_shared       = network_settings['h_dim_shared']\n",
        "        self.h_dim_CS           = network_settings['h_dim_CS']\n",
        "        self.num_layers_shared  = network_settings['num_layers_shared']\n",
        "        self.num_layers_CS      = network_settings['num_layers_CS']\n",
        "\n",
        "        self.active_fn          = network_settings['active_fn']\n",
        "        self.initial_W          = network_settings['initial_W']\n",
        "        self.reg_W              = tf.contrib.layers.l2_regularizer(scale=1e-4)\n",
        "        self.reg_W_out          = tf.contrib.layers.l1_regularizer(scale=1e-4)\n",
        "\n",
        "        self._build_net()\n",
        "\n",
        "\n",
        "    def _build_net(self):\n",
        "        with tf.variable_scope(self.name):\n",
        "            #### PLACEHOLDER DECLARATION\n",
        "            self.mb_size     = tf.placeholder(tf.int32, [], name='batch_size')\n",
        "            self.lr_rate     = tf.placeholder(tf.float32, [], name='learning_rate')\n",
        "            self.keep_prob   = tf.placeholder(tf.float32, [], name='keep_probability')   #keeping rate\n",
        "            self.a           = tf.placeholder(tf.float32, [], name='alpha')\n",
        "            self.b           = tf.placeholder(tf.float32, [], name='beta')\n",
        "\n",
        "            self.x           = tf.placeholder(tf.float32, shape=[None, self.x_dim], name='inputs')\n",
        "            self.k           = tf.placeholder(tf.float32, shape=[None, 1], name='labels')     #event/censoring label (censoring:0)\n",
        "            self.t           = tf.placeholder(tf.float32, shape=[None, 1], name='timetoevents')\n",
        "\n",
        "            self.fc_mask1    = tf.placeholder(tf.float32, shape=[None, self.num_Event, self.num_Category], name='mask1')  #for Loss 1\n",
        "            self.fc_mask2    = tf.placeholder(tf.float32, shape=[None, self.num_Category], name='mask2')  #for Loss 2 / Loss 3\n",
        "\n",
        "\n",
        "            ##### SHARED SUBNETWORK w/ FCNETS\n",
        "            shared_out = create_FCNet(self.x, self.num_layers_shared, self.h_dim_shared, self.active_fn, self.h_dim_shared, self.active_fn, self.initial_W, self.keep_prob, self.reg_W)\n",
        "            last_x = self.x  #for residual connection\n",
        "\n",
        "            h = tf.concat([last_x, shared_out], axis=1)\n",
        "\n",
        "            #(num_layers_CS) layers for cause-specific (num_Event subNets)\n",
        "            out = []\n",
        "            for _ in range(self.num_Event):\n",
        "                cs_out = create_FCNet(h, (self.num_layers_CS), self.h_dim_CS, self.active_fn, self.h_dim_CS, self.active_fn, self.initial_W, self.keep_prob, self.reg_W)\n",
        "                out.append(cs_out)\n",
        "            out = tf.stack(out, axis=1) # stack referenced on subject\n",
        "            out = tf.reshape(out, [-1, self.num_Event*self.h_dim_CS])\n",
        "            out = tf.nn.dropout(out, keep_prob=self.keep_prob)\n",
        "\n",
        "            out = FC_Net(out, self.num_Event * self.num_Category, activation_fn=tf.nn.softmax, \n",
        "                         weights_initializer=self.initial_W, weights_regularizer=self.reg_W_out, scope=\"Output\")\n",
        "            self.out = tf.reshape(out, [-1, self.num_Event, self.num_Category])\n",
        "\n",
        "\n",
        "            ##### GET LOSS FUNCTIONS\n",
        "            self.loss_Log_Likelihood()      #get loss1: Log-Likelihood loss\n",
        "            self.loss_Ranking()             #get loss2: Ranking loss\n",
        "\n",
        "            self.LOSS_TOTAL = self.a*self.LOSS_1 + self.b*self.LOSS_2 \n",
        "            self.solver = tf.train.AdamOptimizer(learning_rate=self.lr_rate).minimize(self.LOSS_TOTAL)\n",
        "\n",
        "\n",
        "    ### LOSS-FUNCTION 1 -- Log-likelihood loss\n",
        "    def loss_Log_Likelihood(self):\n",
        "        I_1 = tf.sign(self.k)\n",
        "\n",
        "        #for uncenosred: log P(T=t,K=k|x)\n",
        "        tmp1 = tf.reduce_sum(tf.reduce_sum(self.fc_mask1 * self.out, reduction_indices=2), reduction_indices=1, keep_dims=True)\n",
        "        tmp1 = I_1 * log(tmp1)\n",
        "\n",
        "        #for censored: log \\sum P(T>t|x)\n",
        "        tmp2 = tf.reduce_sum(tf.reduce_sum(self.fc_mask1 * self.out, reduction_indices=2), reduction_indices=1, keep_dims=True)\n",
        "        tmp2 = (1. - I_1) * log(tmp2)\n",
        "\n",
        "        self.LOSS_1 = - tf.reduce_mean(tmp1 + 1.0*tmp2)\n",
        "\n",
        "\n",
        "    ### LOSS-FUNCTION 2 -- Ranking loss\n",
        "    def loss_Ranking(self):\n",
        "        sigma1 = tf.constant(0.1, dtype=tf.float32)\n",
        "\n",
        "        eta = []\n",
        "        for e in range(self.num_Event):\n",
        "            one_vector = tf.ones_like(self.t, dtype=tf.float32)\n",
        "            I_2 = tf.cast(tf.equal(self.k, e+1), dtype = tf.float32) #indicator for event\n",
        "            I_2 = tf.diag(tf.squeeze(I_2))\n",
        "            tmp_e = tf.reshape(tf.slice(self.out, [0, e, 0], [-1, 1, -1]), [-1, self.num_Category]) #event specific joint prob.\n",
        "\n",
        "            R = tf.matmul(tmp_e, tf.transpose(self.fc_mask2)) #no need to divide by each individual dominator\n",
        "            # r_{ij} = risk of i-th pat based on j-th time-condition (last meas. time ~ event time) , i.e. r_i(T_{j})\n",
        "\n",
        "            diag_R = tf.reshape(tf.diag_part(R), [-1, 1])\n",
        "            R = tf.matmul(one_vector, tf.transpose(diag_R)) - R # R_{ij} = r_{j}(T_{j}) - r_{i}(T_{j})\n",
        "            R = tf.transpose(R)                                 # Now, R_{ij} (i-th row j-th column) = r_{i}(T_{i}) - r_{j}(T_{i})\n",
        "\n",
        "            T = tf.nn.relu(tf.sign(tf.matmul(one_vector, tf.transpose(self.t)) - tf.matmul(self.t, tf.transpose(one_vector))))\n",
        "            # T_{ij}=1 if t_i < t_j  and T_{ij}=0 if t_i >= t_j\n",
        "\n",
        "            T = tf.matmul(I_2, T) # only remains T_{ij}=1 when event occured for subject i\n",
        "\n",
        "            tmp_eta = tf.reduce_mean(T * tf.exp(-R/sigma1), reduction_indices=1, keep_dims=True)\n",
        "\n",
        "            eta.append(tmp_eta)\n",
        "        eta = tf.stack(eta, axis=1) #stack referenced on subjects\n",
        "        eta = tf.reduce_mean(tf.reshape(eta, [-1, self.num_Event]), reduction_indices=1, keep_dims=True)\n",
        "\n",
        "        self.LOSS_2 = tf.reduce_sum(eta) #sum over num_Events\n",
        "\n",
        "\n",
        "\n",
        "   \n",
        "\n",
        "    \n",
        "    def get_cost(self, DATA, MASK, PARAMETERS, keep_prob, lr_train):\n",
        "        (x_mb, k_mb, t_mb) = DATA\n",
        "        (m1_mb, m2_mb) = MASK\n",
        "        (alpha, beta) = PARAMETERS\n",
        "        return self.sess.run(self.LOSS_TOTAL, \n",
        "                             feed_dict={self.x:x_mb, self.k:k_mb, self.t:t_mb, self.fc_mask1: m1_mb, self.fc_mask2:m2_mb, \n",
        "                                        self.a:alpha, self.b:beta,\n",
        "                                        self.mb_size: np.shape(x_mb)[0], self.keep_prob:keep_prob, self.lr_rate:lr_train})\n",
        "\n",
        "    def train(self, DATA, MASK, PARAMETERS, keep_prob, lr_train):\n",
        "        (x_mb, k_mb, t_mb) = DATA\n",
        "        (m1_mb, m2_mb) = MASK\n",
        "        (alpha, beta) = PARAMETERS\n",
        "        return self.sess.run([self.solver, self.LOSS_TOTAL], \n",
        "                             feed_dict={self.x:x_mb, self.k:k_mb, self.t:t_mb, self.fc_mask1: m1_mb, self.fc_mask2:m2_mb, \n",
        "                                        self.a:alpha, self.b:beta,\n",
        "                                        self.mb_size: np.shape(x_mb)[0], self.keep_prob:keep_prob, self.lr_rate:lr_train})\n",
        "    \n",
        "    def predict(self, x_test, keep_prob=1.0):\n",
        "        return self.sess.run(self.out, feed_dict={self.x: x_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n",
        "\n",
        "    # def predict(self, x_test, MASK, keep_prob=1.0):\n",
        "    #     (m1_test, m2_test) = MASK\n",
        "    #     return self.sess.run(self.out, \n",
        "    #                          feed_dict={self.x: x_test, self.rnn_mask1:m1_test, self.rnn_mask2:m2_test, self.keep_prob: keep_prob})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "ErynTKsXMmsG",
        "outputId": "513e7293-4e75-4268-e0cf-d4b16be8de90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n    num_Category            = typically, max event/censoring time * 1.2 (to make enough time horizon)\\n    num_Event               = number of evetns i.e. len(np.unique(label))-1\\n    max_length              = maximum number of measurements\\n    x_dim                   = data dimension including delta (num_features)\\n    mask1, mask2            = used for cause-specific network (FCNet structure)\\n    EVAL_TIMES              = set specific evaluation time horizons at which the validatoin performance is maximized. \\n    \\t\\t\\t\\t\\t\\t  (This must be selected based on the dataset)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "'''\n",
        "This runs random search to find the optimized hyper-parameters using cross-validation\n",
        "INPUTS:\n",
        "    - OUT_ITERATION: # of training/testing splits\n",
        "    - RS_ITERATION: # of random search iteration\n",
        "    - data_mode: mode to select the time-to-event data from \"import_data.py\"\n",
        "    - seed: random seed for training/testing/validation splits\n",
        "    - EVAL_TIMES: list of time-horizons at which the performance is maximized; \n",
        "                  the validation is performed at given EVAL_TIMES (e.g., [12, 24, 36])\n",
        "OUTPUTS:\n",
        "    - \"hyperparameters_log.txt\" is the output\n",
        "    - Once the hyper parameters are optimized, run \"summarize_results.py\" to get the final results.\n",
        "'''\n",
        "import time, datetime, os\n",
        "#import get_main\n",
        "import numpy as np\n",
        "\n",
        "#import import_data as impt\n",
        "\n",
        "\n",
        "# this saves the current hyperparameters\n",
        "def save_logging(dictionary, log_name):\n",
        "    with open(log_name, 'w') as f:\n",
        "        for key, value in dictionary.items():\n",
        "            f.write('%s:%s\\n' % (key, value))\n",
        "\n",
        "# this open can calls the saved hyperparameters\n",
        "def load_logging(filename):\n",
        "    data = dict()\n",
        "    with open(filename) as f:\n",
        "        def is_float(input):\n",
        "            try:\n",
        "                num = float(input)\n",
        "            except ValueError:\n",
        "                return False\n",
        "            return True\n",
        "\n",
        "        for line in f.readlines():\n",
        "            if ':' in line:\n",
        "                key,value = line.strip().split(':', 1)\n",
        "                if value.isdigit():\n",
        "                    data[key] = int(value)\n",
        "                elif is_float(value):\n",
        "                    data[key] = float(value)\n",
        "                elif value == 'None':\n",
        "                    data[key] = None\n",
        "                else:\n",
        "                    data[key] = value\n",
        "            else:\n",
        "                pass # deal with bad lines of text here    \n",
        "    return data\n",
        "\n",
        "\n",
        "# this randomly select hyperparamters based on the given list of candidates\n",
        "def get_random_hyperparameters(out_path):\n",
        "    SET_BATCH_SIZE    = [32, 64, 128] #mb_size\n",
        " \n",
        "    SET_LAYERS        = [1,2,3,5] #number of layers\n",
        "    SET_NODES         = [50, 100, 200, 300] #number of nodes\n",
        "\n",
        "    SET_ACTIVATION_FN = ['relu', 'elu', 'tanh'] #non-linear activation functions\n",
        "\n",
        "    SET_ALPHA         = [0.1, 0.5, 1.0, 3.0, 5.0] #alpha values -> log-likelihood loss \n",
        "    SET_BETA          = [0.1, 0.5, 1.0, 3.0, 5.0] #beta values -> ranking loss\n",
        " \n",
        "\n",
        "    new_parser = {'mb_size': SET_BATCH_SIZE[np.random.randint(len(SET_BATCH_SIZE))],\n",
        "\n",
        "                 'iteration': 50000,\n",
        "\n",
        "                 'keep_prob': 0.6,\n",
        "                 'lr_train': 1e-4,\n",
        "\n",
        "                 'h_dim_shared': SET_NODES[np.random.randint(len(SET_NODES))],\n",
        "                 'h_dim_CS': SET_NODES[np.random.randint(len(SET_NODES))],\n",
        "                 'num_layers_shared':SET_LAYERS[np.random.randint(len(SET_LAYERS))],\n",
        "                 'num_layers_CS':SET_LAYERS[np.random.randint(len(SET_LAYERS))],\n",
        "                 'active_fn': SET_ACTIVATION_FN[np.random.randint(len(SET_ACTIVATION_FN))],\n",
        "\n",
        "                 'alpha':1.0, #default (set alpha = 1.0 and change beta and gamma)\n",
        "                 'beta':SET_BETA[np.random.randint(len(SET_BETA))],\n",
        "                 'gamma':0,   #default (no calibration loss)\n",
        "                 # 'alpha':SET_ALPHA[np.random.randint(len(SET_ALPHA))],\n",
        "                 # 'beta':SET_BETA[np.random.randint(len(SET_BETA))],\n",
        "                 # 'gamma':SET_GAMMA[np.random.randint(len(SET_GAMMA))],\n",
        "\n",
        "                 'out_path':out_path}\n",
        "    \n",
        "    return new_parser #outputs the dictionary of the randomly-chosen hyperparamters\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##### MAIN SETTING\n",
        "OUT_ITERATION               = 5\n",
        "RS_ITERATION                = 50\n",
        "\n",
        "data_mode                   = 'METABRIC'\n",
        "seed                        = 1234\n",
        "\n",
        "\n",
        "##### IMPORT DATASET\n",
        "'''\n",
        "    num_Category            = typically, max event/censoring time * 1.2 (to make enough time horizon)\n",
        "    num_Event               = number of evetns i.e. len(np.unique(label))-1\n",
        "    max_length              = maximum number of measurements\n",
        "    x_dim                   = data dimension including delta (num_features)\n",
        "    mask1, mask2            = used for cause-specific network (FCNet structure)\n",
        "    EVAL_TIMES              = set specific evaluation time horizons at which the validatoin performance is maximized. \n",
        "    \t\t\t\t\t\t  (This must be selected based on the dataset)\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IwxrCNc9MtHo"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This provide the dimension/data/mask to train/test the network.\n",
        "Once must construct a function similar to \"import_dataset_SYNTHETIC\":\n",
        "    - DATA FORMAT:\n",
        "        > data: covariates with x_dim dimension.\n",
        "        > label: 0: censoring, 1 ~ K: K competing(single) risk(s)\n",
        "        > time: time-to-event or time-to-censoring\n",
        "    - Based on the data, creat mask1 and mask2 that are required to calculate loss functions.\n",
        "'''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "\n",
        "##### DEFINE USER-FUNCTIONS #####\n",
        "def f_get_Normalization(X, norm_mode):\n",
        "    num_Patient, num_Feature = np.shape(X)\n",
        "\n",
        "    if norm_mode == 'standard': #zero mean unit variance\n",
        "        for j in range(num_Feature):\n",
        "            if np.std(X[:,j]) != 0:\n",
        "                X[:,j] = (X[:,j] - np.mean(X[:, j]))/np.std(X[:,j])\n",
        "            else:\n",
        "                X[:,j] = (X[:,j] - np.mean(X[:, j]))\n",
        "    elif norm_mode == 'normal': #min-max normalization\n",
        "        for j in range(num_Feature):\n",
        "            X[:,j] = (X[:,j] - np.min(X[:,j]))/(np.max(X[:,j]) - np.min(X[:,j]))\n",
        "    else:\n",
        "        print(\"INPUT MODE ERROR!\")\n",
        "\n",
        "    return X\n",
        "\n",
        "### MASK FUNCTIONS\n",
        "'''\n",
        "    fc_mask2      : To calculate LOSS_1 (log-likelihood loss)\n",
        "    fc_mask3      : To calculate LOSS_2 (ranking loss)\n",
        "'''\n",
        "def f_get_fc_mask2(time, label, num_Event, num_Category):\n",
        "    '''\n",
        "        mask4 is required to get the log-likelihood loss\n",
        "        mask4 size is [N, num_Event, num_Category]\n",
        "            if not censored : one element = 1 (0 elsewhere)\n",
        "            if censored     : fill elements with 1 after the censoring time (for all events)\n",
        "    '''\n",
        "    mask = np.zeros([np.shape(time)[0], num_Event, num_Category]) # for the first loss function\n",
        "    for i in range(np.shape(time)[0]):\n",
        "        if label[i,0] != 0:  #not censored\n",
        "            mask[i,int(label[i,0]-1),int(time[i,0])] = 1\n",
        "        else: #label[i,2]==0: censored\n",
        "            mask[i,:,int(time[i,0]+1):] =  1 #fill 1 until from the censoring time (to get 1 - \\sum F)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def f_get_fc_mask3(time, meas_time, num_Category):\n",
        "    '''\n",
        "        mask5 is required calculate the ranking loss (for pair-wise comparision)\n",
        "        mask5 size is [N, num_Category].\n",
        "        - For longitudinal measurements:\n",
        "             1's from the last measurement to the event time (exclusive and inclusive, respectively)\n",
        "             denom is not needed since comparing is done over the same denom\n",
        "        - For single measurement:\n",
        "             1's from start to the event time(inclusive)\n",
        "    '''\n",
        "    mask = np.zeros([np.shape(time)[0], num_Category]) # for the first loss function\n",
        "    if np.shape(meas_time):  #lonogitudinal measurements\n",
        "        for i in range(np.shape(time)[0]):\n",
        "            t1 = int(meas_time[i, 0]) # last measurement time\n",
        "            t2 = int(time[i, 0]) # censoring/event time\n",
        "            mask[i,(t1+1):(t2+1)] = 1  #this excludes the last measurement time and includes the event time\n",
        "    else:                    #single measurement\n",
        "        for i in range(np.shape(time)[0]):\n",
        "            t = int(time[i, 0]) # censoring/event time\n",
        "            mask[i,:(t+1)] = 1  #this excludes the last measurement time and includes the event time\n",
        "    return mask\n",
        "\n",
        "\n",
        "def import_dataset_SYNTHETIC(norm_mode='standard'):\n",
        "    in_filename = './sample data/SYNTHETIC/synthetic_comprisk.csv'\n",
        "    df = pd.read_csv(in_filename, sep=',')\n",
        "    \n",
        "    label           = np.asarray(df[['label']])\n",
        "    time            = np.asarray(df[['time']])\n",
        "    data            = np.asarray(df.iloc[:,4:])\n",
        "    data            = f_get_Normalization(data, norm_mode)\n",
        "\n",
        "    num_Category    = int(np.max(time) * 2)  #to have enough time-horizon\n",
        "    num_Event       = int(len(np.unique(label)) - 1) #only count the number of events (do not count censoring as an event)\n",
        "\n",
        "    x_dim           = np.shape(data)[1]\n",
        "\n",
        "    mask1           = f_get_fc_mask2(time, label, num_Event, num_Category)\n",
        "    mask2           = f_get_fc_mask3(time, -1, num_Category)\n",
        "\n",
        "    DIM             = (x_dim)\n",
        "    DATA            = (data, time, label)\n",
        "    MASK            = (mask1, mask2)\n",
        "\n",
        "    return DIM, DATA, MASK\n",
        "\n",
        "\n",
        "def import_dataset_METABRIC(norm_mode='standard'):\n",
        "    in_filename1 = './sample data/METABRIC/cleaned_features_final.csv'\n",
        "    in_filename2 = './sample data/METABRIC/label.csv'\n",
        "\n",
        "    df1 = pd.read_csv(in_filename1, sep =',')\n",
        "    df2 = pd.read_csv(in_filename2, sep =',')\n",
        "\n",
        "    data  = np.asarray(df1)\n",
        "    data  = f_get_Normalization(data, norm_mode)\n",
        "    \n",
        "    time  = np.asarray(df2[['event_time']])\n",
        "    # time  = np.round(time/12.) #unit time = month\n",
        "    label = np.asarray(df2[['label']])\n",
        "\n",
        "    \n",
        "    num_Category    = int(np.max(time) * 1.2)        #to have enough time-horizon\n",
        "    num_Event       = int(len(np.unique(label)) - 1) #only count the number of events (do not count censoring as an event)\n",
        "\n",
        "    x_dim           = np.shape(data)[1]\n",
        "\n",
        "    mask1           = f_get_fc_mask2(time, label, num_Event, num_Category)\n",
        "    mask2           = f_get_fc_mask3(time, -1, num_Category)\n",
        "\n",
        "    DIM             = (x_dim)\n",
        "    DATA            = (data, time, label)\n",
        "    MASK            = (mask1, mask2)\n",
        "\n",
        "    return DIM, DATA, MASK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "z_9zJn8QKVQ0"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This train DeepHit, and outputs the validation performance for random search.\n",
        "\n",
        "INPUTS:\n",
        "    - DATA = (data, time, label)\n",
        "    - MASK = (mask1, mask2)\n",
        "    - in_parser: dictionary of hyperparameters\n",
        "    - out_itr: the training/testing split indicator\n",
        "    - eval_time: None or a list (e.g. [12, 24, 36]) at which the validation of the network is performed\n",
        "    - MAX_VALUE: maximum validation value\n",
        "    - OUT_ITERATION: total number of training/testing splits\n",
        "    - seed: random seed for training/testing/validation\n",
        "\n",
        "OUTPUTS:\n",
        "    - the validation performance of the trained network\n",
        "    - save the trained network in the folder directed by \"in_parser['out_path'] + '/itr_' + str(out_itr)\"\n",
        "'''\n",
        "\n",
        "_EPSILON = 1e-08\n",
        "'''\n",
        "This provide time-dependent Concordance index and Brier Score:\n",
        "    - Use weighted_c_index and weighted_brier_score, which are the unbiased estimates.\n",
        "    \n",
        "See equations and descriptions eq. (11) and (12) of the following paper:\n",
        "    - C. Lee, W. R. Zame, A. Alaa, M. van der Schaar, \"Temporal Quilting for Survival Analysis\", AISTATS 2019\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "from lifelines import KaplanMeierFitter\n",
        "\n",
        "\n",
        "### C(t)-INDEX CALCULATION\n",
        "def c_index(Prediction, Time_survival, Death, Time):\n",
        "    '''\n",
        "        This is a cause-specific c(t)-index\n",
        "        - Prediction      : risk at Time (higher --> more risky)\n",
        "        - Time_survival   : survival/censoring time\n",
        "        - Death           :\n",
        "            > 1: death\n",
        "            > 0: censored (including death from other cause)\n",
        "        - Time            : time of evaluation (time-horizon when evaluating C-index)\n",
        "    '''\n",
        "    N = len(Prediction)\n",
        "    A = np.zeros((N,N))\n",
        "    Q = np.zeros((N,N))\n",
        "    N_t = np.zeros((N,N))\n",
        "    Num = 0\n",
        "    Den = 0\n",
        "    for i in range(N):\n",
        "        A[i, np.where(Time_survival[i] < Time_survival)] = 1\n",
        "        Q[i, np.where(Prediction[i] > Prediction)] = 1\n",
        "  \n",
        "        if (Time_survival[i]<=Time and Death[i]==1):\n",
        "            N_t[i,:] = 1\n",
        "\n",
        "    Num  = np.sum(((A)*N_t)*Q)\n",
        "    Den  = np.sum((A)*N_t)\n",
        "\n",
        "    if Num == 0 and Den == 0:\n",
        "        result = -1 # not able to compute c-index!\n",
        "    else:\n",
        "        result = float(Num/Den)\n",
        "\n",
        "    return result\n",
        "\n",
        "### BRIER-SCORE\n",
        "def brier_score(Prediction, Time_survival, Death, Time):\n",
        "    N = len(Prediction)\n",
        "    y_true = ((Time_survival <= Time) * Death).astype(float)\n",
        "\n",
        "    return np.mean((Prediction - y_true)**2)\n",
        "\n",
        "    # result2[k, t] = brier_score_loss(risk[:, k], ((te_time[:,0] <= eval_horizon) * (te_label[:,0] == k+1)).astype(int))\n",
        "\n",
        "\n",
        "##### WEIGHTED C-INDEX & BRIER-SCORE\n",
        "def CensoringProb(Y, T):\n",
        "\n",
        "    T = T.reshape([-1]) # (N,) - np array\n",
        "    Y = Y.reshape([-1]) # (N,) - np array\n",
        "\n",
        "    kmf = KaplanMeierFitter()\n",
        "    kmf.fit(T, event_observed=(Y==0).astype(int))  # censoring prob = survival probability of event \"censoring\"\n",
        "    G = np.asarray(kmf.survival_function_.reset_index()).transpose()\n",
        "    G[1, G[1, :] == 0] = G[1, G[1, :] != 0][-1]  #fill 0 with ZoH (to prevent nan values)\n",
        "    \n",
        "    return G\n",
        "\n",
        "\n",
        "### C(t)-INDEX CALCULATION: this account for the weighted average for unbaised estimation\n",
        "def weighted_c_index(T_train, Y_train, Prediction, T_test, Y_test, Time):\n",
        "    '''\n",
        "        This is a cause-specific c(t)-index\n",
        "        - Prediction      : risk at Time (higher --> more risky)\n",
        "        - Time_survival   : survival/censoring time\n",
        "        - Death           :\n",
        "            > 1: death\n",
        "            > 0: censored (including death from other cause)\n",
        "        - Time            : time of evaluation (time-horizon when evaluating C-index)\n",
        "    '''\n",
        "    G = CensoringProb(Y_train, T_train)\n",
        "\n",
        "    N = len(Prediction)\n",
        "    A = np.zeros((N,N))\n",
        "    Q = np.zeros((N,N))\n",
        "    N_t = np.zeros((N,N))\n",
        "    Num = 0\n",
        "    Den = 0\n",
        "    for i in range(N):\n",
        "        tmp_idx = np.where(G[0,:] >= T_test[i])[0]\n",
        "\n",
        "        if len(tmp_idx) == 0:\n",
        "            W = (1./G[1, -1])**2\n",
        "        else:\n",
        "            W = (1./G[1, tmp_idx[0]])**2\n",
        "\n",
        "        A[i, np.where(T_test[i] < T_test)] = 1. * W\n",
        "        Q[i, np.where(Prediction[i] > Prediction)] = 1. # give weights\n",
        "\n",
        "        if (T_test[i]<=Time and Y_test[i]==1):\n",
        "            N_t[i,:] = 1.\n",
        "\n",
        "    Num  = np.sum(((A)*N_t)*Q)\n",
        "    Den  = np.sum((A)*N_t)\n",
        "\n",
        "    if Num == 0 and Den == 0:\n",
        "        result = -1 # not able to compute c-index!\n",
        "    else:\n",
        "        result = float(Num/Den)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# this account for the weighted average for unbaised estimation\n",
        "def weighted_brier_score(T_train, Y_train, Prediction, T_test, Y_test, Time):\n",
        "    G = CensoringProb(Y_train, T_train)\n",
        "    N = len(Prediction)\n",
        "\n",
        "    W = np.zeros(len(Y_test))\n",
        "    Y_tilde = (T_test > Time).astype(float)\n",
        "\n",
        "    for i in range(N):\n",
        "        tmp_idx1 = np.where(G[0,:] >= T_test[i])[0]\n",
        "        tmp_idx2 = np.where(G[0,:] >= Time)[0]\n",
        "\n",
        "        if len(tmp_idx1) == 0:\n",
        "            G1 = G[1, -1]\n",
        "        else:\n",
        "            G1 = G[1, tmp_idx1[0]]\n",
        "\n",
        "        if len(tmp_idx2) == 0:\n",
        "            G2 = G[1, -1]\n",
        "        else:\n",
        "            G2 = G[1, tmp_idx2[0]]\n",
        "        W[i] = (1. - Y_tilde[i])*float(Y_test[i])/G1 + Y_tilde[i]/G2\n",
        "\n",
        "    y_true = ((T_test <= Time) * Y_test).astype(float)\n",
        "\n",
        "    return np.mean(W*(Y_tilde - (1.-Prediction))**2)\n",
        "\n",
        "'''\n",
        "First implemented: 01/25/2018\n",
        "  > For survival analysis on longitudinal dataset\n",
        "By CHANGHEE LEE\n",
        "Modifcation List:\n",
        "\t- 08/07/2018: weight regularization for FC_NET is added\n",
        "'''\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.contrib.layers import fully_connected as FC_Net\n",
        "\n",
        "\n",
        "### CONSTRUCT MULTICELL FOR MULTI-LAYER RNNS\n",
        "def create_rnn_cell(num_units, num_layers, keep_prob, RNN_type): \n",
        "    '''\n",
        "        GOAL         : create multi-cell (including a single cell) to construct multi-layer RNN\n",
        "        num_units    : number of units in each layer\n",
        "        num_layers   : number of layers in MulticellRNN\n",
        "        keep_prob    : keep probabilty [0, 1]  (if None, dropout is not employed)\n",
        "        RNN_type     : either 'LSTM' or 'GRU'\n",
        "    '''\n",
        "    cells = []\n",
        "    for _ in range(num_layers):\n",
        "        if RNN_type == 'GRU':\n",
        "            cell = tf.contrib.rnn.GRUCell(num_units)\n",
        "        elif RNN_type == 'LSTM':\n",
        "            cell = tf.contrib.rnn.LSTMCell(num_units)\n",
        "        if not keep_prob is None:\n",
        "            cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
        "        cells.append(cell)\n",
        "    cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
        "    \n",
        "    return cell\n",
        "\n",
        "\n",
        "### EXTRACT STATE OUTPUT OF MULTICELL-RNNS\n",
        "def create_concat_state(state, num_layers, RNN_type):\n",
        "    '''\n",
        "        GOAL\t     : concatenate the tuple-type tensor (state) into a single tensor\n",
        "        state        : input state is a tuple ofo MulticellRNN (i.e. output of MulticellRNN)\n",
        "                       consist of only hidden states h for GRU and hidden states c and h for LSTM\n",
        "        num_layers   : number of layers in MulticellRNN\n",
        "        RNN_type     : either 'LSTM' or 'GRU'\n",
        "    '''\n",
        "    for i in range(num_layers):\n",
        "        if RNN_type == 'LSTM':\n",
        "            tmp = state[i][1] ## i-th layer, h state for LSTM\n",
        "        elif RNN_type == 'GRU':\n",
        "            tmp = state[i] ## i-th layer, h state for GRU\n",
        "        else:\n",
        "            print('ERROR: WRONG RNN CELL TYPE')\n",
        "\n",
        "        if i == 0:\n",
        "            rnn_state_out = tmp\n",
        "        else:\n",
        "            rnn_state_out = tf.concat([rnn_state_out, tmp], axis = 1)\n",
        "    \n",
        "    return rnn_state_out\n",
        "\n",
        "\n",
        "### FEEDFORWARD NETWORK\n",
        "def create_FCNet(inputs, num_layers, h_dim, h_fn, o_dim, o_fn, w_init, keep_prob=1.0, w_reg=None):\n",
        "    '''\n",
        "        GOAL             : Create FC network with different specifications \n",
        "        inputs (tensor)  : input tensor\n",
        "        num_layers       : number of layers in FCNet\n",
        "        h_dim  (int)     : number of hidden units\n",
        "        h_fn             : activation function for hidden layers (default: tf.nn.relu)\n",
        "        o_dim  (int)     : number of output units\n",
        "        o_fn             : activation function for output layers (defalut: None)\n",
        "        w_init           : initialization for weight matrix (defalut: Xavier)\n",
        "        keep_prob        : keep probabilty [0, 1]  (if None, dropout is not employed)\n",
        "    '''\n",
        "    # default active functions (hidden: relu, out: None)\n",
        "    if h_fn is None:\n",
        "        h_fn = tf.nn.relu\n",
        "    if o_fn is None:\n",
        "        o_fn = None\n",
        "\n",
        "    # default initialization functions (weight: Xavier, bias: None)\n",
        "    if w_init is None:\n",
        "        w_init = tf.contrib.layers.xavier_initializer() # Xavier initialization\n",
        "\n",
        "    for layer in range(num_layers):\n",
        "        if num_layers == 1:\n",
        "            out = FC_Net(inputs, o_dim, activation_fn=o_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
        "        else:\n",
        "            if layer == 0:\n",
        "                h = FC_Net(inputs, h_dim, activation_fn=h_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
        "                if not keep_prob is None:\n",
        "                    h = tf.nn.dropout(h, keep_prob=keep_prob)\n",
        "\n",
        "            elif layer > 0 and layer != (num_layers-1): # layer > 0:\n",
        "                h = FC_Net(h, h_dim, activation_fn=h_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
        "                if not keep_prob is None:\n",
        "                    h = tf.nn.dropout(h, keep_prob=keep_prob)\n",
        "\n",
        "            else: # layer == num_layers-1 (the last layer)\n",
        "                out = FC_Net(h, o_dim, activation_fn=o_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def _f_get_pred(sess, model, data, data_mi, pred_horizon):\n",
        "    '''\n",
        "        predictions based on the prediction time.\n",
        "        create new_data and new_mask2 that are available previous or equal to the prediction time (no future measurements are used)\n",
        "    '''\n",
        "    new_data    = np.zeros(np.shape(data))\n",
        "    new_data_mi = np.zeros(np.shape(data_mi))\n",
        "\n",
        "    meas_time = np.concatenate([np.zeros([np.shape(data)[0], 1]), np.cumsum(data[:, :, 0], axis=1)[:, :-1]], axis=1)\n",
        "\n",
        "    for i in range(np.shape(data)[0]):\n",
        "        last_meas = np.sum(meas_time[i, :] <= pred_horizon)\n",
        "\n",
        "        new_data[i, :last_meas, :]    = data[i, :last_meas, :]\n",
        "        new_data_mi[i, :last_meas, :] = data_mi[i, :last_meas, :]\n",
        "\n",
        "    return model.predict(new_data, new_data_mi)\n",
        "\n",
        "\n",
        "def f_get_risk_predictions(sess, model, data_, data_mi_, pred_time, eval_time):\n",
        "    \n",
        "    pred = _f_get_pred(sess, model, data_[[0]], data_mi_[[0]], 0)\n",
        "    _, num_Event, num_Category = np.shape(pred)\n",
        "       \n",
        "    risk_all = {}\n",
        "    for k in range(num_Event):\n",
        "        risk_all[k] = np.zeros([np.shape(data_)[0], len(pred_time), len(eval_time)])\n",
        "            \n",
        "    for p, p_time in enumerate(pred_time):\n",
        "        ### PREDICTION\n",
        "        pred_horizon = int(p_time)\n",
        "        pred = _f_get_pred(sess, model, data_, data_mi_, pred_horizon)\n",
        "\n",
        "\n",
        "        for t, t_time in enumerate(eval_time):\n",
        "            eval_horizon = int(t_time) + pred_horizon #if eval_horizon >= num_Category, output the maximum...\n",
        "\n",
        "            # calculate F(t | x, Y, t >= t_M) = \\sum_{t_M <= \\tau < t} P(\\tau | x, Y, \\tau > t_M)\n",
        "            risk = np.sum(pred[:,:,pred_horizon:(eval_horizon+1)], axis=2) #risk score until eval_time\n",
        "            risk = risk / (np.sum(np.sum(pred[:,:,pred_horizon:], axis=2), axis=1, keepdims=True) +_EPSILON) #conditioniong on t > t_pred\n",
        "            \n",
        "            for k in range(num_Event):\n",
        "                risk_all[k][:, p, t] = risk[:, k]\n",
        "                \n",
        "    return risk_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn_8eUccKVSa"
      },
      "source": [
        "## Import function \n",
        "#####      - Users must prepare dataset in csv format and modify 'import_data.py' following our examplar 'PBC2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-0NscAIBKVSt"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This provide the dimension/data/mask to train/test the network.\n",
        "Once must construct a function similar to \"import_dataset_SYNTHETIC\":\n",
        "    - DATA FORMAT:\n",
        "        > data: covariates with x_dim dimension.\n",
        "        > label: 0: censoring, 1 ~ K: K competing(single) risk(s)\n",
        "        > time: time-to-event or time-to-censoring\n",
        "    - Based on the data, creat mask1 and mask2 that are required to calculate loss functions.\n",
        "'''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "\n",
        "##### DEFINE USER-FUNCTIONS #####\n",
        "def f_get_Normalization(X, norm_mode):\n",
        "    num_Patient, num_Feature = np.shape(X)\n",
        "\n",
        "    if norm_mode == 'standard': #zero mean unit variance\n",
        "        for j in range(num_Feature):\n",
        "            if np.std(X[:,j]) != 0:\n",
        "                X[:,j] = (X[:,j] - np.mean(X[:, j]))/np.std(X[:,j])\n",
        "            else:\n",
        "                X[:,j] = (X[:,j] - np.mean(X[:, j]))\n",
        "    elif norm_mode == 'normal': #min-max normalization\n",
        "        for j in range(num_Feature):\n",
        "            X[:,j] = (X[:,j] - np.min(X[:,j]))/(np.max(X[:,j]) - np.min(X[:,j]))\n",
        "    else:\n",
        "        print(\"INPUT MODE ERROR!\")\n",
        "\n",
        "    return X\n",
        "\n",
        "### MASK FUNCTIONS\n",
        "'''\n",
        "    fc_mask2      : To calculate LOSS_1 (log-likelihood loss)\n",
        "    fc_mask3      : To calculate LOSS_2 (ranking loss)\n",
        "'''\n",
        "def f_get_fc_mask2(time, label, num_Event, num_Category):\n",
        "    '''\n",
        "        mask4 is required to get the log-likelihood loss\n",
        "        mask4 size is [N, num_Event, num_Category]\n",
        "            if not censored : one element = 1 (0 elsewhere)\n",
        "            if censored     : fill elements with 1 after the censoring time (for all events)\n",
        "    '''\n",
        "    mask = np.zeros([np.shape(time)[0], num_Event, num_Category]) # for the first loss function\n",
        "    for i in range(np.shape(time)[0]):\n",
        "        if label[i,0] != 0:  #not censored\n",
        "            mask[i,int(label[i,0]-1),int(time[i,0])] = 1\n",
        "        else: #label[i,2]==0: censored\n",
        "            mask[i,:,int(time[i,0]+1):] =  1 #fill 1 until from the censoring time (to get 1 - \\sum F)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def f_get_fc_mask3(time, meas_time, num_Category):\n",
        "    '''\n",
        "        mask5 is required calculate the ranking loss (for pair-wise comparision)\n",
        "        mask5 size is [N, num_Category].\n",
        "        - For longitudinal measurements:\n",
        "             1's from the last measurement to the event time (exclusive and inclusive, respectively)\n",
        "             denom is not needed since comparing is done over the same denom\n",
        "        - For single measurement:\n",
        "             1's from start to the event time(inclusive)\n",
        "    '''\n",
        "    mask = np.zeros([np.shape(time)[0], num_Category]) # for the first loss function\n",
        "    if np.shape(meas_time):  #lonogitudinal measurements\n",
        "        for i in range(np.shape(time)[0]):\n",
        "            t1 = int(meas_time[i, 0]) # last measurement time\n",
        "            t2 = int(time[i, 0]) # censoring/event time\n",
        "            mask[i,(t1+1):(t2+1)] = 1  #this excludes the last measurement time and includes the event time\n",
        "    else:                    #single measurement\n",
        "        for i in range(np.shape(time)[0]):\n",
        "            t = int(time[i, 0]) # censoring/event time\n",
        "            mask[i,:(t+1)] = 1  #this excludes the last measurement time and includes the event time\n",
        "    return mask\n",
        "\n",
        "\n",
        "\n",
        "def import_dataset_METABRIC_mod(norm_mode='standard'):\n",
        "    in_filename1 = '/content/data_train_boot.txt'\n",
        "    df1 = pd.read_csv(in_filename1, sep =',')\n",
        "    df1[['SESSO']] = np.where(df1[['SESSO']]=='F',1,0) \n",
        "    df1[['anno_inizio_ARV']] = np.where(df1[['anno_inizio_ARV']]=='<=2007',1,0) \n",
        "    df1[['FDRn']] = np.where(df1[['FDRn']]=='MSM',1,0)\n",
        "    df1[['HCV']] = np.float32(np.where(df1[['HCV']]=='non noto',0,df1[['HCV']]))\n",
        "    df1[['HBV']] = np.float32(np.where(df1[['HBV']]=='non noto',0,df1[['HBV']]))\n",
        "\n",
        "\n",
        "    bin_list           = [ 'anno_inizio_ARV' ,  'HCV' ]\n",
        "    cont_list          = [ 'AGE_UPD' ,'ipert_si_no' ,  'CD4' ,  'VIREMIA' , 'PLT' ,\n",
        "                            'TRIG' , 'CREATININA'  , \"PI_time_exp\" , 'NRTI_time_exp' , 'NNRTI_time_exp']\n",
        "    feat_list          = cont_list + bin_list\n",
        "    \n",
        "    df1[['label']] = df1[['ID_CVD']]\n",
        "    df2 = df1.loc[:,['label','time']]\n",
        "    df1 = df1.loc[:,feat_list]\n",
        "    data  = np.asarray(df1)\n",
        "    data  = f_get_Normalization(data, norm_mode)\n",
        "    time  = np.asarray(df2[['time']])\n",
        "    # time  = np.round(time/12.) #unit time = month\n",
        "    label = np.asarray(df2[['label']])\n",
        "\n",
        "    \n",
        "    num_Category    = int(np.max(time) * 1.2)        #to have enough time-horizon\n",
        "    num_Event       = int(len(np.unique(label)) - 1) #only count the number of events (do not count censoring as an event)\n",
        "\n",
        "    x_dim           = np.shape(data)[1]\n",
        "\n",
        "    mask1           = f_get_fc_mask2(time, label, num_Event, num_Category)\n",
        "    mask2           = f_get_fc_mask3(time, -1, num_Category)\n",
        "\n",
        "    DIM             = (x_dim)\n",
        "    DATA            = (data, time, label)\n",
        "    MASK            = (mask1, mask2)\n",
        "    return DIM, DATA, MASK\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziNjVgIZKVS2"
      },
      "source": [
        "# Function for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "U6C2_e0TKVS4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_valid_performance_mod(DATA, MASK, DATATE , MASKTE , in_parser, out_itr, eval_time=None, MAX_VALUE = -99, early_stop=7 , \n",
        "                              OUT_ITERATION=5, seed=1234):\n",
        "    ##### DATA & MASK\n",
        "    (tr_data, tr_time, tr_label)  = DATA\n",
        "    (tr_mask1, tr_mask2)       = MASK\n",
        "    (datate, timete, labelte)  = DATATE\n",
        "    (mask1te, mask2te)       = MASKTE\n",
        "\n",
        "    x_dim                       = np.shape(tr_data)[1]\n",
        "    _, num_Event, num_Category  = np.shape(tr_mask1)  # dim of mask1: [subj, Num_Event, Num_Category]\n",
        "    \n",
        "    ACTIVATION_FN               = {'relu': tf.nn.relu, 'elu': tf.nn.elu, 'tanh': tf.nn.tanh}\n",
        "\n",
        "    ##### HYPER-PARAMETERS\n",
        "    mb_size                     = in_parser['mb_size']\n",
        "\n",
        "    iteration                   = in_parser['iteration']\n",
        "\n",
        "    keep_prob                   = in_parser['keep_probability']\n",
        "    lr_train                    = in_parser['lr_train']\n",
        "\n",
        "\n",
        "    alpha                       = in_parser['alpha']  #for log-likelihood loss\n",
        "    beta                        = in_parser['beta']  #for ranking loss\n",
        "    parameter_name              = 'a' + str('%02.0f' %(10*alpha)) + 'b' + str('%02.0f' %(10*beta)) \n",
        "\n",
        "    initial_W                   = tf.contrib.layers.xavier_initializer()\n",
        "\n",
        "\n",
        "    ##### MAKE DICTIONARIES\n",
        "    # INPUT DIMENSIONS\n",
        "    input_dims                  = { 'x_dim'         : x_dim,\n",
        "                                    'num_Event'     : num_Event,\n",
        "                                    'num_Category'  : num_Category}\n",
        "\n",
        "    # NETWORK HYPER-PARMETERS\n",
        "    network_settings            = { 'h_dim_shared'       : in_parser['h_dim_shared'],\n",
        "                                    'num_layers_shared'  : in_parser['num_layers_shared'],\n",
        "                                    'h_dim_CS'           : in_parser['h_dim_CS'],\n",
        "                                    'num_layers_CS'      : in_parser['num_layers_CS'],\n",
        "                                    'active_fn'          : ACTIVATION_FN[in_parser['active_fn']],\n",
        "                                    'initial_W'          : initial_W }\n",
        "\n",
        "\n",
        "    file_path_final = in_parser['out_path'] + '/itr_' + str(out_itr)\n",
        "\n",
        "    #change parameters...\n",
        "    if not os.path.exists(file_path_final + '/models/'):\n",
        "        os.makedirs(file_path_final + '/models/')\n",
        "\n",
        "\n",
        "    print (file_path_final + ' (a:' + str(alpha) + ' b:' + str(beta)  + ')' )\n",
        "\n",
        "    ##### CREATE DEEPFHT NETWORK\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    sess = tf.Session(config=config)\n",
        "\n",
        "    model = Model_DeepHit(sess, \"DeepHit\", input_dims, network_settings)\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "    n =  np.shape(tr_data)[0]\n",
        "    it_per_epo = int(n / mb_size)\n",
        "    max_valid = -99\n",
        "    stop_flag = 0\n",
        "\n",
        "    if eval_time is None:\n",
        "        eval_time = [int(np.percentile(tr_time, 25)), int(np.percentile(tr_time, 50)), int(np.percentile(tr_time, 75))]\n",
        "\n",
        "\n",
        "    ### TRAINING - MAIN\n",
        "    print( \"MAIN TRAINING ...\")\n",
        "    print( \"EVALUATION TIMES: \" + str(eval_time))\n",
        "    tmp_tr = 0\n",
        "    avg_loss = 0\n",
        "    c =[]\n",
        "    ct =[]\n",
        "    for itr in range(iteration):\n",
        "        if stop_flag > early_stop: #for faster early stopping\n",
        "            break\n",
        "        \n",
        "        else:\n",
        "            x_mb, k_mb, t_mb, m1_mb, m2_mb = f_get_minibatch(mb_size, tr_data, tr_label, tr_time, tr_mask1, tr_mask2)\n",
        "            DATA = (x_mb, k_mb, t_mb)\n",
        "            MASK = (m1_mb, m2_mb)\n",
        "            PARAMETERS = (alpha, beta)\n",
        "            _, loss_curr = model.train(DATA, MASK, PARAMETERS, keep_prob, lr_train )\n",
        "            avg_loss += loss_curr/1000\n",
        "                \n",
        "            if (itr+1)%25 == 0:\n",
        "                print('|| ITR: ' + str('%04d' % (itr + 1)) + ' | Loss: ' + colored(str('%.4f' %(avg_loss)), 'yellow' , attrs=['bold']))\n",
        "                avg_loss = 0\n",
        "\n",
        "            ### VALIDATION  (based on average C-index of our interest)\n",
        "            if (itr+1)%it_per_epo == 0:\n",
        "                ### PREDICTION\n",
        "                pred = model.predict(datate)\n",
        "                pred_tr = model.predict(tr_data)\n",
        "                ### EVALUATION\n",
        "                va_result1 = np.zeros([num_Event, len(eval_time)])\n",
        "                tr_result1 = np.zeros([num_Event, len(eval_time)])\n",
        "\n",
        "                \n",
        "                for t, t_time in enumerate(eval_time):\n",
        "                    eval_horizon = int(t_time)\n",
        "\n",
        "                    if eval_horizon >= num_Category:\n",
        "                        print('ERROR: evaluation horizon is out of range')\n",
        "                        va_result1[:, t] = va_result2[:, t] = -1\n",
        "                        print('ccccccccccccc')\n",
        "                        tr_result1[:, t] = tr_result2[:, t] = -1\n",
        "                    else:\n",
        "                        risk = np.sum(pred[:,:,:(eval_horizon+1)], axis=2) #risk score until eval_time\n",
        "                        risk_tr = np.sum(pred_tr[:,:,:(eval_horizon+1)], axis=2) #risk score until eval_time\n",
        "                        for k in range(num_Event):\n",
        "                          if eval_horizon > 2:\n",
        "                            va_result1[k, t] = c_index(risk[:,k], timete, (labelte[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "                          tr_result1[k, t] = c_index(risk_tr[:,k], tr_time, (tr_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n",
        "                            #va_result1[k, t] = weighted_c_index(tr_time, (tr_label[:,0] == k+1).astype(int), risk[:,k], va_time, (va_label[:,0] == k+1).astype(int), eval_horizon)\n",
        "                tmp_valid = np.mean(va_result1)\n",
        "                tmp_tr = np.mean(tr_result1)\n",
        "\n",
        "                if tmp_valid >  max_valid:\n",
        "                    stop_flag = 0\n",
        "                    max_valid = tmp_valid\n",
        "                    best = model\n",
        "                    print( 'updated.... average c-index = ' + str('%.4f' %(tmp_valid)))\n",
        "                    print( 'updated.... average c-index training= ' + str('%.4f' %(tmp_tr)))\n",
        "\n",
        "                    if tmp_valid > MAX_VALUE:\n",
        "                        saver.save(sess, file_path_final + '/models/model_itr_' + str(out_itr))\n",
        "                else:\n",
        "                    stop_flag += 1\n",
        "                    print( 'average c-index = ' + str('%.4f' %(tmp_valid)))\n",
        "                    print( 'average c-index training= ' + str('%.4f' %(tmp_tr)))\n",
        "                c.append(tmp_valid)\n",
        "                ct.append(tmp_tr)\n",
        "    c= np.array(c)                \n",
        "    ct= np.array(ct) \n",
        "    return max_valid, c , ct , best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HypwMA4E6PpP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOLmsMDAKVS-"
      },
      "source": [
        "## Set Hyper-Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4QPoOufqKVS_"
      },
      "outputs": [],
      "source": [
        "data_mode                   = 'PBC2_mod' \n",
        "seed                        = 1235\n",
        "new_parser = {'mb_size': 32,\n",
        "\n",
        "             'iteration_burn_in': 5000,\n",
        "             'iteration': 24000,\n",
        "\n",
        "             'keep_probability': 0.8,\n",
        "             'lr_train': 2*1e-5,\n",
        "\n",
        "             'h_dim_shared': 75,\n",
        "             'h_dim_CS' : 75,\n",
        "             'num_layers_shared': 3,\n",
        "             'num_layers_CS' : 3,\n",
        "\n",
        "\n",
        "             'active_fn' : 'relu',\n",
        "           \n",
        "              'out_path' : 'C:/Users/AgostinoSevilla/Desktop/Tesi/iterazioni',\n",
        "             \n",
        "             'reg_W'         : 1e-6,\n",
        "             'reg_W_out'     : 0.55,\n",
        "\n",
        "             'alpha' :5.,\n",
        "             'beta'  :.5\n",
        "             \n",
        "             }\n",
        "##### IMPORT DATASET\n",
        "'''\n",
        "    num_Category            = max event/censoring time * 1.2\n",
        "    num_Event               = number of evetns i.e. len(np.unique(label))-1\n",
        "    max_length              = maximum number of measurements\n",
        "    x_dim                   = data dimension including delta (1 + num_features)\n",
        "    x_dim_cont              = dim of continuous features\n",
        "    x_dim_bin               = dim of binary features\n",
        "    mask1, mask2, mask3     = used for cause-specific network (FCNet structure)\n",
        "'''\n",
        "#_SYNTHETIC_mod\n",
        "if data_mode == 'PBC2_mod':\n",
        "    x_dim, DATA, MASK = import_dataset_METABRIC_mod(norm_mode = 'standard')\n",
        "    \n",
        "\n",
        "file_path = '{}'.format(data_mode)\n",
        "import os\n",
        "if not os.path.exists(file_path):\n",
        "    os.makedirs(file_path)\n",
        "burn_in_mode                = 'ON' #{'ON', 'OFF'}\n",
        "boost_mode                  = 'ON' #{'ON', 'OFF'}\n",
        "\n",
        "#   SET_ALPHA         = [0.1, 0.5, 1.0, 3.0, 5.0] #alpha values -> log-likelihood loss \n",
        "#   SET_BETA          = [0.1, 0.5, 1.0, 3.0, 5.0] #beta values -> ranking loss\n",
        "#   SET_GAMMA         = [0.1, 0.5, 1.0, 3.0, 5.0] #gamma values -> calibration loss\n",
        "##### HYPER-PARAMETERS\n",
        "\n",
        "\n",
        "\n",
        "def f_get_minibatch(mb_size, x, label, time, mask1, mask2):\n",
        "    idx = range(np.shape(x)[0])\n",
        "    idx = random.sample(idx, mb_size)\n",
        "\n",
        "    x_mb = x[idx, :].astype(np.float32)\n",
        "    k_mb = label[idx, :].astype(np.float32) # censoring(0)/event(1,2,..) label\n",
        "    t_mb = time[idx, :].astype(np.float32)\n",
        "    m1_mb = mask1[idx, :, :].astype(np.float32) #fc_mask\n",
        "    m2_mb = mask2[idx, :].astype(np.float32) #fc_mask\n",
        "    return x_mb, k_mb, t_mb, m1_mb, m2_mb\n",
        "\n",
        "# SAVE HYPERPARAMETERS\n",
        "log_name = file_path + '/hyperparameters_log.txt'\n",
        "save_logging(new_parser, log_name)\n",
        "def import_dataset_METABRIC_mod_test(norm_mode='standard' ):\n",
        "    in_filename1 = '/content/data_test_boost.txt'\n",
        "    df1 = pd.read_csv(in_filename1, sep =',')\n",
        "    df1['time_CVD1'] = np.float32(np.where( df1['time_CVD1'] == 'non evento' , 0 , df1['time_CVD1']))\n",
        "    df1[['SESSO']] = np.where(df1[['SESSO']]=='F',1,0) \n",
        "    df1[['anno_inizio_ARV']] = np.where(df1[['anno_inizio_ARV']]=='<=2007',1,0) \n",
        "    df1[['FDRn']] = np.where(df1[['FDRn']]=='MSM',1,0)\n",
        "    df1[['HCV']] = np.float32(np.where(df1[['HCV']]=='non noto',0,df1[['HCV']]))\n",
        "    df1[['HBV']] = np.float32(np.where(df1[['HBV']]=='non noto',0,df1[['HBV']]))\n",
        "    bin_list           = [ 'anno_inizio_ARV' ,  'HCV' ]\n",
        "    cont_list          = [ 'AGE_UPD' ,'ipert_si_no' ,  'CD4' ,  'VIREMIA' , 'PLT' ,\n",
        "                            'TRIG' , 'CREATININA'  , \"PI_time_exp\" , 'NRTI_time_exp' , 'NNRTI_time_exp']\n",
        "    feat_list          = cont_list + bin_list\n",
        " \n",
        "\n",
        "    df1[['time']] = np.where(df1[['ID_CVD']] == 1 , df1[['time_CVD1']] , df1[['end']])\n",
        "    df1[['label']] = df1[['ID_CVD']]\n",
        "\n",
        "    df2 = df1.loc[:,['label','time']]\n",
        "    df1 = df1.loc[:,feat_list]\n",
        "    data  = np.asarray(df1)\n",
        "    data  = f_get_Normalization(data, norm_mode)\n",
        "    time  = np.asarray(df2[['time']])\n",
        "    # time  = np.round(time/12.) #unit time = month\n",
        "    label = np.asarray(df2[['label']])\n",
        "    num_Category    = int(np.max(time) * 1.2)        #to have enough time-horizon\n",
        "    num_Event       = int(len(np.unique(label)) - 1) #only count the number of events (do not count censoring as an event)\n",
        "\n",
        "    x_dim           = np.shape(data)[1]\n",
        "\n",
        "    mask1           = f_get_fc_mask2(time, label, num_Event, num_Category)\n",
        "    mask2           = f_get_fc_mask3(time, -1, num_Category)\n",
        "\n",
        "    DIM             = (x_dim)\n",
        "    DATA            = (data, time, label)\n",
        "    MASK            = (mask1, mask2)\n",
        "    return DIM, DATA, MASK\n",
        "\n",
        "DIMte, DATAte, MASKte = import_dataset_METABRIC_mod_test()\n",
        "#(datate, timete, labelte)  = DATAte\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "h_rutUvV1qhL",
        "outputId": "a3836db2-587c-4bd3-c510-8650aacae721"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-83f6297c-605c-4928-8053-b4fc7a74260f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>ID</th>\n",
              "      <th>end</th>\n",
              "      <th>tstart</th>\n",
              "      <th>tstop</th>\n",
              "      <th>CVD1_COD_15</th>\n",
              "      <th>time_CVD1</th>\n",
              "      <th>SESSO</th>\n",
              "      <th>RAZZA</th>\n",
              "      <th>anno_inizio_ARV</th>\n",
              "      <th>FDRn</th>\n",
              "      <th>HCV</th>\n",
              "      <th>HBV</th>\n",
              "      <th>diabete_si_no</th>\n",
              "      <th>ipert_si_no</th>\n",
              "      <th>tumore_si_no</th>\n",
              "      <th>aids_si_no</th>\n",
              "      <th>AGE_UPD</th>\n",
              "      <th>VIREMIA</th>\n",
              "      <th>CD4</th>\n",
              "      <th>COLEST</th>\n",
              "      <th>Hb</th>\n",
              "      <th>PLT</th>\n",
              "      <th>TRIG</th>\n",
              "      <th>CREATININA</th>\n",
              "      <th>ALT</th>\n",
              "      <th>AST</th>\n",
              "      <th>INI_time_exp</th>\n",
              "      <th>PI_time_exp</th>\n",
              "      <th>NRTI_time_exp</th>\n",
              "      <th>NNRTI_time_exp</th>\n",
              "      <th>ID_CVD</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3099</td>\n",
              "      <td>133670</td>\n",
              "      <td>10450</td>\n",
              "      <td>4.156776</td>\n",
              "      <td>0</td>\n",
              "      <td>0.17522</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>M</td>\n",
              "      <td>1</td>\n",
              "      <td>&gt;2007</td>\n",
              "      <td>MSM</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>28.48186</td>\n",
              "      <td>2.947924</td>\n",
              "      <td>1088</td>\n",
              "      <td>167.0</td>\n",
              "      <td>16.1</td>\n",
              "      <td>222.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>0.92</td>\n",
              "      <td>17</td>\n",
              "      <td>19</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.196441</td>\n",
              "      <td>1.196441</td>\n",
              "      <td>0</td>\n",
              "      <td>4.156776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3025</td>\n",
              "      <td>138574</td>\n",
              "      <td>11282</td>\n",
              "      <td>8.860404</td>\n",
              "      <td>0</td>\n",
              "      <td>5.79055</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>M</td>\n",
              "      <td>2</td>\n",
              "      <td>&gt;2007</td>\n",
              "      <td>MSM</td>\n",
              "      <td>0</td>\n",
              "      <td>non noto</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>29.53867</td>\n",
              "      <td>0.278754</td>\n",
              "      <td>690</td>\n",
              "      <td>111.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>217.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1.50</td>\n",
              "      <td>30</td>\n",
              "      <td>30</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.418891</td>\n",
              "      <td>0.418891</td>\n",
              "      <td>0</td>\n",
              "      <td>8.860404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2256</td>\n",
              "      <td>144913</td>\n",
              "      <td>94193</td>\n",
              "      <td>0.737201</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00821</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>F</td>\n",
              "      <td>1</td>\n",
              "      <td>&gt;2007</td>\n",
              "      <td>HETERO</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39.11841</td>\n",
              "      <td>4.906879</td>\n",
              "      <td>217</td>\n",
              "      <td>154.0</td>\n",
              "      <td>12.2</td>\n",
              "      <td>80.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>0.91</td>\n",
              "      <td>18</td>\n",
              "      <td>23</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008214</td>\n",
              "      <td>0.008214</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.737201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1446</td>\n",
              "      <td>106585</td>\n",
              "      <td>8377</td>\n",
              "      <td>0.967180</td>\n",
              "      <td>0</td>\n",
              "      <td>0.91718</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>F</td>\n",
              "      <td>1</td>\n",
              "      <td>&gt;2007</td>\n",
              "      <td>OTH</td>\n",
              "      <td>non noto</td>\n",
              "      <td>non noto</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>43.66872</td>\n",
              "      <td>1.568202</td>\n",
              "      <td>179</td>\n",
              "      <td>279.0</td>\n",
              "      <td>11.7</td>\n",
              "      <td>309.0</td>\n",
              "      <td>161.0</td>\n",
              "      <td>0.74</td>\n",
              "      <td>60</td>\n",
              "      <td>54</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.610541</td>\n",
              "      <td>0.610541</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.967180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1283</td>\n",
              "      <td>134587</td>\n",
              "      <td>10516</td>\n",
              "      <td>15.124610</td>\n",
              "      <td>0</td>\n",
              "      <td>9.97399</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>M</td>\n",
              "      <td>1</td>\n",
              "      <td>&lt;=2007</td>\n",
              "      <td>MSM</td>\n",
              "      <td>0</td>\n",
              "      <td>non noto</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32.35044</td>\n",
              "      <td>0.278754</td>\n",
              "      <td>684</td>\n",
              "      <td>231.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>230.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>1.07</td>\n",
              "      <td>33</td>\n",
              "      <td>22</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.488706</td>\n",
              "      <td>7.887748</td>\n",
              "      <td>5.399042</td>\n",
              "      <td>0</td>\n",
              "      <td>15.124610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25258</th>\n",
              "      <td>519</td>\n",
              "      <td>101824</td>\n",
              "      <td>8126</td>\n",
              "      <td>11.757050</td>\n",
              "      <td>0</td>\n",
              "      <td>0.65161</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>F</td>\n",
              "      <td>1</td>\n",
              "      <td>&gt;2007</td>\n",
              "      <td>HETERO</td>\n",
              "      <td>non noto</td>\n",
              "      <td>non noto</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>49.77687</td>\n",
              "      <td>1.568202</td>\n",
              "      <td>368</td>\n",
              "      <td>229.0</td>\n",
              "      <td>13.1</td>\n",
              "      <td>265.0</td>\n",
              "      <td>98.0</td>\n",
              "      <td>0.47</td>\n",
              "      <td>24</td>\n",
              "      <td>26</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.539357</td>\n",
              "      <td>4.539357</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>11.757050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25259</th>\n",
              "      <td>1943</td>\n",
              "      <td>59240</td>\n",
              "      <td>5655</td>\n",
              "      <td>20.044520</td>\n",
              "      <td>0</td>\n",
              "      <td>1.16359</td>\n",
              "      <td>0</td>\n",
              "      <td>16.923</td>\n",
              "      <td>M</td>\n",
              "      <td>1</td>\n",
              "      <td>&lt;=2007</td>\n",
              "      <td>HETERO</td>\n",
              "      <td>0</td>\n",
              "      <td>non noto</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>53.11978</td>\n",
              "      <td>1.698970</td>\n",
              "      <td>702</td>\n",
              "      <td>205.0</td>\n",
              "      <td>12.1</td>\n",
              "      <td>200.0</td>\n",
              "      <td>158.0</td>\n",
              "      <td>0.60</td>\n",
              "      <td>21</td>\n",
              "      <td>22</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>20.044520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25260</th>\n",
              "      <td>406</td>\n",
              "      <td>134986</td>\n",
              "      <td>10540</td>\n",
              "      <td>5.262868</td>\n",
              "      <td>0</td>\n",
              "      <td>0.02190</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>M</td>\n",
              "      <td>1</td>\n",
              "      <td>&gt;2007</td>\n",
              "      <td>MSM</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>35.73443</td>\n",
              "      <td>6.157124</td>\n",
              "      <td>6</td>\n",
              "      <td>202.0</td>\n",
              "      <td>13.7</td>\n",
              "      <td>159.0</td>\n",
              "      <td>206.0</td>\n",
              "      <td>1.22</td>\n",
              "      <td>97</td>\n",
              "      <td>43</td>\n",
              "      <td>1.478439</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.478439</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>5.262868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25261</th>\n",
              "      <td>1347</td>\n",
              "      <td>53711</td>\n",
              "      <td>5401</td>\n",
              "      <td>19.691340</td>\n",
              "      <td>0</td>\n",
              "      <td>1.46749</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>M</td>\n",
              "      <td>1</td>\n",
              "      <td>&lt;=2007</td>\n",
              "      <td>MSM</td>\n",
              "      <td>0</td>\n",
              "      <td>non noto</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>37.76318</td>\n",
              "      <td>1.698970</td>\n",
              "      <td>549</td>\n",
              "      <td>118.0</td>\n",
              "      <td>16.7</td>\n",
              "      <td>249.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>0.94</td>\n",
              "      <td>474</td>\n",
              "      <td>294</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>19.691340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25262</th>\n",
              "      <td>1096</td>\n",
              "      <td>140332</td>\n",
              "      <td>11564</td>\n",
              "      <td>6.451095</td>\n",
              "      <td>0</td>\n",
              "      <td>3.97262</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>M</td>\n",
              "      <td>1</td>\n",
              "      <td>&gt;2007</td>\n",
              "      <td>MSM</td>\n",
              "      <td>0</td>\n",
              "      <td>non noto</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25.60712</td>\n",
              "      <td>0.278754</td>\n",
              "      <td>723</td>\n",
              "      <td>153.0</td>\n",
              "      <td>16.8</td>\n",
              "      <td>217.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.96</td>\n",
              "      <td>29</td>\n",
              "      <td>21</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.211499</td>\n",
              "      <td>3.211499</td>\n",
              "      <td>0</td>\n",
              "      <td>6.451095</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25263 rows × 34 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83f6297c-605c-4928-8053-b4fc7a74260f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-83f6297c-605c-4928-8053-b4fc7a74260f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-83f6297c-605c-4928-8053-b4fc7a74260f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       Unnamed: 0  Unnamed: 0.1     ID  ...  NNRTI_time_exp  ID_CVD       time\n",
              "0            3099        133670  10450  ...        1.196441       0   4.156776\n",
              "1            3025        138574  11282  ...        0.418891       0   8.860404\n",
              "2            2256        144913  94193  ...        0.000000       0   0.737201\n",
              "3            1446        106585   8377  ...        0.000000       0   0.967180\n",
              "4            1283        134587  10516  ...        5.399042       0  15.124610\n",
              "...           ...           ...    ...  ...             ...     ...        ...\n",
              "25258         519        101824   8126  ...        0.000000       0  11.757050\n",
              "25259        1943         59240   5655  ...        0.000000       0  20.044520\n",
              "25260         406        134986  10540  ...        0.000000       0   5.262868\n",
              "25261        1347         53711   5401  ...        0.000000       0  19.691340\n",
              "25262        1096        140332  11564  ...        3.211499       0   6.451095\n",
              "\n",
              "[25263 rows x 34 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "in_filename1 = '/content/data_train_boot.txt'\n",
        "\n",
        "\n",
        "df1 = pd.read_csv(in_filename1, sep =',')\n",
        "df1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2t1hdjchQhX"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCkj4ESMKVTC"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBGHEGfGKVTD",
        "outputId": "e292f471-2869-4186-b4b0-07f88e82201d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "C:/Users/AgostinoSevilla/Desktop/Tesi/iterazioni/itr_0.8 (a:2.0 b:0.5)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-14-1782104f821c>:253: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-11-41c13f68a02e>:116: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "MAIN TRAINING ...\n",
            "EVALUATION TIMES: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
            "|| ITR: 0025 | Loss: \u001b[1m\u001b[33m0.0394\u001b[0m\n",
            "|| ITR: 0050 | Loss: \u001b[1m\u001b[33m0.0365\u001b[0m\n",
            "|| ITR: 0075 | Loss: \u001b[1m\u001b[33m0.0399\u001b[0m\n",
            "|| ITR: 0100 | Loss: \u001b[1m\u001b[33m0.0391\u001b[0m\n",
            "|| ITR: 0125 | Loss: \u001b[1m\u001b[33m0.0360\u001b[0m\n",
            "|| ITR: 0150 | Loss: \u001b[1m\u001b[33m0.0375\u001b[0m\n",
            "|| ITR: 0175 | Loss: \u001b[1m\u001b[33m0.0360\u001b[0m\n",
            "|| ITR: 0200 | Loss: \u001b[1m\u001b[33m0.0361\u001b[0m\n",
            "|| ITR: 0225 | Loss: \u001b[1m\u001b[33m0.0366\u001b[0m\n",
            "|| ITR: 0250 | Loss: \u001b[1m\u001b[33m0.0382\u001b[0m\n",
            "|| ITR: 0275 | Loss: \u001b[1m\u001b[33m0.0352\u001b[0m\n",
            "|| ITR: 0300 | Loss: \u001b[1m\u001b[33m0.0372\u001b[0m\n",
            "|| ITR: 0325 | Loss: \u001b[1m\u001b[33m0.0350\u001b[0m\n",
            "|| ITR: 0350 | Loss: \u001b[1m\u001b[33m0.0365\u001b[0m\n",
            "|| ITR: 0375 | Loss: \u001b[1m\u001b[33m0.0352\u001b[0m\n",
            "|| ITR: 0400 | Loss: \u001b[1m\u001b[33m0.0377\u001b[0m\n",
            "|| ITR: 0425 | Loss: \u001b[1m\u001b[33m0.0352\u001b[0m\n",
            "|| ITR: 0450 | Loss: \u001b[1m\u001b[33m0.0349\u001b[0m\n",
            "|| ITR: 0475 | Loss: \u001b[1m\u001b[33m0.0327\u001b[0m\n",
            "|| ITR: 0500 | Loss: \u001b[1m\u001b[33m0.0372\u001b[0m\n",
            "|| ITR: 0525 | Loss: \u001b[1m\u001b[33m0.0344\u001b[0m\n",
            "|| ITR: 0550 | Loss: \u001b[1m\u001b[33m0.0306\u001b[0m\n",
            "|| ITR: 0575 | Loss: \u001b[1m\u001b[33m0.0332\u001b[0m\n",
            "|| ITR: 0600 | Loss: \u001b[1m\u001b[33m0.0321\u001b[0m\n",
            "|| ITR: 0625 | Loss: \u001b[1m\u001b[33m0.0322\u001b[0m\n",
            "|| ITR: 0650 | Loss: \u001b[1m\u001b[33m0.0251\u001b[0m\n",
            "|| ITR: 0675 | Loss: \u001b[1m\u001b[33m0.0276\u001b[0m\n",
            "|| ITR: 0700 | Loss: \u001b[1m\u001b[33m0.0292\u001b[0m\n",
            "|| ITR: 0725 | Loss: \u001b[1m\u001b[33m0.0354\u001b[0m\n",
            "|| ITR: 0750 | Loss: \u001b[1m\u001b[33m0.0273\u001b[0m\n",
            "|| ITR: 0775 | Loss: \u001b[1m\u001b[33m0.0287\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "new_parser = {'mb_size': 32,\n",
        "\n",
        "             'iteration_burn_in': 1000,\n",
        "             'iteration': 14000,\n",
        "\n",
        "             'keep_probability': 0.8,\n",
        "             'lr_train':32*1e-6,\n",
        "\n",
        "             'h_dim_shared': 75,\n",
        "             'h_dim_CS' : 75,\n",
        "             'num_layers_shared': 3,\n",
        "             'num_layers_CS' : 3,\n",
        "\n",
        "\n",
        "             'active_fn' : 'relu',\n",
        "           \n",
        "              'out_path' : 'C:/Users/AgostinoSevilla/Desktop/Tesi/iterazioni',\n",
        "             \n",
        "             'reg_W'         : 1e-6,\n",
        "             'reg_W_out'     : 0.5,\n",
        "\n",
        "             'alpha' :2.,\n",
        "             'beta'  :.5\n",
        "             \n",
        "             }\n",
        "[m , c , ct , model] = get_valid_performance_mod(DATA,MASK, DATAte , MASKte,new_parser,out_itr=.8,eval_time=np.arange(1,16,1), early_stop=8 , OUT_ITERATION = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8CwuBgGc-Ky"
      },
      "outputs": [],
      "source": [
        "(datate, timete, labelte)  = DATAte\n",
        "p = model.predict( datate)\n",
        "model.x_dim\n",
        "p.shape\n",
        " \n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321  ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,3)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321  )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], timete, (labelte[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_m = np.mean(np.array(cind[3:14]))\n",
        "cind_m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAoxO5Tykzfd"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iynal4RCANOz"
      },
      "outputs": [],
      "source": [
        "from lifelines.utils import concordance_index\n",
        "\n",
        "DIM, DATA, MASK = import_dataset_METABRIC_mod()\n",
        "(datate, timete, labelte) \n",
        "p = model.predict( datate)\n",
        "\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], timete, (labelte[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_m = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPV-xBGmANO0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cff8LQJWhA0y"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.set(rc={'figure.figsize':(15,10)})\n",
        "sns.lineplot(np.arange(len(ct)),ct )\n",
        "plt.xlabel('Epoche')\n",
        "plt.ylabel('C-index')\n",
        "plt.title('Concordance index per ogni epoca')\n",
        "plt.savefig('Epoc.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDyjalyO_PtQ"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.set(rc={'figure.figsize':(15,10)})\n",
        "sns.lineplot(np.arange(len(c)),c )\n",
        "plt.xlabel('Epoche')\n",
        "plt.ylabel('C-index')\n",
        "plt.title('Concordance index per ogni epoca')\n",
        "plt.savefig('Epoc.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQSQ_mbHg55b"
      },
      "outputs": [],
      "source": [
        "label = labelte\n",
        "sum(label)\n",
        "p = model.predict( datate)\n",
        "\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321  ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321  )\n",
        "\n",
        "risk = pd.DataFrame(risk)\n",
        "sns.set(rc={'figure.figsize':(15,10)})\n",
        "for i in range(6321 ):\n",
        "  if label[i] == 0:\n",
        "    sns.lineplot(np.arange(1,16,1),1-risk.loc[i,:] , alpha = .1, color = 'darkblue')\n",
        "for i in range(6321  ):\n",
        "  if label[i] == 1:\n",
        "    sns.lineplot(np.arange(1,16,1),1-risk.loc[i,:] , alpha = .8, color = 'indianred')\n",
        "plt.title('Probabilità di evento MACE predetta per tutti i pazienti')   \n",
        "plt.savefig('sop_pred=.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss4amR_RvBr9"
      },
      "outputs": [],
      "source": [
        "for i in range(6321  ):\n",
        "  if label[i] == 1:\n",
        "    sns.lineplot(np.arange(1,16,1),1-risk.loc[i,:] , alpha = .8, color = 'indianred')\n",
        "plt.title('Probabilità di evento MACE predetta per tutti i pazienti')  \n",
        "plt.savefig('sop_pred.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuQyq21aO8DO"
      },
      "outputs": [],
      "source": [
        "cind = []\n",
        "risk = np.zeros(shape=(6321  ,15))\n",
        "p = model.predict(datate)\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321  )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], timete, (labelte[:,0] == 1).astype(int), eval_horizon) )  \n",
        "cind_m = np.mean(np.array(cind))\n",
        "cind_m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VWvQlCTSaUS"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nUYMv_TrA9q"
      },
      "outputs": [],
      "source": [
        "def import_dataset_METABRIC_mod_shuffle(col,norm_mode='standard' ):\n",
        "    in_filename1 = '/content/data_test_boost.txt'\n",
        "    \n",
        "    df1 = pd.read_csv(in_filename1, sep =',')\n",
        "    df1['time_CVD1'] = np.float32(np.where( df1['time_CVD1'] == 'non evento' , 0 , df1['time_CVD1']))\n",
        "\n",
        "    df1[['SESSO']] = np.where(df1[['SESSO']]=='F',1,0) \n",
        "    df1[['anno_inizio_ARV']] = np.where(df1[['anno_inizio_ARV']]=='<=2007',1,0) \n",
        "    df1[['FDRn']] = np.where(df1[['FDRn']]=='MSM',1,0)\n",
        "    df1[['HCV']] = np.float32(np.where(df1[['HCV']]=='non noto',0,df1[['HCV']]))\n",
        "    df1[['HBV']] = np.float32(np.where(df1[['HBV']]=='non noto',0,df1[['HBV']]))\n",
        "\n",
        "\n",
        "    bin_list           = [ 'anno_inizio_ARV' ,  'HCV' ]\n",
        "    cont_list          = [ 'AGE_UPD' ,'ipert_si_no' ,  'CD4' ,  'VIREMIA' , 'PLT' ,\n",
        "                            'TRIG' , 'CREATININA'  , \"PI_time_exp\" , 'NRTI_time_exp' , 'NNRTI_time_exp']\n",
        "    feat_list          = cont_list + bin_list\n",
        " \n",
        "    df1[col] = np.random.permutation( df1[col])\n",
        "\n",
        "    df1[['time']] = np.where(df1[['ID_CVD']] == 1 , df1[['time_CVD1']] , df1[['end']])\n",
        "    df1[['label']] = df1[['ID_CVD']]\n",
        "\n",
        "    print(df1)\n",
        "    df2 = df1.loc[:,['label','time']]\n",
        "    df1 = df1.loc[:,feat_list]\n",
        "    data  = np.asarray(df1)\n",
        "    data  = f_get_Normalization(data, norm_mode)\n",
        "    print('ok')\n",
        "  \n",
        "    time  = np.asarray(df2[['time']])\n",
        "    # time  = np.round(time/12.) #unit time = month\n",
        "    label = np.asarray(df2[['label']])\n",
        "\n",
        "    \n",
        "    num_Category    = int(np.max(time) * 1.2)        #to have enough time-horizon\n",
        "    num_Event       = int(len(np.unique(label)) - 1) #only count the number of events (do not count censoring as an event)\n",
        "\n",
        "    x_dim           = np.shape(data)[1]\n",
        "\n",
        "    mask1           = f_get_fc_mask2(time, label, num_Event, num_Category)\n",
        "    mask2           = f_get_fc_mask3(time, -1, num_Category)\n",
        "\n",
        "    DIM             = (x_dim)\n",
        "    DATA            = (data, time, label)\n",
        "    MASK            = (mask1, mask2)\n",
        "\n",
        "    return DIM, DATA, MASK\n",
        "\n",
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('AGE_UPD')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )  \n",
        "cind_age = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkJGZhZzNBLr"
      },
      "outputs": [],
      "source": [
        "DIM, DATA, MASK = import_dataset_METABRIC_mod_shuffle('VIREMIA')\n",
        "(data, time, label)  = DATA\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_vir = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ms7LGMJNBdw"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('COLEST')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_col = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L95fEC1-NBrN"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('CD4')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_cd = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWZfOKVwNB2z"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('diabete_si_no')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_dia = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8te1TExNCC0"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('SESSO')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_sex = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK5SAhfdD9nT"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('ipert_si_no')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_iper_si_no = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnl5aIdpD9nZ"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('tumore_si_no')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_tumore_si_no = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJkoU_UZD9na"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('AST')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_AST= np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPPU-qHQD9nb"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('ALT')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_ALT = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMVDe9wwEERY"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('HCV')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_HCV = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pv76snTTEERZ"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('HBV')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_HBV = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMaNJfUQEERa"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('CREATININA')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_CREATININA = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7KSshd-EERb"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('aids_si_no')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_aids_si_no = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UQuK8G1Eh5v"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('PLT')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_PLT = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvkBkjauEh5w"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('TRIG')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_TRIG = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMBAVh5OEh5x"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('Hb')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_Hb = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6TcFTNIEh5x"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('INI_time_exp')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_INI_time_exp = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnB3HvVoEriE"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('PI_time_exp')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_PI_time_exp = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpaFWNK2Erie"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('NRTI_time_exp')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_NRTI_time_exp = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WzSBAxkErif"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('HCV')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_HCV = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA9eStrkKa1c"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('HBV')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_HBV = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "280s1f1MKa1q"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('NNRTI_time_exp')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_NRTTI_time_exp = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkOsFHQSKkqT"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('RAZZA')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_RAZZA = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQyfbH27KkqU"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('FDRn')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_FDRn = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUt2KmB7KkqV"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod_shuffle('anno_inizio_ARV')\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "\n",
        "p = model.predict( data_imp)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], time_imp, (label_imp[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_anno_inizio_ARV = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sWVO9VPPN98"
      },
      "outputs": [],
      "source": [
        "DIM_imp, DATA_imp , MASK_imp = import_dataset_METABRIC_mod()\n",
        "(data_imp, time_imp, label_imp)  = DATA_imp\n",
        "\n",
        "\n",
        "p = model.predict( datate)\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321   ,15))\n",
        "for t, t_time in enumerate(np.arange(3,16,1)):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321   )\n",
        "\n",
        "\n",
        "  cind.append(c_index(risk[:,t], timete, (labelte[:,0] == 1).astype(int), eval_horizon) )   \n",
        "cind_m = np.mean(np.array(cind))\n",
        "cind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72PH7sV3m0ah"
      },
      "outputs": [],
      "source": [
        "sns.set(rc={'figure.figsize':(40,5)})\n",
        "\n",
        "d = pd.DataFrame({'Colonne':['età' , 'cd4' , 'viremia' , \"PI_time_exp\" , 'NRTI_time_exp' , 'NNRTI_time_exp',\n",
        "                              'RAZZA', 'anno_inizio_ARV' , 'HCV', 'TRIG'  ], \n",
        "                  'valori':[cind_m-cind_age , cind_m-cind_cd,cind_m-cind_vir,\n",
        "                            cind_m-cind_PI_time_exp , cind_m-cind_NRTI_time_exp , cind_m-cind_NRTTI_time_exp , \n",
        "                            cind_m-cind_RAZZA , cind_m-cind_anno_inizio_ARV, cind_m-cind_HCV , cind_m-cind_TRIG]/cind_m})\n",
        "sns.barplot(y = 'Colonne',x = 'valori',data=d, orient = 'h')\n",
        "sns.barplot(x = 'Colonne',y = 'valori',data=d)\n",
        "plt.savefig('feature_imp1.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfexFwqLINle"
      },
      "source": [
        "# Dynamic DeepHit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aC6342fwIRNR"
      },
      "outputs": [],
      "source": [
        "print('ok')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_kh0vL3Ryig"
      },
      "outputs": [],
      "source": [
        "pip install shap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-yGHXopRwuq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost.sklearn import XGBRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn import tree\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXpqQVqmTJGV"
      },
      "outputs": [],
      "source": [
        "def pred(data):\n",
        "  risk = model.predict(data)\n",
        "  return risk[:,0,12]\n",
        "\n",
        "rf_shap_values = shap.KernelExplainer(pred, shap.sample(datate, nsamples = 10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyvPEIApXyH4"
      },
      "outputs": [],
      "source": [
        "shap_val = rf_shap_values.shap_values(datate ,nsamples=10 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwNs84u4a_ki"
      },
      "outputs": [],
      "source": [
        "shap.initjs()\n",
        "\n",
        "bin_list           = [ 'anno_inizio_ARV' ,  'HCV' ]\n",
        "cont_list          = [ 'AGE_UPD' ,'ipert_si_no' ,  'CD4' ,  'VIREMIA' , 'PLT' ,\n",
        "                        'TRIG' , 'CREATININA'  , \"PI_time_exp\" , 'NRTI_time_exp' , 'NNRTI_time_exp']\n",
        "feat_list          = cont_list + bin_list\n",
        "shap.force_plot(rf_shap_values.expected_value, shap_val , datate, link='logit' , feature_names= feat_list )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyypm_Ci8xdo"
      },
      "outputs": [],
      "source": [
        "s = shap.force_plot(rf_shap_values.expected_value, shap_val , datate, link='logit' , feature_names= feat_list )\n",
        "\n",
        "shap.save_html(\"/content/shapley.html\",s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np5vzRTzfDrF"
      },
      "outputs": [],
      "source": [
        "d = pd.DataFrame(datate , columns=feat_list )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4t2In6QbNue"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SotIlQBbNDp"
      },
      "outputs": [],
      "source": [
        "label = labelte\n",
        "sum(label)\n",
        "p = model.predict( datate )\n",
        "\n",
        "p.shape\n",
        "cind = []\n",
        "risk = np.zeros(shape=(6321  ,3000))\n",
        "for t, t_time in enumerate(np.linspace(1,15,3000 )):\n",
        "  eval_horizon = int(t_time)\n",
        "  risk[:,t] = np.reshape(np.sum(p[:,:,:(eval_horizon+1)], axis=2), 6321  )\n",
        "\n",
        "risk = pd.DataFrame(risk)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFB7DP9BkYb8"
      },
      "outputs": [],
      "source": [
        "time_pred = np.zeros(6321)\n",
        "t = np.linspace(0,15,3000)\n",
        "for k in range(6321):\n",
        "  if sum((risk.loc[k,:]>0.02))>0:\n",
        "    time_pred[k] = t[(risk.loc[k,:]>0.02)][0]\n",
        "  else:\n",
        "    time_pred[k] = 15\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUJ_Yw0QoV-K"
      },
      "outputs": [],
      "source": [
        "time_true = timete\n",
        "MSE_e = np.mean(np.square(time_true[np.reshape(labelte , 6321) == 1]-time_pred[np.reshape(labelte , 6321) == 1]))\n",
        "MSE_a = np.mean(np.square(time_true[np.reshape(timete , 6321) <= 15]-time_pred[np.reshape(timete , 6321) <= 15]))\n",
        "cnt = 0\n",
        "for i in range(6321):\n",
        "  if time_true[i] <= 15:\n",
        "    if time_true[i] < time_pred[i]:\n",
        "      cnt +=1\n",
        "perc = cnt/sum(time_true <= 15)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS5f9W6Bvlz3"
      },
      "outputs": [],
      "source": [
        "sum(labelte)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8d6PF2nkBlb"
      },
      "outputs": [],
      "source": [
        "print(MSE_e)\n",
        "print(MSE_a)\n",
        "print(perc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPPuyJGhrxoF"
      },
      "outputs": [],
      "source": [
        "lab_pred = time_pred != 15."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deK-AnefsJyC"
      },
      "outputs": [],
      "source": [
        "TP = sum(np.diag(np.multiply(lab_pred,labelte)))\n",
        "TN = sum(np.diag(labelte+lab_pred) == 0)\n",
        "FP = sum(lab_pred) - TP\n",
        "FN = sum(lab_pred==False) - TN\n",
        "print(\"sens:\"  + str(TP / (TP+FN)))\n",
        "print(\"acc:\" + str((TP+TN)/(TP+TN+FP+FN)))\n",
        "\n",
        "print(\"sec:\" + str((TN)/(TN+FP)))\n",
        "print(str(TP)+\"|\"+str(FN))\n",
        "print(str(FP)+\"|\"+str(TN))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Bootstrap_DH.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}